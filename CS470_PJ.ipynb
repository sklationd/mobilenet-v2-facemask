{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS470_PJ.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sklationd/mobilenet-v2-facemask/blob/main/CS470_PJ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFifDWoz20QY"
      },
      "source": [
        "# 0. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWemigXMuUqk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y_g93YMKedb"
      },
      "source": [
        "# 1. Check is GPU available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASflVQJpKdwm"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LWKvs-oK7SM"
      },
      "source": [
        "# 2. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo8G_KsJK6cu",
        "outputId": "b04231af-759b-4757-d0e4-628e43e851c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "root = '/gdrive/My Drive/CS470/Project'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EFhO3f0NdGs"
      },
      "source": [
        "# 3. Define datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl9BYYPtNjLB"
      },
      "source": [
        "class MaskDataset(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        # self.imgs = list(sorted(os.listdir(Path(root) / 'images')))\n",
        "        self.imgs=list(sorted(os.listdir(Path(root)/'dataset')))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.imgs[idx]\n",
        "        img_path = os.path.join(Path(root)/ 'dataset/',file_name)\n",
        "        img = Image.open(img_path).convert(\"RGB\")    \n",
        "        \n",
        "        #~~~~~~.jpg                       0 ; without mask\n",
        "        #00001_Mask.jpg                   1\n",
        "        #00001_Mask_Chin.jpg.             2\n",
        "        #00001_Mask_Mouth_Chin.jpg        3\n",
        "        #00001_Mask_Nose_Mouth.jpg        4\n",
        "\n",
        "        p_list=str(file_name).split('_')\n",
        "        if len(p_list)==1:\n",
        "          label=0\n",
        "        elif len(p_list)==2:\n",
        "          label=1\n",
        "        elif len(p_list)==3:\n",
        "          label=2\n",
        "        elif p_list[-1]=='Chin.jpg':\n",
        "          label=3\n",
        "        else:\n",
        "          label=4\n",
        "          \n",
        "        #Generate Label\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "            \n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dNa-kYyLy4F"
      },
      "source": [
        "# 4. Build dataloader and transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dts-OlpGOP_S"
      },
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bK1F9MFRcsS"
      },
      "source": [
        "dataset = MaskDataset(preprocess)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=4, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=4, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGs60j8MUNNt",
        "outputId": "77daaf42-9623-4f6a-dbd8-191734915a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import cv2\n",
        "\n",
        "print('size of train datasets :',len(train_loader.dataset))\n",
        "print('size of test datasets  :',len(test_loader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of train datasets : 2145\n",
            "size of test datasets  : 537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR2vpmVLXN2i"
      },
      "source": [
        "# 5. Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ias2d-IevhPI"
      },
      "source": [
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from typing import Callable, Any, Optional, List\n",
        "\n",
        "\n",
        "__all__ = ['MobileNetV2', 'mobilenet_v2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_planes: int,\n",
        "        out_planes: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int,\n",
        "        oup: int,\n",
        "        stride: int,\n",
        "        expand_ratio: int,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n",
        "        layers.extend([\n",
        "            # dw\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            norm_layer(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 1000,\n",
        "        width_mult: float = 1.0,\n",
        "        inverted_residual_setting: Optional[List[List[int]]] = None,\n",
        "        round_nearest: int = 8,\n",
        "        block: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        MobileNet V2 main class\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
        "            inverted_residual_setting: Network structure\n",
        "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
        "            Set to 1 to turn off rounding\n",
        "            block: Module specifying inverted residual building block for mobilenet\n",
        "            norm_layer: Module specifying the normalization layer to use\n",
        "\n",
        "        \"\"\"\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        if block is None:\n",
        "            block = InvertedResidual\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        if inverted_residual_setting is None:\n",
        "            inverted_residual_setting = [\n",
        "                # t, c, n, s\n",
        "                [1, 16, 1, 1],\n",
        "                [6, 24, 2, 2],\n",
        "                [6, 32, 3, 2],\n",
        "                [6, 64, 4, 2],\n",
        "                [6, 96, 3, 1],\n",
        "                [6, 160, 3, 2],\n",
        "                [6, 320, 1, 1],\n",
        "            ]\n",
        "\n",
        "        # only check the first element, assuming user knows t,c,n,s are required\n",
        "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
        "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
        "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
        "        features: List[nn.Module] = [ConvBNReLU(3, input_channel, stride=2, norm_layer=norm_layer)]\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
        "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
        "        x = self.features(x)\n",
        "        # Cannot use \"squeeze\" as batch-size can be 1 => must use reshape with x.shape[0]\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1)).reshape(x.shape[0], -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tC7p2aUXNae",
        "outputId": "9714f25d-4f38-410d-d8af-1ecdbbf52584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# model = torch.hub.load('pytorch/vision:v0.6.0', 'mobilenet_v2', pretrained=True)\n",
        "model = MobileNetV2(num_classes=5)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): ConvBNReLU(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvBNReLU(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvBNReLU(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): ConvBNReLU(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=1280, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwj4TMsyhIor"
      },
      "source": [
        "# 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDjaLKOd3mAt"
      },
      "source": [
        "# pre-setup\n",
        "num_epochs = 50\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "learning_rate = 0.0005\n",
        "optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=0.9, weight_decay=0.000001)\n",
        "loss_func = torch.nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxO2GOrUg3RP"
      },
      "source": [
        "# for plotting\n",
        "from matplotlib import pyplot as plt\n",
        "train_loss_list = []\n",
        "test_loss_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhDSfvXv3s13",
        "outputId": "cd7fe44d-a2cc-4216-e6cd-b946623735f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "\n",
        "  epoch_loss = 0\n",
        "  test_loss = 0\n",
        "  best_test_loss = 1\n",
        "\n",
        "  for i, samples in enumerate(train_loader):\n",
        "      imgs, annotations = samples\n",
        "      imgs, annotations = imgs.to(device), annotations.to(device)\n",
        "      \n",
        "      output = model(imgs)\n",
        "      loss = loss_func(output,annotations)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print(f'Iteration: {i+1}/{len(train_loader)}, Loss: {loss.item()}')\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  avg_train_loss = epoch_loss/len(train_loader)\n",
        "\n",
        "  # Save result for plotting\n",
        "  train_loss_list.append(avg_train_loss)\n",
        "\n",
        "  # Print epoch's test loss\n",
        "  print(f'Epoch {epoch} train loss: {avg_train_loss}')\n",
        "\n",
        "  # validation\n",
        "  for i,test_samples in enumerate(test_loader):\n",
        "    test_imgs, test_annotations = test_samples\n",
        "    test_imgs, test_annotations = test_imgs.to(device), test_annotations.to(device)\n",
        "\n",
        "    test_output = model(test_imgs)\n",
        "    loss = loss_func(test_output, test_annotations)\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    # print(f'Iteration: {i+1}/{len(test_loader)}, Loss: {loss.item()}')\n",
        "  \n",
        "  avg_test_loss = test_loss/len(test_loader)\n",
        "\n",
        "  # Save result for plotting\n",
        "  test_loss_list.append(avg_test_loss)\n",
        "\n",
        "  # Print epoch's test loss\n",
        "  print(f'Epoch {epoch} test loss: {avg_test_loss}')\n",
        "  \n",
        "  # save best\n",
        "  if best_test_loss > avg_test_loss:\n",
        "    best_test_loss = avg_test_loss\n",
        "    torch.save(model.state_dict(), Path(root) / 'best.pt')\n",
        "\n",
        "  print('-------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Iteration: 402/537, Loss: 0.0018567400984466076\n",
            "Iteration: 403/537, Loss: 0.0008716234588064253\n",
            "Iteration: 404/537, Loss: 0.0003247935383114964\n",
            "Iteration: 405/537, Loss: 0.002723329234868288\n",
            "Iteration: 406/537, Loss: 0.0009552498813718557\n",
            "Iteration: 407/537, Loss: 0.0008072820492088795\n",
            "Iteration: 408/537, Loss: 0.0019619774539023638\n",
            "Iteration: 409/537, Loss: 0.001559925964102149\n",
            "Iteration: 410/537, Loss: 0.00029521877877414227\n",
            "Iteration: 411/537, Loss: 0.0006268244469538331\n",
            "Iteration: 412/537, Loss: 0.0030129600781947374\n",
            "Iteration: 413/537, Loss: 0.00046640547225251794\n",
            "Iteration: 414/537, Loss: 0.0009791059419512749\n",
            "Iteration: 415/537, Loss: 0.00458819605410099\n",
            "Iteration: 416/537, Loss: 0.002226887969300151\n",
            "Iteration: 417/537, Loss: 0.0010690500494092703\n",
            "Iteration: 418/537, Loss: 0.0016576835187152028\n",
            "Iteration: 419/537, Loss: 0.0002491243649274111\n",
            "Iteration: 420/537, Loss: 0.052346765995025635\n",
            "Iteration: 421/537, Loss: 0.0015918919816613197\n",
            "Iteration: 422/537, Loss: 0.0006980150355957448\n",
            "Iteration: 423/537, Loss: 0.00017028448928613216\n",
            "Iteration: 424/537, Loss: 0.0060976468957960606\n",
            "Iteration: 425/537, Loss: 0.0004901199718005955\n",
            "Iteration: 426/537, Loss: 0.016789635643363\n",
            "Iteration: 427/537, Loss: 0.012073667719960213\n",
            "Iteration: 428/537, Loss: 0.00040950311813503504\n",
            "Iteration: 429/537, Loss: 0.0013044411316514015\n",
            "Iteration: 430/537, Loss: 0.00044709068606607616\n",
            "Iteration: 431/537, Loss: 0.3156454265117645\n",
            "Iteration: 432/537, Loss: 0.0022830632515251637\n",
            "Iteration: 433/537, Loss: 0.00023545844305772334\n",
            "Iteration: 434/537, Loss: 0.00025098316837102175\n",
            "Iteration: 435/537, Loss: 0.024686088785529137\n",
            "Iteration: 436/537, Loss: 0.0007712399819865823\n",
            "Iteration: 437/537, Loss: 0.004021261818706989\n",
            "Iteration: 438/537, Loss: 0.0004917827900499105\n",
            "Iteration: 439/537, Loss: 0.002673670183867216\n",
            "Iteration: 440/537, Loss: 0.08248211443424225\n",
            "Iteration: 441/537, Loss: 0.007523552980273962\n",
            "Iteration: 442/537, Loss: 0.0009132158011198044\n",
            "Iteration: 443/537, Loss: 0.007840813137590885\n",
            "Iteration: 444/537, Loss: 0.0002745299134403467\n",
            "Iteration: 445/537, Loss: 0.003139142645522952\n",
            "Iteration: 446/537, Loss: 0.0009056284325197339\n",
            "Iteration: 447/537, Loss: 0.011020384728908539\n",
            "Iteration: 448/537, Loss: 0.001243396895006299\n",
            "Iteration: 449/537, Loss: 0.13992942869663239\n",
            "Iteration: 450/537, Loss: 0.0025658172089606524\n",
            "Iteration: 451/537, Loss: 0.002396185649558902\n",
            "Iteration: 452/537, Loss: 0.00020331300038378686\n",
            "Iteration: 453/537, Loss: 0.0029532653279602528\n",
            "Iteration: 454/537, Loss: 0.0007669475744478405\n",
            "Iteration: 455/537, Loss: 0.001524115912616253\n",
            "Iteration: 456/537, Loss: 0.0003828538756351918\n",
            "Iteration: 457/537, Loss: 0.012381797656416893\n",
            "Iteration: 458/537, Loss: 0.0013076122850179672\n",
            "Iteration: 459/537, Loss: 0.0021842524874955416\n",
            "Iteration: 460/537, Loss: 0.0018909291829913855\n",
            "Iteration: 461/537, Loss: 0.001245393417775631\n",
            "Iteration: 462/537, Loss: 0.7140628695487976\n",
            "Iteration: 463/537, Loss: 0.0582144558429718\n",
            "Iteration: 464/537, Loss: 0.00029183647711761296\n",
            "Iteration: 465/537, Loss: 0.00201762979850173\n",
            "Iteration: 466/537, Loss: 0.00012974278070032597\n",
            "Iteration: 467/537, Loss: 0.002346423454582691\n",
            "Iteration: 468/537, Loss: 0.011389675550162792\n",
            "Iteration: 469/537, Loss: 0.0012435487005859613\n",
            "Iteration: 470/537, Loss: 0.0010658458340913057\n",
            "Iteration: 471/537, Loss: 0.0027327670250087976\n",
            "Iteration: 472/537, Loss: 0.010864024050533772\n",
            "Iteration: 473/537, Loss: 0.0019563015084713697\n",
            "Iteration: 474/537, Loss: 0.0007719519198872149\n",
            "Iteration: 475/537, Loss: 0.3928350806236267\n",
            "Iteration: 476/537, Loss: 0.0008070069015957415\n",
            "Iteration: 477/537, Loss: 0.003962688148021698\n",
            "Iteration: 478/537, Loss: 0.0009329541935585439\n",
            "Iteration: 479/537, Loss: 0.0012989877723157406\n",
            "Iteration: 480/537, Loss: 0.0010588918812572956\n",
            "Iteration: 481/537, Loss: 0.4339476525783539\n",
            "Iteration: 482/537, Loss: 0.00237623555585742\n",
            "Iteration: 483/537, Loss: 0.0009235283359885216\n",
            "Iteration: 484/537, Loss: 0.0010495138121768832\n",
            "Iteration: 485/537, Loss: 0.0006302352412603796\n",
            "Iteration: 486/537, Loss: 0.0009855750249698758\n",
            "Iteration: 487/537, Loss: 0.0007145758718252182\n",
            "Iteration: 488/537, Loss: 0.0012395053636282682\n",
            "Iteration: 489/537, Loss: 0.0019666634034365416\n",
            "Iteration: 490/537, Loss: 0.0008477919036522508\n",
            "Iteration: 491/537, Loss: 0.0006397717515937984\n",
            "Iteration: 492/537, Loss: 0.00502731092274189\n",
            "Iteration: 493/537, Loss: 0.0011067038867622614\n",
            "Iteration: 494/537, Loss: 0.0004418081953190267\n",
            "Iteration: 495/537, Loss: 0.00040401751175522804\n",
            "Iteration: 496/537, Loss: 0.00246606906875968\n",
            "Iteration: 497/537, Loss: 0.0013428027741611004\n",
            "Iteration: 498/537, Loss: 0.0008347061229869723\n",
            "Iteration: 499/537, Loss: 0.002783285453915596\n",
            "Iteration: 500/537, Loss: 0.0008395033073611557\n",
            "Iteration: 501/537, Loss: 0.0008039893582463264\n",
            "Iteration: 502/537, Loss: 0.0004664865555241704\n",
            "Iteration: 503/537, Loss: 0.0006773275672458112\n",
            "Iteration: 504/537, Loss: 0.00045888242311775684\n",
            "Iteration: 505/537, Loss: 0.00014726517838425934\n",
            "Iteration: 506/537, Loss: 0.0005651291576214135\n",
            "Iteration: 507/537, Loss: 0.0009111901745200157\n",
            "Iteration: 508/537, Loss: 0.0026907578576356173\n",
            "Iteration: 509/537, Loss: 0.0026112047489732504\n",
            "Iteration: 510/537, Loss: 0.0007878734031692147\n",
            "Iteration: 511/537, Loss: 0.0032218717969954014\n",
            "Iteration: 512/537, Loss: 0.00041792436968535185\n",
            "Iteration: 513/537, Loss: 0.00576731888577342\n",
            "Iteration: 514/537, Loss: 0.0013688568724319339\n",
            "Iteration: 515/537, Loss: 0.000722588156349957\n",
            "Iteration: 516/537, Loss: 0.007217276841402054\n",
            "Iteration: 517/537, Loss: 0.0024579723831266165\n",
            "Iteration: 518/537, Loss: 0.0008245612261816859\n",
            "Iteration: 519/537, Loss: 0.000395626702811569\n",
            "Iteration: 520/537, Loss: 0.0005715743754990399\n",
            "Iteration: 521/537, Loss: 0.007704120595008135\n",
            "Iteration: 522/537, Loss: 0.0009229637216776609\n",
            "Iteration: 523/537, Loss: 0.0013136907946318388\n",
            "Iteration: 524/537, Loss: 0.0021933098323643208\n",
            "Iteration: 525/537, Loss: 0.002986917272210121\n",
            "Iteration: 526/537, Loss: 0.004083819221705198\n",
            "Iteration: 527/537, Loss: 0.0004645968438126147\n",
            "Iteration: 528/537, Loss: 0.0004682514409068972\n",
            "Iteration: 529/537, Loss: 0.000912732386495918\n",
            "Iteration: 530/537, Loss: 0.00041523168329149485\n",
            "Iteration: 531/537, Loss: 0.002058363286778331\n",
            "Iteration: 532/537, Loss: 0.0010842425981536508\n",
            "Iteration: 533/537, Loss: 0.00040312446071766317\n",
            "Iteration: 534/537, Loss: 0.014255152083933353\n",
            "Iteration: 535/537, Loss: 0.0013998968061059713\n",
            "Iteration: 536/537, Loss: 0.00020875864720437676\n",
            "Iteration: 537/537, Loss: 0.24458634853363037\n",
            "Epoch 40 train loss: 0.016547095710394324\n",
            "Epoch 40 test loss: 0.08596828773879679\n",
            "-------------------------------------\n",
            "Iteration: 1/537, Loss: 0.12570258975028992\n",
            "Iteration: 2/537, Loss: 0.004575648810714483\n",
            "Iteration: 3/537, Loss: 0.00022750539937987924\n",
            "Iteration: 4/537, Loss: 0.0005659201997332275\n",
            "Iteration: 5/537, Loss: 0.0016657010419294238\n",
            "Iteration: 6/537, Loss: 0.0011456627398729324\n",
            "Iteration: 7/537, Loss: 0.0002708126849029213\n",
            "Iteration: 8/537, Loss: 0.000555224425625056\n",
            "Iteration: 9/537, Loss: 0.011801359243690968\n",
            "Iteration: 10/537, Loss: 0.008909463882446289\n",
            "Iteration: 11/537, Loss: 0.0021007684990763664\n",
            "Iteration: 12/537, Loss: 0.0043028295040130615\n",
            "Iteration: 13/537, Loss: 0.0008161083678714931\n",
            "Iteration: 14/537, Loss: 0.0020815692842006683\n",
            "Iteration: 15/537, Loss: 0.0004542407696135342\n",
            "Iteration: 16/537, Loss: 0.0029541344847530127\n",
            "Iteration: 17/537, Loss: 0.004085709806531668\n",
            "Iteration: 18/537, Loss: 0.0006659047212451696\n",
            "Iteration: 19/537, Loss: 0.00454411655664444\n",
            "Iteration: 20/537, Loss: 0.0008306067320518196\n",
            "Iteration: 21/537, Loss: 0.00032411879510618746\n",
            "Iteration: 22/537, Loss: 0.0038344874046742916\n",
            "Iteration: 23/537, Loss: 0.000805947813205421\n",
            "Iteration: 24/537, Loss: 0.0033305278047919273\n",
            "Iteration: 25/537, Loss: 0.001065366086550057\n",
            "Iteration: 26/537, Loss: 0.5663836002349854\n",
            "Iteration: 27/537, Loss: 0.00026132218772545457\n",
            "Iteration: 28/537, Loss: 0.018654974177479744\n",
            "Iteration: 29/537, Loss: 0.14546716213226318\n",
            "Iteration: 30/537, Loss: 0.018187645822763443\n",
            "Iteration: 31/537, Loss: 0.00037757749669253826\n",
            "Iteration: 32/537, Loss: 0.0004577222280204296\n",
            "Iteration: 33/537, Loss: 0.00036794753395952284\n",
            "Iteration: 34/537, Loss: 0.0013637534575536847\n",
            "Iteration: 35/537, Loss: 0.002177804009988904\n",
            "Iteration: 36/537, Loss: 0.00045228615636006\n",
            "Iteration: 37/537, Loss: 0.0008247278747148812\n",
            "Iteration: 38/537, Loss: 0.0025762938894331455\n",
            "Iteration: 39/537, Loss: 0.03981885313987732\n",
            "Iteration: 40/537, Loss: 0.0003851062210742384\n",
            "Iteration: 41/537, Loss: 0.0002563843154348433\n",
            "Iteration: 42/537, Loss: 0.0009964247001335025\n",
            "Iteration: 43/537, Loss: 0.0018399867694824934\n",
            "Iteration: 44/537, Loss: 0.0011367890983819962\n",
            "Iteration: 45/537, Loss: 0.0007022181525826454\n",
            "Iteration: 46/537, Loss: 0.00021816934167873114\n",
            "Iteration: 47/537, Loss: 0.011967796832323074\n",
            "Iteration: 48/537, Loss: 0.0003020463336724788\n",
            "Iteration: 49/537, Loss: 0.0004684534505940974\n",
            "Iteration: 50/537, Loss: 0.009216362610459328\n",
            "Iteration: 51/537, Loss: 0.00437717093154788\n",
            "Iteration: 52/537, Loss: 0.005703269969671965\n",
            "Iteration: 53/537, Loss: 0.0008059748797677457\n",
            "Iteration: 54/537, Loss: 0.0002176951093133539\n",
            "Iteration: 55/537, Loss: 0.002541428431868553\n",
            "Iteration: 56/537, Loss: 0.0039054607041180134\n",
            "Iteration: 57/537, Loss: 0.00036796688800677657\n",
            "Iteration: 58/537, Loss: 0.00046328690950758755\n",
            "Iteration: 59/537, Loss: 0.0017781956121325493\n",
            "Iteration: 60/537, Loss: 0.001292387954890728\n",
            "Iteration: 61/537, Loss: 0.0018965472700074315\n",
            "Iteration: 62/537, Loss: 0.028408264741301537\n",
            "Iteration: 63/537, Loss: 0.000573911122046411\n",
            "Iteration: 64/537, Loss: 0.0010336008854210377\n",
            "Iteration: 65/537, Loss: 0.003087835619226098\n",
            "Iteration: 66/537, Loss: 0.004571851342916489\n",
            "Iteration: 67/537, Loss: 0.0007262016297318041\n",
            "Iteration: 68/537, Loss: 0.00038199694245122373\n",
            "Iteration: 69/537, Loss: 0.22553491592407227\n",
            "Iteration: 70/537, Loss: 0.12409427762031555\n",
            "Iteration: 71/537, Loss: 0.0018916610861197114\n",
            "Iteration: 72/537, Loss: 0.0002575477701611817\n",
            "Iteration: 73/537, Loss: 0.0231772568076849\n",
            "Iteration: 74/537, Loss: 0.001628179452382028\n",
            "Iteration: 75/537, Loss: 0.0019090085988864303\n",
            "Iteration: 76/537, Loss: 0.00021181507327128202\n",
            "Iteration: 77/537, Loss: 0.024928608909249306\n",
            "Iteration: 78/537, Loss: 0.01329280249774456\n",
            "Iteration: 79/537, Loss: 0.0002777855843305588\n",
            "Iteration: 80/537, Loss: 0.0012371510965749621\n",
            "Iteration: 81/537, Loss: 0.003288556821644306\n",
            "Iteration: 82/537, Loss: 0.0053557963110506535\n",
            "Iteration: 83/537, Loss: 0.002967357635498047\n",
            "Iteration: 84/537, Loss: 0.0031350995413959026\n",
            "Iteration: 85/537, Loss: 0.008759167045354843\n",
            "Iteration: 86/537, Loss: 0.0004919155617244542\n",
            "Iteration: 87/537, Loss: 0.0007820058963261545\n",
            "Iteration: 88/537, Loss: 0.006837394088506699\n",
            "Iteration: 89/537, Loss: 0.0009168934775516391\n",
            "Iteration: 90/537, Loss: 0.00024504936300218105\n",
            "Iteration: 91/537, Loss: 0.0005028882878832519\n",
            "Iteration: 92/537, Loss: 0.009392024949193\n",
            "Iteration: 93/537, Loss: 0.002162566175684333\n",
            "Iteration: 94/537, Loss: 0.008657587692141533\n",
            "Iteration: 95/537, Loss: 0.00020664429757744074\n",
            "Iteration: 96/537, Loss: 0.0002057278179563582\n",
            "Iteration: 97/537, Loss: 0.0019672538619488478\n",
            "Iteration: 98/537, Loss: 0.014586050994694233\n",
            "Iteration: 99/537, Loss: 0.7049520611763\n",
            "Iteration: 100/537, Loss: 0.004840032197535038\n",
            "Iteration: 101/537, Loss: 0.0015520022716373205\n",
            "Iteration: 102/537, Loss: 0.0021988453809171915\n",
            "Iteration: 103/537, Loss: 0.00402845349162817\n",
            "Iteration: 104/537, Loss: 0.00045222710468806326\n",
            "Iteration: 105/537, Loss: 0.002804933814331889\n",
            "Iteration: 106/537, Loss: 0.006333472207188606\n",
            "Iteration: 107/537, Loss: 0.004928271286189556\n",
            "Iteration: 108/537, Loss: 0.003603885183110833\n",
            "Iteration: 109/537, Loss: 0.00043673577602021396\n",
            "Iteration: 110/537, Loss: 0.003070254810154438\n",
            "Iteration: 111/537, Loss: 0.009337399154901505\n",
            "Iteration: 112/537, Loss: 0.000591387739405036\n",
            "Iteration: 113/537, Loss: 0.0034819042775779963\n",
            "Iteration: 114/537, Loss: 0.0044475761242210865\n",
            "Iteration: 115/537, Loss: 0.0016682124696671963\n",
            "Iteration: 116/537, Loss: 0.07022947072982788\n",
            "Iteration: 117/537, Loss: 0.0032869717106223106\n",
            "Iteration: 118/537, Loss: 0.002908512018620968\n",
            "Iteration: 119/537, Loss: 0.0012043740134686232\n",
            "Iteration: 120/537, Loss: 0.0036300483625382185\n",
            "Iteration: 121/537, Loss: 0.0014887386932969093\n",
            "Iteration: 122/537, Loss: 0.004039802122861147\n",
            "Iteration: 123/537, Loss: 0.0013272063806653023\n",
            "Iteration: 124/537, Loss: 0.15378382802009583\n",
            "Iteration: 125/537, Loss: 0.0037671977188438177\n",
            "Iteration: 126/537, Loss: 0.0014156735269352794\n",
            "Iteration: 127/537, Loss: 0.002631253097206354\n",
            "Iteration: 128/537, Loss: 0.0017776150489225984\n",
            "Iteration: 129/537, Loss: 0.006409851368516684\n",
            "Iteration: 130/537, Loss: 0.017751963809132576\n",
            "Iteration: 131/537, Loss: 0.02034001052379608\n",
            "Iteration: 132/537, Loss: 0.0022035641595721245\n",
            "Iteration: 133/537, Loss: 0.008309063501656055\n",
            "Iteration: 134/537, Loss: 0.005490372888743877\n",
            "Iteration: 135/537, Loss: 0.0004925541579723358\n",
            "Iteration: 136/537, Loss: 0.0007138282526284456\n",
            "Iteration: 137/537, Loss: 0.0025492049753665924\n",
            "Iteration: 138/537, Loss: 0.0038755936548113823\n",
            "Iteration: 139/537, Loss: 0.010786044411361217\n",
            "Iteration: 140/537, Loss: 0.010353085584938526\n",
            "Iteration: 141/537, Loss: 0.0012534274719655514\n",
            "Iteration: 142/537, Loss: 0.018613537773489952\n",
            "Iteration: 143/537, Loss: 0.009344151243567467\n",
            "Iteration: 144/537, Loss: 0.010088198818266392\n",
            "Iteration: 145/537, Loss: 0.009489650838077068\n",
            "Iteration: 146/537, Loss: 0.06720136106014252\n",
            "Iteration: 147/537, Loss: 0.005768053699284792\n",
            "Iteration: 148/537, Loss: 0.04551248624920845\n",
            "Iteration: 149/537, Loss: 0.007191965356469154\n",
            "Iteration: 150/537, Loss: 0.0013416811125352979\n",
            "Iteration: 151/537, Loss: 0.001999804750084877\n",
            "Iteration: 152/537, Loss: 0.0006407301989383996\n",
            "Iteration: 153/537, Loss: 0.000970255583524704\n",
            "Iteration: 154/537, Loss: 0.00040672780596651137\n",
            "Iteration: 155/537, Loss: 0.002017466351389885\n",
            "Iteration: 156/537, Loss: 0.0019463342614471912\n",
            "Iteration: 157/537, Loss: 0.03848012164235115\n",
            "Iteration: 158/537, Loss: 0.0010324146132916212\n",
            "Iteration: 159/537, Loss: 0.0027702972292900085\n",
            "Iteration: 160/537, Loss: 0.0005636586574837565\n",
            "Iteration: 161/537, Loss: 0.0005534124211408198\n",
            "Iteration: 162/537, Loss: 0.0010233389912173152\n",
            "Iteration: 163/537, Loss: 0.0041705043986439705\n",
            "Iteration: 164/537, Loss: 0.003513513831421733\n",
            "Iteration: 165/537, Loss: 0.0036448047030717134\n",
            "Iteration: 166/537, Loss: 0.0010752296075224876\n",
            "Iteration: 167/537, Loss: 0.0010670009069144726\n",
            "Iteration: 168/537, Loss: 0.0021557447034865618\n",
            "Iteration: 169/537, Loss: 0.011211056262254715\n",
            "Iteration: 170/537, Loss: 0.01400752179324627\n",
            "Iteration: 171/537, Loss: 0.00020581751596182585\n",
            "Iteration: 172/537, Loss: 0.0017200749134644866\n",
            "Iteration: 173/537, Loss: 0.0016679317923262715\n",
            "Iteration: 174/537, Loss: 0.0025903021451085806\n",
            "Iteration: 175/537, Loss: 0.0026870309375226498\n",
            "Iteration: 176/537, Loss: 0.003945664037019014\n",
            "Iteration: 177/537, Loss: 0.004230624996125698\n",
            "Iteration: 178/537, Loss: 0.0008628697250969708\n",
            "Iteration: 179/537, Loss: 0.00821988470852375\n",
            "Iteration: 180/537, Loss: 0.001342367846518755\n",
            "Iteration: 181/537, Loss: 0.0003801246639341116\n",
            "Iteration: 182/537, Loss: 0.00028072716668248177\n",
            "Iteration: 183/537, Loss: 0.0006489675142802298\n",
            "Iteration: 184/537, Loss: 0.0017056509386748075\n",
            "Iteration: 185/537, Loss: 0.0010638140374794602\n",
            "Iteration: 186/537, Loss: 0.008107731118798256\n",
            "Iteration: 187/537, Loss: 0.0001499212085036561\n",
            "Iteration: 188/537, Loss: 0.00021725340047851205\n",
            "Iteration: 189/537, Loss: 0.04380342736840248\n",
            "Iteration: 190/537, Loss: 0.005429944023489952\n",
            "Iteration: 191/537, Loss: 0.0012346693547442555\n",
            "Iteration: 192/537, Loss: 0.00030046398751437664\n",
            "Iteration: 193/537, Loss: 0.0031977046746760607\n",
            "Iteration: 194/537, Loss: 0.004441859666258097\n",
            "Iteration: 195/537, Loss: 0.0015364880673587322\n",
            "Iteration: 196/537, Loss: 0.005850766319781542\n",
            "Iteration: 197/537, Loss: 0.0007801108295097947\n",
            "Iteration: 198/537, Loss: 0.0002684230566956103\n",
            "Iteration: 199/537, Loss: 0.00043200896470807493\n",
            "Iteration: 200/537, Loss: 0.0028139089699834585\n",
            "Iteration: 201/537, Loss: 0.006342681124806404\n",
            "Iteration: 202/537, Loss: 0.0005519462283700705\n",
            "Iteration: 203/537, Loss: 0.8228480219841003\n",
            "Iteration: 204/537, Loss: 0.00371105526573956\n",
            "Iteration: 205/537, Loss: 0.002633352531120181\n",
            "Iteration: 206/537, Loss: 0.0009786386508494616\n",
            "Iteration: 207/537, Loss: 0.0005462400149554014\n",
            "Iteration: 208/537, Loss: 0.003716918407008052\n",
            "Iteration: 209/537, Loss: 0.0006318444502539933\n",
            "Iteration: 210/537, Loss: 0.0016046268865466118\n",
            "Iteration: 211/537, Loss: 0.0013820384629070759\n",
            "Iteration: 212/537, Loss: 0.001572062959894538\n",
            "Iteration: 213/537, Loss: 0.01895221322774887\n",
            "Iteration: 214/537, Loss: 0.0011758458567783237\n",
            "Iteration: 215/537, Loss: 0.000660312594845891\n",
            "Iteration: 216/537, Loss: 0.001510005909949541\n",
            "Iteration: 217/537, Loss: 0.0036567419301718473\n",
            "Iteration: 218/537, Loss: 0.0004187566810287535\n",
            "Iteration: 219/537, Loss: 0.010691641829907894\n",
            "Iteration: 220/537, Loss: 0.0027416215743869543\n",
            "Iteration: 221/537, Loss: 0.002273014048114419\n",
            "Iteration: 222/537, Loss: 0.0010635023936629295\n",
            "Iteration: 223/537, Loss: 0.0020209786016494036\n",
            "Iteration: 224/537, Loss: 0.0006117945886217058\n",
            "Iteration: 225/537, Loss: 0.0009762567351572216\n",
            "Iteration: 226/537, Loss: 0.001322663389146328\n",
            "Iteration: 227/537, Loss: 0.003784346394240856\n",
            "Iteration: 228/537, Loss: 0.002660465659573674\n",
            "Iteration: 229/537, Loss: 0.001476628938689828\n",
            "Iteration: 230/537, Loss: 0.005515262484550476\n",
            "Iteration: 231/537, Loss: 0.000937595556024462\n",
            "Iteration: 232/537, Loss: 0.0009072170360013843\n",
            "Iteration: 233/537, Loss: 0.08592624217271805\n",
            "Iteration: 234/537, Loss: 0.0030878607649356127\n",
            "Iteration: 235/537, Loss: 0.0009268894209526479\n",
            "Iteration: 236/537, Loss: 0.05482963100075722\n",
            "Iteration: 237/537, Loss: 0.000979830278083682\n",
            "Iteration: 238/537, Loss: 0.0031200116500258446\n",
            "Iteration: 239/537, Loss: 0.007424239534884691\n",
            "Iteration: 240/537, Loss: 0.000744628778193146\n",
            "Iteration: 241/537, Loss: 0.0010387023212388158\n",
            "Iteration: 242/537, Loss: 0.0017062112456187606\n",
            "Iteration: 243/537, Loss: 0.007830439135432243\n",
            "Iteration: 244/537, Loss: 0.261410117149353\n",
            "Iteration: 245/537, Loss: 0.001063323812559247\n",
            "Iteration: 246/537, Loss: 0.0007388481171801686\n",
            "Iteration: 247/537, Loss: 0.00019772836822085083\n",
            "Iteration: 248/537, Loss: 0.004203991498798132\n",
            "Iteration: 249/537, Loss: 0.02082422561943531\n",
            "Iteration: 250/537, Loss: 0.0007453740108758211\n",
            "Iteration: 251/537, Loss: 0.0020337682217359543\n",
            "Iteration: 252/537, Loss: 0.0280788391828537\n",
            "Iteration: 253/537, Loss: 0.012421863153576851\n",
            "Iteration: 254/537, Loss: 0.0013714984524995089\n",
            "Iteration: 255/537, Loss: 0.001764348242431879\n",
            "Iteration: 256/537, Loss: 0.0002683696220628917\n",
            "Iteration: 257/537, Loss: 0.0022920609917491674\n",
            "Iteration: 258/537, Loss: 0.0006359181134030223\n",
            "Iteration: 259/537, Loss: 0.0012053208192810416\n",
            "Iteration: 260/537, Loss: 0.0011337528703734279\n",
            "Iteration: 261/537, Loss: 0.000579396728426218\n",
            "Iteration: 262/537, Loss: 0.0012199314078316092\n",
            "Iteration: 263/537, Loss: 0.005948090460151434\n",
            "Iteration: 264/537, Loss: 0.00559498043730855\n",
            "Iteration: 265/537, Loss: 0.0003598511393647641\n",
            "Iteration: 266/537, Loss: 0.0013120623771101236\n",
            "Iteration: 267/537, Loss: 0.003645933698862791\n",
            "Iteration: 268/537, Loss: 0.0011446706485003233\n",
            "Iteration: 269/537, Loss: 0.05333663523197174\n",
            "Iteration: 270/537, Loss: 0.0017126825405284762\n",
            "Iteration: 271/537, Loss: 0.37879103422164917\n",
            "Iteration: 272/537, Loss: 0.002510643098503351\n",
            "Iteration: 273/537, Loss: 0.0024960832670331\n",
            "Iteration: 274/537, Loss: 0.0015221531502902508\n",
            "Iteration: 275/537, Loss: 0.003037890186533332\n",
            "Iteration: 276/537, Loss: 0.007234970107674599\n",
            "Iteration: 277/537, Loss: 0.0011610533110797405\n",
            "Iteration: 278/537, Loss: 0.01047426089644432\n",
            "Iteration: 279/537, Loss: 0.002245914191007614\n",
            "Iteration: 280/537, Loss: 0.023226136341691017\n",
            "Iteration: 281/537, Loss: 0.0004158110823482275\n",
            "Iteration: 282/537, Loss: 0.0007973121828399599\n",
            "Iteration: 283/537, Loss: 0.0010277610272169113\n",
            "Iteration: 284/537, Loss: 0.001046189572662115\n",
            "Iteration: 285/537, Loss: 0.0018481730949133635\n",
            "Iteration: 286/537, Loss: 0.3291875720024109\n",
            "Iteration: 287/537, Loss: 0.0017085919389501214\n",
            "Iteration: 288/537, Loss: 0.0027672264259308577\n",
            "Iteration: 289/537, Loss: 0.05984754487872124\n",
            "Iteration: 290/537, Loss: 0.0019113277085125446\n",
            "Iteration: 291/537, Loss: 0.0032678009010851383\n",
            "Iteration: 292/537, Loss: 0.0037180185317993164\n",
            "Iteration: 293/537, Loss: 0.1373642534017563\n",
            "Iteration: 294/537, Loss: 0.006570347119122744\n",
            "Iteration: 295/537, Loss: 0.0011404320830479264\n",
            "Iteration: 296/537, Loss: 0.0015755877830088139\n",
            "Iteration: 297/537, Loss: 0.0011578910052776337\n",
            "Iteration: 298/537, Loss: 0.0012822728604078293\n",
            "Iteration: 299/537, Loss: 0.0012682379456236959\n",
            "Iteration: 300/537, Loss: 0.04182327911257744\n",
            "Iteration: 301/537, Loss: 0.007371861487627029\n",
            "Iteration: 302/537, Loss: 0.0015465478645637631\n",
            "Iteration: 303/537, Loss: 0.0027449335902929306\n",
            "Iteration: 304/537, Loss: 0.0007387943915091455\n",
            "Iteration: 305/537, Loss: 0.002484647324308753\n",
            "Iteration: 306/537, Loss: 0.0026198059786111116\n",
            "Iteration: 307/537, Loss: 0.0014676813734695315\n",
            "Iteration: 308/537, Loss: 0.0005149194621481001\n",
            "Iteration: 309/537, Loss: 0.0012631501303985715\n",
            "Iteration: 310/537, Loss: 0.00040615853504277766\n",
            "Iteration: 311/537, Loss: 0.32761257886886597\n",
            "Iteration: 312/537, Loss: 0.0004604713758453727\n",
            "Iteration: 313/537, Loss: 0.019189797341823578\n",
            "Iteration: 314/537, Loss: 0.10868823528289795\n",
            "Iteration: 315/537, Loss: 0.003110772231593728\n",
            "Iteration: 316/537, Loss: 0.0005518700927495956\n",
            "Iteration: 317/537, Loss: 0.0014174195239320397\n",
            "Iteration: 318/537, Loss: 0.002959344070404768\n",
            "Iteration: 319/537, Loss: 0.008520602248609066\n",
            "Iteration: 320/537, Loss: 0.0030487189069390297\n",
            "Iteration: 321/537, Loss: 0.0007713146624155343\n",
            "Iteration: 322/537, Loss: 0.0016031551640480757\n",
            "Iteration: 323/537, Loss: 0.0011057463707402349\n",
            "Iteration: 324/537, Loss: 0.0010458976030349731\n",
            "Iteration: 325/537, Loss: 0.010072835721075535\n",
            "Iteration: 326/537, Loss: 0.003254919545724988\n",
            "Iteration: 327/537, Loss: 0.0012967376969754696\n",
            "Iteration: 328/537, Loss: 0.001288457540795207\n",
            "Iteration: 329/537, Loss: 0.37549200654029846\n",
            "Iteration: 330/537, Loss: 0.0005683524650521576\n",
            "Iteration: 331/537, Loss: 0.0010556051274761558\n",
            "Iteration: 332/537, Loss: 0.0013295067474246025\n",
            "Iteration: 333/537, Loss: 0.0006132809794507921\n",
            "Iteration: 334/537, Loss: 0.0008069040486589074\n",
            "Iteration: 335/537, Loss: 0.0034889678936451674\n",
            "Iteration: 336/537, Loss: 0.003105737268924713\n",
            "Iteration: 337/537, Loss: 0.13706542551517487\n",
            "Iteration: 338/537, Loss: 0.0015988649101927876\n",
            "Iteration: 339/537, Loss: 0.0015150613617151976\n",
            "Iteration: 340/537, Loss: 0.0030236486345529556\n",
            "Iteration: 341/537, Loss: 0.002885929076001048\n",
            "Iteration: 342/537, Loss: 0.0006442478625103831\n",
            "Iteration: 343/537, Loss: 0.00019622444233391434\n",
            "Iteration: 344/537, Loss: 0.002997743198648095\n",
            "Iteration: 345/537, Loss: 0.0011374755995348096\n",
            "Iteration: 346/537, Loss: 0.036382224410772324\n",
            "Iteration: 347/537, Loss: 0.004116014577448368\n",
            "Iteration: 348/537, Loss: 0.003085632808506489\n",
            "Iteration: 349/537, Loss: 0.00775650842115283\n",
            "Iteration: 350/537, Loss: 0.0016378914006054401\n",
            "Iteration: 351/537, Loss: 0.011558894999325275\n",
            "Iteration: 352/537, Loss: 0.001771863317117095\n",
            "Iteration: 353/537, Loss: 0.0022077742032706738\n",
            "Iteration: 354/537, Loss: 0.0026976284570991993\n",
            "Iteration: 355/537, Loss: 0.00266467803157866\n",
            "Iteration: 356/537, Loss: 0.005122449714690447\n",
            "Iteration: 357/537, Loss: 0.00025928145623765886\n",
            "Iteration: 358/537, Loss: 0.0448114313185215\n",
            "Iteration: 359/537, Loss: 0.006027312483638525\n",
            "Iteration: 360/537, Loss: 0.0009719819063320756\n",
            "Iteration: 361/537, Loss: 0.10878510773181915\n",
            "Iteration: 362/537, Loss: 0.00406130775809288\n",
            "Iteration: 363/537, Loss: 0.005356647539883852\n",
            "Iteration: 364/537, Loss: 0.0012404529843479395\n",
            "Iteration: 365/537, Loss: 0.0028495867736637592\n",
            "Iteration: 366/537, Loss: 0.0017267994116991758\n",
            "Iteration: 367/537, Loss: 0.000984985614195466\n",
            "Iteration: 368/537, Loss: 0.002401944249868393\n",
            "Iteration: 369/537, Loss: 0.10856850445270538\n",
            "Iteration: 370/537, Loss: 0.0017730522667989135\n",
            "Iteration: 371/537, Loss: 0.0029809868428856134\n",
            "Iteration: 372/537, Loss: 0.0004968175780959427\n",
            "Iteration: 373/537, Loss: 0.004287471994757652\n",
            "Iteration: 374/537, Loss: 0.0008873229380697012\n",
            "Iteration: 375/537, Loss: 0.0722929835319519\n",
            "Iteration: 376/537, Loss: 0.0028801807202398777\n",
            "Iteration: 377/537, Loss: 0.002876496873795986\n",
            "Iteration: 378/537, Loss: 0.0006720342789776623\n",
            "Iteration: 379/537, Loss: 0.0023573837243020535\n",
            "Iteration: 380/537, Loss: 0.06815456598997116\n",
            "Iteration: 381/537, Loss: 0.0007399905589409173\n",
            "Iteration: 382/537, Loss: 0.004513266962021589\n",
            "Iteration: 383/537, Loss: 0.0012354128994047642\n",
            "Iteration: 384/537, Loss: 0.009077164344489574\n",
            "Iteration: 385/537, Loss: 0.019785964861512184\n",
            "Iteration: 386/537, Loss: 0.0011335425078868866\n",
            "Iteration: 387/537, Loss: 0.0005400735535658896\n",
            "Iteration: 388/537, Loss: 0.7946317195892334\n",
            "Iteration: 389/537, Loss: 0.003930423408746719\n",
            "Iteration: 390/537, Loss: 0.002175695728510618\n",
            "Iteration: 391/537, Loss: 0.184213325381279\n",
            "Iteration: 392/537, Loss: 0.004024574067443609\n",
            "Iteration: 393/537, Loss: 0.00044412841089069843\n",
            "Iteration: 394/537, Loss: 0.0011789300478994846\n",
            "Iteration: 395/537, Loss: 0.0747300460934639\n",
            "Iteration: 396/537, Loss: 0.0019059551414102316\n",
            "Iteration: 397/537, Loss: 0.0003973761631641537\n",
            "Iteration: 398/537, Loss: 0.0005335031892172992\n",
            "Iteration: 399/537, Loss: 0.0003691288293339312\n",
            "Iteration: 400/537, Loss: 0.0014665962662547827\n",
            "Iteration: 401/537, Loss: 0.0010785192716866732\n",
            "Iteration: 402/537, Loss: 0.013160346075892448\n",
            "Iteration: 403/537, Loss: 0.0006148430984467268\n",
            "Iteration: 404/537, Loss: 0.015896745026111603\n",
            "Iteration: 405/537, Loss: 0.0007614667993038893\n",
            "Iteration: 406/537, Loss: 0.003800229635089636\n",
            "Iteration: 407/537, Loss: 0.2050935924053192\n",
            "Iteration: 408/537, Loss: 0.002449352527037263\n",
            "Iteration: 409/537, Loss: 0.00030749401776120067\n",
            "Iteration: 410/537, Loss: 0.00033164137857966125\n",
            "Iteration: 411/537, Loss: 0.0016926184762269258\n",
            "Iteration: 412/537, Loss: 0.0009637841721996665\n",
            "Iteration: 413/537, Loss: 0.0012544606579467654\n",
            "Iteration: 414/537, Loss: 0.001940908026881516\n",
            "Iteration: 415/537, Loss: 0.0011699929600581527\n",
            "Iteration: 416/537, Loss: 0.014946348965168\n",
            "Iteration: 417/537, Loss: 0.0015796368243172765\n",
            "Iteration: 418/537, Loss: 0.1331600695848465\n",
            "Iteration: 419/537, Loss: 0.00030250707641243935\n",
            "Iteration: 420/537, Loss: 0.18063859641551971\n",
            "Iteration: 421/537, Loss: 0.004173170775175095\n",
            "Iteration: 422/537, Loss: 0.0003017428098246455\n",
            "Iteration: 423/537, Loss: 0.001004109624773264\n",
            "Iteration: 424/537, Loss: 0.0006428069900721312\n",
            "Iteration: 425/537, Loss: 0.011768899857997894\n",
            "Iteration: 426/537, Loss: 0.0013526137918233871\n",
            "Iteration: 427/537, Loss: 0.004256480373442173\n",
            "Iteration: 428/537, Loss: 0.0016866186633706093\n",
            "Iteration: 429/537, Loss: 0.0030416888184845448\n",
            "Iteration: 430/537, Loss: 0.00015892177179921418\n",
            "Iteration: 431/537, Loss: 0.0004515610053204\n",
            "Iteration: 432/537, Loss: 0.00044737057760357857\n",
            "Iteration: 433/537, Loss: 0.008965343236923218\n",
            "Iteration: 434/537, Loss: 0.0017801669891923666\n",
            "Iteration: 435/537, Loss: 0.10908292233943939\n",
            "Iteration: 436/537, Loss: 0.0005730889970436692\n",
            "Iteration: 437/537, Loss: 9.857967961579561e-05\n",
            "Iteration: 438/537, Loss: 0.0024807939771562815\n",
            "Iteration: 439/537, Loss: 8.502024138579145e-05\n",
            "Iteration: 440/537, Loss: 0.0004862505884375423\n",
            "Iteration: 441/537, Loss: 0.001574986265040934\n",
            "Iteration: 442/537, Loss: 0.008381076157093048\n",
            "Iteration: 443/537, Loss: 0.0004267297626938671\n",
            "Iteration: 444/537, Loss: 0.0005377082270570099\n",
            "Iteration: 445/537, Loss: 0.29173845052719116\n",
            "Iteration: 446/537, Loss: 0.0006251841550692916\n",
            "Iteration: 447/537, Loss: 0.0002387127751717344\n",
            "Iteration: 448/537, Loss: 1.0183616876602173\n",
            "Iteration: 449/537, Loss: 0.0009497866267338395\n",
            "Iteration: 450/537, Loss: 0.0030928244814276695\n",
            "Iteration: 451/537, Loss: 0.1355723738670349\n",
            "Iteration: 452/537, Loss: 0.005785045679658651\n",
            "Iteration: 453/537, Loss: 0.016536451876163483\n",
            "Iteration: 454/537, Loss: 0.010019449517130852\n",
            "Iteration: 455/537, Loss: 0.0003515531134326011\n",
            "Iteration: 456/537, Loss: 0.000733683817088604\n",
            "Iteration: 457/537, Loss: 0.0011549282353371382\n",
            "Iteration: 458/537, Loss: 0.00017044857668224722\n",
            "Iteration: 459/537, Loss: 0.0003063521580770612\n",
            "Iteration: 460/537, Loss: 0.004621642641723156\n",
            "Iteration: 461/537, Loss: 0.001050904393196106\n",
            "Iteration: 462/537, Loss: 0.05279943346977234\n",
            "Iteration: 463/537, Loss: 0.000367360538803041\n",
            "Iteration: 464/537, Loss: 0.000583270622882992\n",
            "Iteration: 465/537, Loss: 0.0007470499840565026\n",
            "Iteration: 466/537, Loss: 0.002786004450172186\n",
            "Iteration: 467/537, Loss: 0.0014084955910220742\n",
            "Iteration: 468/537, Loss: 0.0035854189191013575\n",
            "Iteration: 469/537, Loss: 0.01506895199418068\n",
            "Iteration: 470/537, Loss: 0.0012133585987612605\n",
            "Iteration: 471/537, Loss: 0.002674486953765154\n",
            "Iteration: 472/537, Loss: 0.0033021452836692333\n",
            "Iteration: 473/537, Loss: 0.0004807872464880347\n",
            "Iteration: 474/537, Loss: 0.002814758103340864\n",
            "Iteration: 475/537, Loss: 0.001363909337669611\n",
            "Iteration: 476/537, Loss: 0.0041430979035794735\n",
            "Iteration: 477/537, Loss: 0.005664031021296978\n",
            "Iteration: 478/537, Loss: 0.0008898270316421986\n",
            "Iteration: 479/537, Loss: 0.0035955344792455435\n",
            "Iteration: 480/537, Loss: 0.0006046472699381411\n",
            "Iteration: 481/537, Loss: 0.0002465631696395576\n",
            "Iteration: 482/537, Loss: 0.0005354469176381826\n",
            "Iteration: 483/537, Loss: 0.025309300050139427\n",
            "Iteration: 484/537, Loss: 0.001626651152037084\n",
            "Iteration: 485/537, Loss: 0.5691946744918823\n",
            "Iteration: 486/537, Loss: 0.0058551374822855\n",
            "Iteration: 487/537, Loss: 0.0035114458296447992\n",
            "Iteration: 488/537, Loss: 0.0006549548706971109\n",
            "Iteration: 489/537, Loss: 0.000626767345238477\n",
            "Iteration: 490/537, Loss: 0.0008846445707604289\n",
            "Iteration: 491/537, Loss: 0.00026533682830631733\n",
            "Iteration: 492/537, Loss: 0.015713253989815712\n",
            "Iteration: 493/537, Loss: 0.0012479063589125872\n",
            "Iteration: 494/537, Loss: 0.005314717069268227\n",
            "Iteration: 495/537, Loss: 0.027083689346909523\n",
            "Iteration: 496/537, Loss: 0.0015880733262747526\n",
            "Iteration: 497/537, Loss: 0.0011536515085026622\n",
            "Iteration: 498/537, Loss: 0.0005567839834839106\n",
            "Iteration: 499/537, Loss: 0.0003036257694475353\n",
            "Iteration: 500/537, Loss: 0.0010658365208655596\n",
            "Iteration: 501/537, Loss: 0.001898181508295238\n",
            "Iteration: 502/537, Loss: 0.000545913353562355\n",
            "Iteration: 503/537, Loss: 0.06600815057754517\n",
            "Iteration: 504/537, Loss: 0.08066235482692719\n",
            "Iteration: 505/537, Loss: 0.0009055756381712854\n",
            "Iteration: 506/537, Loss: 0.0003461222513578832\n",
            "Iteration: 507/537, Loss: 0.001149586634710431\n",
            "Iteration: 508/537, Loss: 0.0008741297060623765\n",
            "Iteration: 509/537, Loss: 0.014070922508835793\n",
            "Iteration: 510/537, Loss: 0.0007087602280080318\n",
            "Iteration: 511/537, Loss: 0.004480220377445221\n",
            "Iteration: 512/537, Loss: 0.0004336020792834461\n",
            "Iteration: 513/537, Loss: 0.002604988869279623\n",
            "Iteration: 514/537, Loss: 0.0011958512477576733\n",
            "Iteration: 515/537, Loss: 0.034308429807424545\n",
            "Iteration: 516/537, Loss: 0.008420683443546295\n",
            "Iteration: 517/537, Loss: 0.014781709760427475\n",
            "Iteration: 518/537, Loss: 0.0006839156267233193\n",
            "Iteration: 519/537, Loss: 0.001942052156664431\n",
            "Iteration: 520/537, Loss: 0.002016098704189062\n",
            "Iteration: 521/537, Loss: 0.0023311548866331577\n",
            "Iteration: 522/537, Loss: 0.012987976893782616\n",
            "Iteration: 523/537, Loss: 0.0002969792112708092\n",
            "Iteration: 524/537, Loss: 0.005989033728837967\n",
            "Iteration: 525/537, Loss: 0.06997982412576675\n",
            "Iteration: 526/537, Loss: 0.00021466263569891453\n",
            "Iteration: 527/537, Loss: 0.0014262720942497253\n",
            "Iteration: 528/537, Loss: 0.003780142869800329\n",
            "Iteration: 529/537, Loss: 0.001103409449569881\n",
            "Iteration: 530/537, Loss: 0.0014823856763541698\n",
            "Iteration: 531/537, Loss: 0.0006296548526734114\n",
            "Iteration: 532/537, Loss: 0.009776931256055832\n",
            "Iteration: 533/537, Loss: 0.2822997570037842\n",
            "Iteration: 534/537, Loss: 0.0009960142197087407\n",
            "Iteration: 535/537, Loss: 0.030157851055264473\n",
            "Iteration: 536/537, Loss: 0.0011487588053569198\n",
            "Iteration: 537/537, Loss: 0.031810566782951355\n",
            "Epoch 41 train loss: 0.022259297736996184\n",
            "Epoch 41 test loss: 0.0784364718706692\n",
            "-------------------------------------\n",
            "Iteration: 1/537, Loss: 0.0005880704266019166\n",
            "Iteration: 2/537, Loss: 0.004064739681780338\n",
            "Iteration: 3/537, Loss: 0.004838182590901852\n",
            "Iteration: 4/537, Loss: 0.0903673842549324\n",
            "Iteration: 5/537, Loss: 0.005756600294262171\n",
            "Iteration: 6/537, Loss: 0.0019881385378539562\n",
            "Iteration: 7/537, Loss: 0.00014609942445531487\n",
            "Iteration: 8/537, Loss: 0.002387362765148282\n",
            "Iteration: 9/537, Loss: 0.002940983744338155\n",
            "Iteration: 10/537, Loss: 0.0006366941379383206\n",
            "Iteration: 11/537, Loss: 0.000777952081989497\n",
            "Iteration: 12/537, Loss: 0.00020539567049127072\n",
            "Iteration: 13/537, Loss: 0.0003011752269230783\n",
            "Iteration: 14/537, Loss: 0.009713142178952694\n",
            "Iteration: 15/537, Loss: 0.0013853132259100676\n",
            "Iteration: 16/537, Loss: 0.01067021582275629\n",
            "Iteration: 17/537, Loss: 0.0026470343582332134\n",
            "Iteration: 18/537, Loss: 0.0037458189763128757\n",
            "Iteration: 19/537, Loss: 0.0010327899362891912\n",
            "Iteration: 20/537, Loss: 0.0006314433412626386\n",
            "Iteration: 21/537, Loss: 0.000919955491553992\n",
            "Iteration: 22/537, Loss: 0.0030452311038970947\n",
            "Iteration: 23/537, Loss: 0.0037139507476240396\n",
            "Iteration: 24/537, Loss: 0.002431245520710945\n",
            "Iteration: 25/537, Loss: 0.002385696629062295\n",
            "Iteration: 26/537, Loss: 0.092104971408844\n",
            "Iteration: 27/537, Loss: 0.00696532242000103\n",
            "Iteration: 28/537, Loss: 0.0008119648555293679\n",
            "Iteration: 29/537, Loss: 0.0001559315132908523\n",
            "Iteration: 30/537, Loss: 0.0022441998589783907\n",
            "Iteration: 31/537, Loss: 0.00702940858900547\n",
            "Iteration: 32/537, Loss: 0.0008713585557416081\n",
            "Iteration: 33/537, Loss: 0.0012383025605231524\n",
            "Iteration: 34/537, Loss: 0.20207427442073822\n",
            "Iteration: 35/537, Loss: 0.002080395584926009\n",
            "Iteration: 36/537, Loss: 0.0013201202964410186\n",
            "Iteration: 37/537, Loss: 0.008096729405224323\n",
            "Iteration: 38/537, Loss: 0.015383165329694748\n",
            "Iteration: 39/537, Loss: 0.0020212032832205296\n",
            "Iteration: 40/537, Loss: 0.002393235918134451\n",
            "Iteration: 41/537, Loss: 0.0020085256546735764\n",
            "Iteration: 42/537, Loss: 0.0023884596303105354\n",
            "Iteration: 43/537, Loss: 0.0015287166461348534\n",
            "Iteration: 44/537, Loss: 0.0004759682051371783\n",
            "Iteration: 45/537, Loss: 0.0008421695092692971\n",
            "Iteration: 46/537, Loss: 0.00025648484006524086\n",
            "Iteration: 47/537, Loss: 0.0068634990602731705\n",
            "Iteration: 48/537, Loss: 0.02492316998541355\n",
            "Iteration: 49/537, Loss: 0.011105927638709545\n",
            "Iteration: 50/537, Loss: 0.002179152797907591\n",
            "Iteration: 51/537, Loss: 0.0011019454104825854\n",
            "Iteration: 52/537, Loss: 0.0018355927895754576\n",
            "Iteration: 53/537, Loss: 0.0012885169126093388\n",
            "Iteration: 54/537, Loss: 0.006803401280194521\n",
            "Iteration: 55/537, Loss: 0.001547951833344996\n",
            "Iteration: 56/537, Loss: 0.0011433223262429237\n",
            "Iteration: 57/537, Loss: 0.018123339861631393\n",
            "Iteration: 58/537, Loss: 0.045295558869838715\n",
            "Iteration: 59/537, Loss: 0.0008116593235172331\n",
            "Iteration: 60/537, Loss: 0.0006155818700790405\n",
            "Iteration: 61/537, Loss: 0.0014056116342544556\n",
            "Iteration: 62/537, Loss: 0.004177441354840994\n",
            "Iteration: 63/537, Loss: 0.0008185835322365165\n",
            "Iteration: 64/537, Loss: 0.005434216000139713\n",
            "Iteration: 65/537, Loss: 0.0037437956780195236\n",
            "Iteration: 66/537, Loss: 0.0016379388980567455\n",
            "Iteration: 67/537, Loss: 1.7218743562698364\n",
            "Iteration: 68/537, Loss: 0.0019037302117794752\n",
            "Iteration: 69/537, Loss: 0.006141642574220896\n",
            "Iteration: 70/537, Loss: 0.006899305619299412\n",
            "Iteration: 71/537, Loss: 0.0021367690060287714\n",
            "Iteration: 72/537, Loss: 0.0009639851050451398\n",
            "Iteration: 73/537, Loss: 0.008490192703902721\n",
            "Iteration: 74/537, Loss: 0.003575671464204788\n",
            "Iteration: 75/537, Loss: 0.004426737315952778\n",
            "Iteration: 76/537, Loss: 0.0011073816567659378\n",
            "Iteration: 77/537, Loss: 0.001244556624442339\n",
            "Iteration: 78/537, Loss: 0.010860452428460121\n",
            "Iteration: 79/537, Loss: 0.0015069815563037992\n",
            "Iteration: 80/537, Loss: 0.0013791564851999283\n",
            "Iteration: 81/537, Loss: 0.0006609842530451715\n",
            "Iteration: 82/537, Loss: 0.01704215258359909\n",
            "Iteration: 83/537, Loss: 0.0029932595789432526\n",
            "Iteration: 84/537, Loss: 0.0007247310131788254\n",
            "Iteration: 85/537, Loss: 0.0013117552734911442\n",
            "Iteration: 86/537, Loss: 0.0009011350339278579\n",
            "Iteration: 87/537, Loss: 0.00185139081440866\n",
            "Iteration: 88/537, Loss: 0.001424010843038559\n",
            "Iteration: 89/537, Loss: 0.0018223299412056804\n",
            "Iteration: 90/537, Loss: 0.0021270643919706345\n",
            "Iteration: 91/537, Loss: 0.0014434057520702481\n",
            "Iteration: 92/537, Loss: 0.00017752371786627918\n",
            "Iteration: 93/537, Loss: 0.008301246911287308\n",
            "Iteration: 94/537, Loss: 0.0014295647852122784\n",
            "Iteration: 95/537, Loss: 0.005494552198797464\n",
            "Iteration: 96/537, Loss: 0.0018739851657301188\n",
            "Iteration: 97/537, Loss: 0.0043769460171461105\n",
            "Iteration: 98/537, Loss: 0.007771308533847332\n",
            "Iteration: 99/537, Loss: 0.0008317311876453459\n",
            "Iteration: 100/537, Loss: 0.0018566917860880494\n",
            "Iteration: 101/537, Loss: 0.004704710561782122\n",
            "Iteration: 102/537, Loss: 0.001190514420159161\n",
            "Iteration: 103/537, Loss: 0.001574980327859521\n",
            "Iteration: 104/537, Loss: 0.0005673260893672705\n",
            "Iteration: 105/537, Loss: 0.0027257774490863085\n",
            "Iteration: 106/537, Loss: 0.0020372834987938404\n",
            "Iteration: 107/537, Loss: 0.0013799601001664996\n",
            "Iteration: 108/537, Loss: 0.007584680803120136\n",
            "Iteration: 109/537, Loss: 0.003050881437957287\n",
            "Iteration: 110/537, Loss: 0.0006292309262789786\n",
            "Iteration: 111/537, Loss: 0.0007773840916343033\n",
            "Iteration: 112/537, Loss: 0.001491058967076242\n",
            "Iteration: 113/537, Loss: 0.0012669381685554981\n",
            "Iteration: 114/537, Loss: 0.003219078993424773\n",
            "Iteration: 115/537, Loss: 0.008109343238174915\n",
            "Iteration: 116/537, Loss: 0.0025052109267562628\n",
            "Iteration: 117/537, Loss: 0.002980416174978018\n",
            "Iteration: 118/537, Loss: 0.0009046858176589012\n",
            "Iteration: 119/537, Loss: 0.0017614222597330809\n",
            "Iteration: 120/537, Loss: 0.0004424037761054933\n",
            "Iteration: 121/537, Loss: 0.0006754911155439913\n",
            "Iteration: 122/537, Loss: 0.005753174424171448\n",
            "Iteration: 123/537, Loss: 0.0038546831347048283\n",
            "Iteration: 124/537, Loss: 0.002445571357384324\n",
            "Iteration: 125/537, Loss: 0.010784502141177654\n",
            "Iteration: 126/537, Loss: 0.0021568667143583298\n",
            "Iteration: 127/537, Loss: 0.0009101813193410635\n",
            "Iteration: 128/537, Loss: 0.003940864931792021\n",
            "Iteration: 129/537, Loss: 0.008500289171934128\n",
            "Iteration: 130/537, Loss: 0.0040151095017790794\n",
            "Iteration: 131/537, Loss: 0.0007541014347225428\n",
            "Iteration: 132/537, Loss: 0.004481285810470581\n",
            "Iteration: 133/537, Loss: 0.0010264451848343015\n",
            "Iteration: 134/537, Loss: 0.0004963716492056847\n",
            "Iteration: 135/537, Loss: 0.0020942934788763523\n",
            "Iteration: 136/537, Loss: 0.05999883636832237\n",
            "Iteration: 137/537, Loss: 0.0003768661408685148\n",
            "Iteration: 138/537, Loss: 0.0005367520498111844\n",
            "Iteration: 139/537, Loss: 0.0027862824499607086\n",
            "Iteration: 140/537, Loss: 0.00020171071810182184\n",
            "Iteration: 141/537, Loss: 0.0012877255212515593\n",
            "Iteration: 142/537, Loss: 0.0020435499027371407\n",
            "Iteration: 143/537, Loss: 0.1064133420586586\n",
            "Iteration: 144/537, Loss: 0.004628474824130535\n",
            "Iteration: 145/537, Loss: 0.05632524937391281\n",
            "Iteration: 146/537, Loss: 0.0005348350969143212\n",
            "Iteration: 147/537, Loss: 0.0011342520592734218\n",
            "Iteration: 148/537, Loss: 0.0016766926273703575\n",
            "Iteration: 149/537, Loss: 0.0016678620595484972\n",
            "Iteration: 150/537, Loss: 0.0014830436557531357\n",
            "Iteration: 151/537, Loss: 0.0037003362085670233\n",
            "Iteration: 152/537, Loss: 0.000634229276329279\n",
            "Iteration: 153/537, Loss: 0.001219886471517384\n",
            "Iteration: 154/537, Loss: 0.00012161278573330492\n",
            "Iteration: 155/537, Loss: 0.0005615644040517509\n",
            "Iteration: 156/537, Loss: 0.0035107643343508244\n",
            "Iteration: 157/537, Loss: 0.002524123527109623\n",
            "Iteration: 158/537, Loss: 0.0002441475517116487\n",
            "Iteration: 159/537, Loss: 0.0015293217729777098\n",
            "Iteration: 160/537, Loss: 0.001909581944346428\n",
            "Iteration: 161/537, Loss: 0.00014791949070058763\n",
            "Iteration: 162/537, Loss: 0.0004156752838753164\n",
            "Iteration: 163/537, Loss: 0.0005739781772717834\n",
            "Iteration: 164/537, Loss: 0.00041856360621750355\n",
            "Iteration: 165/537, Loss: 0.001534879906103015\n",
            "Iteration: 166/537, Loss: 0.001487063942477107\n",
            "Iteration: 167/537, Loss: 0.00038391019916161895\n",
            "Iteration: 168/537, Loss: 0.007301407866179943\n",
            "Iteration: 169/537, Loss: 0.001840119482949376\n",
            "Iteration: 170/537, Loss: 0.0007928062113933265\n",
            "Iteration: 171/537, Loss: 0.008377603255212307\n",
            "Iteration: 172/537, Loss: 0.00026499343221075833\n",
            "Iteration: 173/537, Loss: 0.00038496649358421564\n",
            "Iteration: 174/537, Loss: 0.0004109929723199457\n",
            "Iteration: 175/537, Loss: 0.00124204414896667\n",
            "Iteration: 176/537, Loss: 0.00973642710596323\n",
            "Iteration: 177/537, Loss: 0.002881075255572796\n",
            "Iteration: 178/537, Loss: 0.00042357464553788304\n",
            "Iteration: 179/537, Loss: 0.0010832073166966438\n",
            "Iteration: 180/537, Loss: 0.0005450848257169127\n",
            "Iteration: 181/537, Loss: 0.0008503768476657569\n",
            "Iteration: 182/537, Loss: 0.0003753092314582318\n",
            "Iteration: 183/537, Loss: 0.00028060059412382543\n",
            "Iteration: 184/537, Loss: 0.00043130788253620267\n",
            "Iteration: 185/537, Loss: 0.00038918497739359736\n",
            "Iteration: 186/537, Loss: 0.0003056968853343278\n",
            "Iteration: 187/537, Loss: 0.0006412447546608746\n",
            "Iteration: 188/537, Loss: 0.0005561714060604572\n",
            "Iteration: 189/537, Loss: 0.00389196933247149\n",
            "Iteration: 190/537, Loss: 0.002430518390610814\n",
            "Iteration: 191/537, Loss: 0.0002552613150328398\n",
            "Iteration: 192/537, Loss: 0.0015551226679235697\n",
            "Iteration: 193/537, Loss: 0.0008114900556392968\n",
            "Iteration: 194/537, Loss: 0.0011096755042672157\n",
            "Iteration: 195/537, Loss: 0.05718616396188736\n",
            "Iteration: 196/537, Loss: 0.0006565675721503794\n",
            "Iteration: 197/537, Loss: 0.003973362967371941\n",
            "Iteration: 198/537, Loss: 0.18005025386810303\n",
            "Iteration: 199/537, Loss: 0.008498432114720345\n",
            "Iteration: 200/537, Loss: 0.00042283773655071855\n",
            "Iteration: 201/537, Loss: 0.0004110243171453476\n",
            "Iteration: 202/537, Loss: 0.0005831732996739447\n",
            "Iteration: 203/537, Loss: 0.005151405464857817\n",
            "Iteration: 204/537, Loss: 0.005841981619596481\n",
            "Iteration: 205/537, Loss: 0.07322517782449722\n",
            "Iteration: 206/537, Loss: 0.0006159131298772991\n",
            "Iteration: 207/537, Loss: 0.00131605367641896\n",
            "Iteration: 208/537, Loss: 0.005772698670625687\n",
            "Iteration: 209/537, Loss: 0.0005959898699074984\n",
            "Iteration: 210/537, Loss: 0.001200429629534483\n",
            "Iteration: 211/537, Loss: 0.0004019691259600222\n",
            "Iteration: 212/537, Loss: 0.0014673526166006923\n",
            "Iteration: 213/537, Loss: 0.00023239826259668916\n",
            "Iteration: 214/537, Loss: 0.06612514704465866\n",
            "Iteration: 215/537, Loss: 0.0005358608905225992\n",
            "Iteration: 216/537, Loss: 0.0009082247270271182\n",
            "Iteration: 217/537, Loss: 0.0007532448507845402\n",
            "Iteration: 218/537, Loss: 0.0024569856468588114\n",
            "Iteration: 219/537, Loss: 0.002685649087652564\n",
            "Iteration: 220/537, Loss: 0.0030850267503410578\n",
            "Iteration: 221/537, Loss: 0.00350125040858984\n",
            "Iteration: 222/537, Loss: 0.00025127053959295154\n",
            "Iteration: 223/537, Loss: 0.00031759412377141416\n",
            "Iteration: 224/537, Loss: 0.0017602454172447324\n",
            "Iteration: 225/537, Loss: 0.0004763180040754378\n",
            "Iteration: 226/537, Loss: 0.010666491463780403\n",
            "Iteration: 227/537, Loss: 0.0036164219491183758\n",
            "Iteration: 228/537, Loss: 0.003925813362002373\n",
            "Iteration: 229/537, Loss: 0.009876830503344536\n",
            "Iteration: 230/537, Loss: 0.0003665017138700932\n",
            "Iteration: 231/537, Loss: 0.0003254542825743556\n",
            "Iteration: 232/537, Loss: 0.01836501620709896\n",
            "Iteration: 233/537, Loss: 0.0005850721499882638\n",
            "Iteration: 234/537, Loss: 0.002468554303050041\n",
            "Iteration: 235/537, Loss: 0.0010926835238933563\n",
            "Iteration: 236/537, Loss: 0.0027876868844032288\n",
            "Iteration: 237/537, Loss: 0.0002624634944368154\n",
            "Iteration: 238/537, Loss: 0.00048469420289620757\n",
            "Iteration: 239/537, Loss: 0.012078413739800453\n",
            "Iteration: 240/537, Loss: 0.005243134219199419\n",
            "Iteration: 241/537, Loss: 0.008925138041377068\n",
            "Iteration: 242/537, Loss: 0.0013614380732178688\n",
            "Iteration: 243/537, Loss: 0.000304237415548414\n",
            "Iteration: 244/537, Loss: 0.013571921736001968\n",
            "Iteration: 245/537, Loss: 0.0003883388126268983\n",
            "Iteration: 246/537, Loss: 0.004428362473845482\n",
            "Iteration: 247/537, Loss: 0.0008614850812591612\n",
            "Iteration: 248/537, Loss: 0.0052113342098891735\n",
            "Iteration: 249/537, Loss: 0.0008454245980829\n",
            "Iteration: 250/537, Loss: 0.0004995534545741975\n",
            "Iteration: 251/537, Loss: 0.05640621855854988\n",
            "Iteration: 252/537, Loss: 0.0004385120701044798\n",
            "Iteration: 253/537, Loss: 0.00036826019641011953\n",
            "Iteration: 254/537, Loss: 0.01692783087491989\n",
            "Iteration: 255/537, Loss: 0.0010516205802559853\n",
            "Iteration: 256/537, Loss: 0.0007878952892497182\n",
            "Iteration: 257/537, Loss: 0.004333014599978924\n",
            "Iteration: 258/537, Loss: 0.0009592847200110555\n",
            "Iteration: 259/537, Loss: 0.003567569423466921\n",
            "Iteration: 260/537, Loss: 0.0015894980169832706\n",
            "Iteration: 261/537, Loss: 0.0005820741062052548\n",
            "Iteration: 262/537, Loss: 0.00046467833453789353\n",
            "Iteration: 263/537, Loss: 0.000885295681655407\n",
            "Iteration: 264/537, Loss: 0.00355670228600502\n",
            "Iteration: 265/537, Loss: 0.0004921744111925364\n",
            "Iteration: 266/537, Loss: 0.00443942891433835\n",
            "Iteration: 267/537, Loss: 0.005267939064651728\n",
            "Iteration: 268/537, Loss: 0.0012743198312819004\n",
            "Iteration: 269/537, Loss: 0.005403102841228247\n",
            "Iteration: 270/537, Loss: 0.0019723589066416025\n",
            "Iteration: 271/537, Loss: 0.0019894565921276808\n",
            "Iteration: 272/537, Loss: 0.0019732932560145855\n",
            "Iteration: 273/537, Loss: 0.002907803514972329\n",
            "Iteration: 274/537, Loss: 0.010259883478283882\n",
            "Iteration: 275/537, Loss: 0.00018865146557800472\n",
            "Iteration: 276/537, Loss: 0.0015577676240354776\n",
            "Iteration: 277/537, Loss: 0.0017556267557665706\n",
            "Iteration: 278/537, Loss: 0.00015979979070834816\n",
            "Iteration: 279/537, Loss: 0.0026821033097803593\n",
            "Iteration: 280/537, Loss: 0.00022849378001410514\n",
            "Iteration: 281/537, Loss: 0.0014013029867783189\n",
            "Iteration: 282/537, Loss: 0.004250640515238047\n",
            "Iteration: 283/537, Loss: 0.007768403273075819\n",
            "Iteration: 284/537, Loss: 0.000350516609614715\n",
            "Iteration: 285/537, Loss: 0.0004710674984380603\n",
            "Iteration: 286/537, Loss: 0.0002784977841656655\n",
            "Iteration: 287/537, Loss: 0.0007072922307997942\n",
            "Iteration: 288/537, Loss: 0.0005099195404909551\n",
            "Iteration: 289/537, Loss: 0.0007547384011559188\n",
            "Iteration: 290/537, Loss: 0.0009279801743105054\n",
            "Iteration: 291/537, Loss: 0.027320295572280884\n",
            "Iteration: 292/537, Loss: 0.000779365305788815\n",
            "Iteration: 293/537, Loss: 0.004220536444336176\n",
            "Iteration: 294/537, Loss: 0.052649326622486115\n",
            "Iteration: 295/537, Loss: 0.000644143670797348\n",
            "Iteration: 296/537, Loss: 0.007716994732618332\n",
            "Iteration: 297/537, Loss: 0.09411976486444473\n",
            "Iteration: 298/537, Loss: 0.00017700334137771279\n",
            "Iteration: 299/537, Loss: 0.00014993699733167887\n",
            "Iteration: 300/537, Loss: 1.0317634344100952\n",
            "Iteration: 301/537, Loss: 0.02153676375746727\n",
            "Iteration: 302/537, Loss: 0.0008171707158908248\n",
            "Iteration: 303/537, Loss: 0.2404424548149109\n",
            "Iteration: 304/537, Loss: 0.002637169323861599\n",
            "Iteration: 305/537, Loss: 0.0020674774423241615\n",
            "Iteration: 306/537, Loss: 0.10852444171905518\n",
            "Iteration: 307/537, Loss: 0.0017971040215343237\n",
            "Iteration: 308/537, Loss: 0.002955702133476734\n",
            "Iteration: 309/537, Loss: 0.0002266662777401507\n",
            "Iteration: 310/537, Loss: 0.006370706483721733\n",
            "Iteration: 311/537, Loss: 0.0017138505354523659\n",
            "Iteration: 312/537, Loss: 0.0009785545989871025\n",
            "Iteration: 313/537, Loss: 0.007917091250419617\n",
            "Iteration: 314/537, Loss: 0.00478964252397418\n",
            "Iteration: 315/537, Loss: 0.004343483131378889\n",
            "Iteration: 316/537, Loss: 0.008149266242980957\n",
            "Iteration: 317/537, Loss: 0.0013465378433465958\n",
            "Iteration: 318/537, Loss: 0.000895546399988234\n",
            "Iteration: 319/537, Loss: 0.0010399440070614219\n",
            "Iteration: 320/537, Loss: 0.0007821149192750454\n",
            "Iteration: 321/537, Loss: 0.0014735489385202527\n",
            "Iteration: 322/537, Loss: 0.07236635684967041\n",
            "Iteration: 323/537, Loss: 0.005851465277373791\n",
            "Iteration: 324/537, Loss: 0.0007793210097588599\n",
            "Iteration: 325/537, Loss: 0.0018802797421813011\n",
            "Iteration: 326/537, Loss: 0.005132860038429499\n",
            "Iteration: 327/537, Loss: 0.0028559917118400335\n",
            "Iteration: 328/537, Loss: 0.1172449141740799\n",
            "Iteration: 329/537, Loss: 0.000318558159051463\n",
            "Iteration: 330/537, Loss: 0.017753321677446365\n",
            "Iteration: 331/537, Loss: 0.00522826611995697\n",
            "Iteration: 332/537, Loss: 0.0017652728129178286\n",
            "Iteration: 333/537, Loss: 0.03156385198235512\n",
            "Iteration: 334/537, Loss: 0.0013637610245496035\n",
            "Iteration: 335/537, Loss: 0.0009849013295024633\n",
            "Iteration: 336/537, Loss: 0.1624179184436798\n",
            "Iteration: 337/537, Loss: 0.0036501181311905384\n",
            "Iteration: 338/537, Loss: 0.0008425453561358154\n",
            "Iteration: 339/537, Loss: 0.0024837865494191647\n",
            "Iteration: 340/537, Loss: 0.0004401207552291453\n",
            "Iteration: 341/537, Loss: 0.00034261768450960517\n",
            "Iteration: 342/537, Loss: 0.0014612623490393162\n",
            "Iteration: 343/537, Loss: 0.0026099744718521833\n",
            "Iteration: 344/537, Loss: 0.002630452159792185\n",
            "Iteration: 345/537, Loss: 0.006875548046082258\n",
            "Iteration: 346/537, Loss: 0.005511199589818716\n",
            "Iteration: 347/537, Loss: 0.014830868691205978\n",
            "Iteration: 348/537, Loss: 0.007682952098548412\n",
            "Iteration: 349/537, Loss: 0.0018632940482348204\n",
            "Iteration: 350/537, Loss: 0.005160831846296787\n",
            "Iteration: 351/537, Loss: 0.001670733792707324\n",
            "Iteration: 352/537, Loss: 0.001883636461570859\n",
            "Iteration: 353/537, Loss: 0.0004172399640083313\n",
            "Iteration: 354/537, Loss: 0.0029292500112205744\n",
            "Iteration: 355/537, Loss: 0.0009480188600718975\n",
            "Iteration: 356/537, Loss: 0.0008172289235517383\n",
            "Iteration: 357/537, Loss: 0.002259290311485529\n",
            "Iteration: 358/537, Loss: 0.0016515199095010757\n",
            "Iteration: 359/537, Loss: 0.0006105299107730389\n",
            "Iteration: 360/537, Loss: 0.001078460831195116\n",
            "Iteration: 361/537, Loss: 0.00041193643119186163\n",
            "Iteration: 362/537, Loss: 0.000560558692086488\n",
            "Iteration: 363/537, Loss: 0.0029956535436213017\n",
            "Iteration: 364/537, Loss: 0.0039720917120575905\n",
            "Iteration: 365/537, Loss: 0.0010102239903062582\n",
            "Iteration: 366/537, Loss: 0.0006697959033772349\n",
            "Iteration: 367/537, Loss: 0.0009917211718857288\n",
            "Iteration: 368/537, Loss: 0.0014350072015076876\n",
            "Iteration: 369/537, Loss: 0.0011092774802818894\n",
            "Iteration: 370/537, Loss: 0.0007049678824841976\n",
            "Iteration: 371/537, Loss: 0.0009646770777180791\n",
            "Iteration: 372/537, Loss: 0.24935010075569153\n",
            "Iteration: 373/537, Loss: 0.004967387765645981\n",
            "Iteration: 374/537, Loss: 0.002342434134334326\n",
            "Iteration: 375/537, Loss: 0.0004286923795007169\n",
            "Iteration: 376/537, Loss: 0.0012343155685812235\n",
            "Iteration: 377/537, Loss: 0.0032005885150283575\n",
            "Iteration: 378/537, Loss: 0.0027398192323744297\n",
            "Iteration: 379/537, Loss: 0.0010291642975062132\n",
            "Iteration: 380/537, Loss: 0.00836529303342104\n",
            "Iteration: 381/537, Loss: 0.0014835416804999113\n",
            "Iteration: 382/537, Loss: 0.0014940579421818256\n",
            "Iteration: 383/537, Loss: 0.0014407823327928782\n",
            "Iteration: 384/537, Loss: 0.005894611589610577\n",
            "Iteration: 385/537, Loss: 0.0012113337870687246\n",
            "Iteration: 386/537, Loss: 0.000583894841838628\n",
            "Iteration: 387/537, Loss: 0.00194900156930089\n",
            "Iteration: 388/537, Loss: 0.00017863333050627261\n",
            "Iteration: 389/537, Loss: 0.0005401140078902245\n",
            "Iteration: 390/537, Loss: 0.0785132497549057\n",
            "Iteration: 391/537, Loss: 0.001860946649685502\n",
            "Iteration: 392/537, Loss: 0.023818034678697586\n",
            "Iteration: 393/537, Loss: 0.0029992677737027407\n",
            "Iteration: 394/537, Loss: 0.00066786800744012\n",
            "Iteration: 395/537, Loss: 0.0009331100736744702\n",
            "Iteration: 396/537, Loss: 0.0016037963796406984\n",
            "Iteration: 397/537, Loss: 0.3371656537055969\n",
            "Iteration: 398/537, Loss: 0.009506385773420334\n",
            "Iteration: 399/537, Loss: 0.14744895696640015\n",
            "Iteration: 400/537, Loss: 0.00084688636707142\n",
            "Iteration: 401/537, Loss: 0.0009540275204926729\n",
            "Iteration: 402/537, Loss: 0.006328384391963482\n",
            "Iteration: 403/537, Loss: 0.01919098012149334\n",
            "Iteration: 404/537, Loss: 0.002590093994513154\n",
            "Iteration: 405/537, Loss: 0.0019207354635000229\n",
            "Iteration: 406/537, Loss: 0.0023335376754403114\n",
            "Iteration: 407/537, Loss: 0.0004632847267203033\n",
            "Iteration: 408/537, Loss: 0.0014741402119398117\n",
            "Iteration: 409/537, Loss: 0.0031211189925670624\n",
            "Iteration: 410/537, Loss: 0.0004030066484119743\n",
            "Iteration: 411/537, Loss: 0.0008830445585772395\n",
            "Iteration: 412/537, Loss: 0.0008846735581755638\n",
            "Iteration: 413/537, Loss: 0.05825939029455185\n",
            "Iteration: 414/537, Loss: 0.000916810764465481\n",
            "Iteration: 415/537, Loss: 0.002221023663878441\n",
            "Iteration: 416/537, Loss: 0.0020243418402969837\n",
            "Iteration: 417/537, Loss: 0.006143930833786726\n",
            "Iteration: 418/537, Loss: 0.0038745866622775793\n",
            "Iteration: 419/537, Loss: 0.000812493497505784\n",
            "Iteration: 420/537, Loss: 0.0005526008317247033\n",
            "Iteration: 421/537, Loss: 0.005603168159723282\n",
            "Iteration: 422/537, Loss: 0.0015809733886271715\n",
            "Iteration: 423/537, Loss: 0.002953876741230488\n",
            "Iteration: 424/537, Loss: 0.004748941864818335\n",
            "Iteration: 425/537, Loss: 0.0015437777619808912\n",
            "Iteration: 426/537, Loss: 0.0022578872740268707\n",
            "Iteration: 427/537, Loss: 0.011643164791166782\n",
            "Iteration: 428/537, Loss: 0.015927452594041824\n",
            "Iteration: 429/537, Loss: 0.320585697889328\n",
            "Iteration: 430/537, Loss: 0.003494454547762871\n",
            "Iteration: 431/537, Loss: 0.0037742999847978354\n",
            "Iteration: 432/537, Loss: 0.00033709986018948257\n",
            "Iteration: 433/537, Loss: 0.0029808860272169113\n",
            "Iteration: 434/537, Loss: 0.0024898520205169916\n",
            "Iteration: 435/537, Loss: 0.06333064287900925\n",
            "Iteration: 436/537, Loss: 0.002482227748259902\n",
            "Iteration: 437/537, Loss: 0.004683103412389755\n",
            "Iteration: 438/537, Loss: 0.0075923483818769455\n",
            "Iteration: 439/537, Loss: 0.009560315869748592\n",
            "Iteration: 440/537, Loss: 0.019074492156505585\n",
            "Iteration: 441/537, Loss: 0.006043697707355022\n",
            "Iteration: 442/537, Loss: 0.010228300467133522\n",
            "Iteration: 443/537, Loss: 0.08924169838428497\n",
            "Iteration: 444/537, Loss: 0.0034891485702246428\n",
            "Iteration: 445/537, Loss: 0.0019899592734873295\n",
            "Iteration: 446/537, Loss: 0.0012002461589872837\n",
            "Iteration: 447/537, Loss: 0.0005947103491052985\n",
            "Iteration: 448/537, Loss: 0.8881977796554565\n",
            "Iteration: 449/537, Loss: 0.010052712634205818\n",
            "Iteration: 450/537, Loss: 0.0026469812728464603\n",
            "Iteration: 451/537, Loss: 0.0015707938000559807\n",
            "Iteration: 452/537, Loss: 0.004805670585483313\n",
            "Iteration: 453/537, Loss: 0.0021127460058778524\n",
            "Iteration: 454/537, Loss: 0.0037190106231719255\n",
            "Iteration: 455/537, Loss: 0.0024982925970107317\n",
            "Iteration: 456/537, Loss: 0.002866206457838416\n",
            "Iteration: 457/537, Loss: 0.0004390172543935478\n",
            "Iteration: 458/537, Loss: 0.0007604032871313393\n",
            "Iteration: 459/537, Loss: 0.0016049840487539768\n",
            "Iteration: 460/537, Loss: 0.0025361909065395594\n",
            "Iteration: 461/537, Loss: 0.00644292077049613\n",
            "Iteration: 462/537, Loss: 0.0015132296830415726\n",
            "Iteration: 463/537, Loss: 0.0007622161065228283\n",
            "Iteration: 464/537, Loss: 0.002126681851223111\n",
            "Iteration: 465/537, Loss: 0.002156007569283247\n",
            "Iteration: 466/537, Loss: 0.004415726754814386\n",
            "Iteration: 467/537, Loss: 0.007084069773554802\n",
            "Iteration: 468/537, Loss: 0.001573157962411642\n",
            "Iteration: 469/537, Loss: 0.0024056744296103716\n",
            "Iteration: 470/537, Loss: 0.004045519977807999\n",
            "Iteration: 471/537, Loss: 0.004770366474986076\n",
            "Iteration: 472/537, Loss: 0.0037090801633894444\n",
            "Iteration: 473/537, Loss: 0.0548877976834774\n",
            "Iteration: 474/537, Loss: 0.0023226586636155844\n",
            "Iteration: 475/537, Loss: 0.0014595077373087406\n",
            "Iteration: 476/537, Loss: 0.0018124328926205635\n",
            "Iteration: 477/537, Loss: 0.0017662079771980643\n",
            "Iteration: 478/537, Loss: 0.006874657701700926\n",
            "Iteration: 479/537, Loss: 0.01422533392906189\n",
            "Iteration: 480/537, Loss: 0.0030430175829678774\n",
            "Iteration: 481/537, Loss: 0.006010745652019978\n",
            "Iteration: 482/537, Loss: 0.004302319139242172\n",
            "Iteration: 483/537, Loss: 0.0005329598789103329\n",
            "Iteration: 484/537, Loss: 0.05254807695746422\n",
            "Iteration: 485/537, Loss: 0.0007887886604294181\n",
            "Iteration: 486/537, Loss: 0.016629334539175034\n",
            "Iteration: 487/537, Loss: 0.0015523082111030817\n",
            "Iteration: 488/537, Loss: 0.0002935061347670853\n",
            "Iteration: 489/537, Loss: 0.00964648462831974\n",
            "Iteration: 490/537, Loss: 0.004562826827168465\n",
            "Iteration: 491/537, Loss: 0.003916061483323574\n",
            "Iteration: 492/537, Loss: 0.0012628643307834864\n",
            "Iteration: 493/537, Loss: 0.010868591256439686\n",
            "Iteration: 494/537, Loss: 0.0006765025900676847\n",
            "Iteration: 495/537, Loss: 0.0008350625867024064\n",
            "Iteration: 496/537, Loss: 0.0021138263400644064\n",
            "Iteration: 497/537, Loss: 0.0006084891501814127\n",
            "Iteration: 498/537, Loss: 0.013485947623848915\n",
            "Iteration: 499/537, Loss: 0.0002391287125647068\n",
            "Iteration: 500/537, Loss: 0.013389931991696358\n",
            "Iteration: 501/537, Loss: 0.0034841326996684074\n",
            "Iteration: 502/537, Loss: 0.0006532385013997555\n",
            "Iteration: 503/537, Loss: 0.0016145467525348067\n",
            "Iteration: 504/537, Loss: 0.0033672030549496412\n",
            "Iteration: 505/537, Loss: 0.0004459468473214656\n",
            "Iteration: 506/537, Loss: 0.001309468294493854\n",
            "Iteration: 507/537, Loss: 0.0016171454917639494\n",
            "Iteration: 508/537, Loss: 0.0004643519641831517\n",
            "Iteration: 509/537, Loss: 0.0006753497291356325\n",
            "Iteration: 510/537, Loss: 0.017604734748601913\n",
            "Iteration: 511/537, Loss: 0.002298976294696331\n",
            "Iteration: 512/537, Loss: 0.0002686175284907222\n",
            "Iteration: 513/537, Loss: 0.43557503819465637\n",
            "Iteration: 514/537, Loss: 0.004867382813245058\n",
            "Iteration: 515/537, Loss: 0.0017536759842187166\n",
            "Iteration: 516/537, Loss: 0.00207692664116621\n",
            "Iteration: 517/537, Loss: 0.0018443395383656025\n",
            "Iteration: 518/537, Loss: 0.007135005667805672\n",
            "Iteration: 519/537, Loss: 0.0017557532992213964\n",
            "Iteration: 520/537, Loss: 0.00370976934209466\n",
            "Iteration: 521/537, Loss: 0.01798848807811737\n",
            "Iteration: 522/537, Loss: 0.0006873923121020198\n",
            "Iteration: 523/537, Loss: 0.0006620421772822738\n",
            "Iteration: 524/537, Loss: 0.008184686303138733\n",
            "Iteration: 525/537, Loss: 0.001786220003850758\n",
            "Iteration: 526/537, Loss: 0.0008416865603066981\n",
            "Iteration: 527/537, Loss: 0.0001975039194803685\n",
            "Iteration: 528/537, Loss: 0.0005642076721414924\n",
            "Iteration: 529/537, Loss: 0.001930894679389894\n",
            "Iteration: 530/537, Loss: 0.01374133862555027\n",
            "Iteration: 531/537, Loss: 0.002051349962130189\n",
            "Iteration: 532/537, Loss: 0.005152540281414986\n",
            "Iteration: 533/537, Loss: 0.0032309184316545725\n",
            "Iteration: 534/537, Loss: 0.006668722257018089\n",
            "Iteration: 535/537, Loss: 0.0013655511429533362\n",
            "Iteration: 536/537, Loss: 0.013698245398700237\n",
            "Iteration: 537/537, Loss: 1.2044599056243896\n",
            "Epoch 42 train loss: 0.019224169135826\n",
            "Epoch 42 test loss: 0.06120703452788466\n",
            "-------------------------------------\n",
            "Iteration: 1/537, Loss: 0.00016893133579287678\n",
            "Iteration: 2/537, Loss: 0.001150211552157998\n",
            "Iteration: 3/537, Loss: 0.0007772514363750815\n",
            "Iteration: 4/537, Loss: 0.0006741004181094468\n",
            "Iteration: 5/537, Loss: 0.007705978117883205\n",
            "Iteration: 6/537, Loss: 0.0013800710439682007\n",
            "Iteration: 7/537, Loss: 0.000318037171382457\n",
            "Iteration: 8/537, Loss: 0.002987573854625225\n",
            "Iteration: 9/537, Loss: 0.0017414353787899017\n",
            "Iteration: 10/537, Loss: 0.001276380498893559\n",
            "Iteration: 11/537, Loss: 0.0005628466024063528\n",
            "Iteration: 12/537, Loss: 0.0007120775408111513\n",
            "Iteration: 13/537, Loss: 0.00034442730247974396\n",
            "Iteration: 14/537, Loss: 0.0016566989943385124\n",
            "Iteration: 15/537, Loss: 0.000779313500970602\n",
            "Iteration: 16/537, Loss: 0.00826373603194952\n",
            "Iteration: 17/537, Loss: 0.003048979677259922\n",
            "Iteration: 18/537, Loss: 0.0023654361721128225\n",
            "Iteration: 19/537, Loss: 0.0021202797070145607\n",
            "Iteration: 20/537, Loss: 0.049309711903333664\n",
            "Iteration: 21/537, Loss: 0.001013569999486208\n",
            "Iteration: 22/537, Loss: 0.011193104088306427\n",
            "Iteration: 23/537, Loss: 0.0005674036801792681\n",
            "Iteration: 24/537, Loss: 0.00031364927417598665\n",
            "Iteration: 25/537, Loss: 0.0007144591654650867\n",
            "Iteration: 26/537, Loss: 0.0013235686346888542\n",
            "Iteration: 27/537, Loss: 0.002979581244289875\n",
            "Iteration: 28/537, Loss: 0.0028847139328718185\n",
            "Iteration: 29/537, Loss: 0.014911591075360775\n",
            "Iteration: 30/537, Loss: 0.002945133950561285\n",
            "Iteration: 31/537, Loss: 0.08640310913324356\n",
            "Iteration: 32/537, Loss: 0.0020516952499747276\n",
            "Iteration: 33/537, Loss: 0.07743684202432632\n",
            "Iteration: 34/537, Loss: 0.008296824991703033\n",
            "Iteration: 35/537, Loss: 0.0005967185716144741\n",
            "Iteration: 36/537, Loss: 0.0023425216786563396\n",
            "Iteration: 37/537, Loss: 0.00033628096571192145\n",
            "Iteration: 38/537, Loss: 0.00034366693580523133\n",
            "Iteration: 39/537, Loss: 0.00047139354865066707\n",
            "Iteration: 40/537, Loss: 0.005816231481730938\n",
            "Iteration: 41/537, Loss: 0.0005514873191714287\n",
            "Iteration: 42/537, Loss: 0.003887101076543331\n",
            "Iteration: 43/537, Loss: 0.00013888327521272004\n",
            "Iteration: 44/537, Loss: 0.0017510337056592107\n",
            "Iteration: 45/537, Loss: 0.05831282213330269\n",
            "Iteration: 46/537, Loss: 0.0022544399835169315\n",
            "Iteration: 47/537, Loss: 0.04621502384543419\n",
            "Iteration: 48/537, Loss: 0.1382666677236557\n",
            "Iteration: 49/537, Loss: 0.0005088555626571178\n",
            "Iteration: 50/537, Loss: 0.000559354608412832\n",
            "Iteration: 51/537, Loss: 0.0027765403501689434\n",
            "Iteration: 52/537, Loss: 0.0006809264305047691\n",
            "Iteration: 53/537, Loss: 0.02373315580189228\n",
            "Iteration: 54/537, Loss: 0.0020974650979042053\n",
            "Iteration: 55/537, Loss: 0.3539671301841736\n",
            "Iteration: 56/537, Loss: 8.838676876621321e-05\n",
            "Iteration: 57/537, Loss: 0.0354623943567276\n",
            "Iteration: 58/537, Loss: 0.18259599804878235\n",
            "Iteration: 59/537, Loss: 0.0028985822573304176\n",
            "Iteration: 60/537, Loss: 0.2834738492965698\n",
            "Iteration: 61/537, Loss: 0.000636336044408381\n",
            "Iteration: 62/537, Loss: 0.0003075451240874827\n",
            "Iteration: 63/537, Loss: 0.0024657705798745155\n",
            "Iteration: 64/537, Loss: 0.012550611048936844\n",
            "Iteration: 65/537, Loss: 0.004374787211418152\n",
            "Iteration: 66/537, Loss: 0.0003428818890824914\n",
            "Iteration: 67/537, Loss: 0.0009278771467506886\n",
            "Iteration: 68/537, Loss: 0.0016835350543260574\n",
            "Iteration: 69/537, Loss: 0.012028496712446213\n",
            "Iteration: 70/537, Loss: 0.00033837903174571693\n",
            "Iteration: 71/537, Loss: 0.0054002441465854645\n",
            "Iteration: 72/537, Loss: 0.0004505762190092355\n",
            "Iteration: 73/537, Loss: 0.004441788420081139\n",
            "Iteration: 74/537, Loss: 0.10394968092441559\n",
            "Iteration: 75/537, Loss: 0.0006871403311379254\n",
            "Iteration: 76/537, Loss: 0.010595782659947872\n",
            "Iteration: 77/537, Loss: 0.0014132559299468994\n",
            "Iteration: 78/537, Loss: 0.0017254219856113195\n",
            "Iteration: 79/537, Loss: 0.0016468630637973547\n",
            "Iteration: 80/537, Loss: 0.0012658215127885342\n",
            "Iteration: 81/537, Loss: 0.00030303167295642197\n",
            "Iteration: 82/537, Loss: 0.0004779675218742341\n",
            "Iteration: 83/537, Loss: 0.0005528924521058798\n",
            "Iteration: 84/537, Loss: 0.0015221154317259789\n",
            "Iteration: 85/537, Loss: 0.0007953611784614623\n",
            "Iteration: 86/537, Loss: 0.0006161309429444373\n",
            "Iteration: 87/537, Loss: 0.0002747449616435915\n",
            "Iteration: 88/537, Loss: 0.00043991892016492784\n",
            "Iteration: 89/537, Loss: 0.0005994421662762761\n",
            "Iteration: 90/537, Loss: 0.0073218559846282005\n",
            "Iteration: 91/537, Loss: 0.0016317024128511548\n",
            "Iteration: 92/537, Loss: 0.000751886167563498\n",
            "Iteration: 93/537, Loss: 0.003651123959571123\n",
            "Iteration: 94/537, Loss: 0.0058929529041051865\n",
            "Iteration: 95/537, Loss: 0.002635180251672864\n",
            "Iteration: 96/537, Loss: 0.0006456630653701723\n",
            "Iteration: 97/537, Loss: 0.00200703926384449\n",
            "Iteration: 98/537, Loss: 0.001282965298742056\n",
            "Iteration: 99/537, Loss: 0.00215101963840425\n",
            "Iteration: 100/537, Loss: 0.00047629227628931403\n",
            "Iteration: 101/537, Loss: 0.00032429114799015224\n",
            "Iteration: 102/537, Loss: 0.0014271503314375877\n",
            "Iteration: 103/537, Loss: 0.0009920838056132197\n",
            "Iteration: 104/537, Loss: 0.0009426348842680454\n",
            "Iteration: 105/537, Loss: 0.0018558693118393421\n",
            "Iteration: 106/537, Loss: 0.0027039863634854555\n",
            "Iteration: 107/537, Loss: 0.0007169789751060307\n",
            "Iteration: 108/537, Loss: 0.00040799955604597926\n",
            "Iteration: 109/537, Loss: 0.001113296253606677\n",
            "Iteration: 110/537, Loss: 0.0008138188859447837\n",
            "Iteration: 111/537, Loss: 0.000616279779933393\n",
            "Iteration: 112/537, Loss: 0.002884454792365432\n",
            "Iteration: 113/537, Loss: 0.000599825638346374\n",
            "Iteration: 114/537, Loss: 0.010976809076964855\n",
            "Iteration: 115/537, Loss: 0.0016609345329925418\n",
            "Iteration: 116/537, Loss: 0.003158382372930646\n",
            "Iteration: 117/537, Loss: 0.0014881039969623089\n",
            "Iteration: 118/537, Loss: 0.0013808759395033121\n",
            "Iteration: 119/537, Loss: 0.0009837956167757511\n",
            "Iteration: 120/537, Loss: 0.0003338842070661485\n",
            "Iteration: 121/537, Loss: 0.011359195224940777\n",
            "Iteration: 122/537, Loss: 0.002789571415632963\n",
            "Iteration: 123/537, Loss: 0.0018145306967198849\n",
            "Iteration: 124/537, Loss: 0.0005480939289554954\n",
            "Iteration: 125/537, Loss: 0.0013436459703370929\n",
            "Iteration: 126/537, Loss: 0.005386378616094589\n",
            "Iteration: 127/537, Loss: 0.0013284925371408463\n",
            "Iteration: 128/537, Loss: 0.0003702525864355266\n",
            "Iteration: 129/537, Loss: 0.00018694644677452743\n",
            "Iteration: 130/537, Loss: 0.002970667090266943\n",
            "Iteration: 131/537, Loss: 0.0002358608617214486\n",
            "Iteration: 132/537, Loss: 0.20086538791656494\n",
            "Iteration: 133/537, Loss: 0.004036468919366598\n",
            "Iteration: 134/537, Loss: 0.0005425092531368136\n",
            "Iteration: 135/537, Loss: 0.006147065665572882\n",
            "Iteration: 136/537, Loss: 0.00022659044770989567\n",
            "Iteration: 137/537, Loss: 0.002539199311286211\n",
            "Iteration: 138/537, Loss: 0.0004216811212245375\n",
            "Iteration: 139/537, Loss: 0.0005753946607001126\n",
            "Iteration: 140/537, Loss: 0.0008681623148731887\n",
            "Iteration: 141/537, Loss: 0.001133010839112103\n",
            "Iteration: 142/537, Loss: 0.0007975904736667871\n",
            "Iteration: 143/537, Loss: 0.0007079592905938625\n",
            "Iteration: 144/537, Loss: 0.0013109598075971007\n",
            "Iteration: 145/537, Loss: 0.002884471556171775\n",
            "Iteration: 146/537, Loss: 0.0009569506510160863\n",
            "Iteration: 147/537, Loss: 0.0040517826564610004\n",
            "Iteration: 148/537, Loss: 0.0070571256801486015\n",
            "Iteration: 149/537, Loss: 0.0010644224239513278\n",
            "Iteration: 150/537, Loss: 0.0008630120428279042\n",
            "Iteration: 151/537, Loss: 0.0004936154000461102\n",
            "Iteration: 152/537, Loss: 0.0005384504329413176\n",
            "Iteration: 153/537, Loss: 0.001079894369468093\n",
            "Iteration: 154/537, Loss: 0.0003218499477952719\n",
            "Iteration: 155/537, Loss: 0.002372859511524439\n",
            "Iteration: 156/537, Loss: 0.0009859714191406965\n",
            "Iteration: 157/537, Loss: 0.004917946644127369\n",
            "Iteration: 158/537, Loss: 0.0003089273232035339\n",
            "Iteration: 159/537, Loss: 0.004011231474578381\n",
            "Iteration: 160/537, Loss: 0.0019399730954319239\n",
            "Iteration: 161/537, Loss: 0.0050627123564481735\n",
            "Iteration: 162/537, Loss: 0.0005787130212411284\n",
            "Iteration: 163/537, Loss: 0.0009533651755191386\n",
            "Iteration: 164/537, Loss: 0.0004433333524502814\n",
            "Iteration: 165/537, Loss: 0.002009389456361532\n",
            "Iteration: 166/537, Loss: 0.002432838547974825\n",
            "Iteration: 167/537, Loss: 0.01863793656229973\n",
            "Iteration: 168/537, Loss: 0.0006001372239552438\n",
            "Iteration: 169/537, Loss: 0.0013988739810883999\n",
            "Iteration: 170/537, Loss: 0.0020335009321570396\n",
            "Iteration: 171/537, Loss: 0.07567156106233597\n",
            "Iteration: 172/537, Loss: 0.0005340698407962918\n",
            "Iteration: 173/537, Loss: 0.0009077170980162919\n",
            "Iteration: 174/537, Loss: 0.035947736352682114\n",
            "Iteration: 175/537, Loss: 0.005605026613920927\n",
            "Iteration: 176/537, Loss: 0.00022326096950564533\n",
            "Iteration: 177/537, Loss: 0.00022997873020358384\n",
            "Iteration: 178/537, Loss: 0.005040313117206097\n",
            "Iteration: 179/537, Loss: 0.0033067113254219294\n",
            "Iteration: 180/537, Loss: 0.0003041102609131485\n",
            "Iteration: 181/537, Loss: 0.004953415598720312\n",
            "Iteration: 182/537, Loss: 0.012013472616672516\n",
            "Iteration: 183/537, Loss: 0.001939990557730198\n",
            "Iteration: 184/537, Loss: 0.0007436858722940087\n",
            "Iteration: 185/537, Loss: 0.006130490452051163\n",
            "Iteration: 186/537, Loss: 0.0011047108564525843\n",
            "Iteration: 187/537, Loss: 0.000459543225588277\n",
            "Iteration: 188/537, Loss: 0.11329321563243866\n",
            "Iteration: 189/537, Loss: 0.003141761524602771\n",
            "Iteration: 190/537, Loss: 0.0024090956430882215\n",
            "Iteration: 191/537, Loss: 0.0010716881370171905\n",
            "Iteration: 192/537, Loss: 0.00023294196580536664\n",
            "Iteration: 193/537, Loss: 0.0014062286354601383\n",
            "Iteration: 194/537, Loss: 0.15204118192195892\n",
            "Iteration: 195/537, Loss: 0.0003277128271292895\n",
            "Iteration: 196/537, Loss: 0.00023291810066439211\n",
            "Iteration: 197/537, Loss: 0.0003489833034109324\n",
            "Iteration: 198/537, Loss: 0.1724247932434082\n",
            "Iteration: 199/537, Loss: 0.0027689081616699696\n",
            "Iteration: 200/537, Loss: 0.0022268001921474934\n",
            "Iteration: 201/537, Loss: 0.0005320920026861131\n",
            "Iteration: 202/537, Loss: 0.0008043848793022335\n",
            "Iteration: 203/537, Loss: 0.00158112368080765\n",
            "Iteration: 204/537, Loss: 0.0005244441563263535\n",
            "Iteration: 205/537, Loss: 0.00024268191191367805\n",
            "Iteration: 206/537, Loss: 0.0004969045985490084\n",
            "Iteration: 207/537, Loss: 0.0017505281139165163\n",
            "Iteration: 208/537, Loss: 0.0019171300809830427\n",
            "Iteration: 209/537, Loss: 0.023689400404691696\n",
            "Iteration: 210/537, Loss: 0.0003832013753708452\n",
            "Iteration: 211/537, Loss: 0.0274663046002388\n",
            "Iteration: 212/537, Loss: 0.0003654535976238549\n",
            "Iteration: 213/537, Loss: 0.0011559827253222466\n",
            "Iteration: 214/537, Loss: 0.0006665299879387021\n",
            "Iteration: 215/537, Loss: 0.0004397836746647954\n",
            "Iteration: 216/537, Loss: 0.0005675351130776107\n",
            "Iteration: 217/537, Loss: 0.0005780477076768875\n",
            "Iteration: 218/537, Loss: 0.0027618908789008856\n",
            "Iteration: 219/537, Loss: 0.0859675332903862\n",
            "Iteration: 220/537, Loss: 0.0009668114362284541\n",
            "Iteration: 221/537, Loss: 0.0022011748515069485\n",
            "Iteration: 222/537, Loss: 0.003767474787309766\n",
            "Iteration: 223/537, Loss: 0.002496651839464903\n",
            "Iteration: 224/537, Loss: 0.0005544298910535872\n",
            "Iteration: 225/537, Loss: 0.0009066892089322209\n",
            "Iteration: 226/537, Loss: 0.0020004427060484886\n",
            "Iteration: 227/537, Loss: 0.00048576376866549253\n",
            "Iteration: 228/537, Loss: 0.0024230736307799816\n",
            "Iteration: 229/537, Loss: 0.08091341704130173\n",
            "Iteration: 230/537, Loss: 0.0008025527349673212\n",
            "Iteration: 231/537, Loss: 0.04093528166413307\n",
            "Iteration: 232/537, Loss: 0.0016601174138486385\n",
            "Iteration: 233/537, Loss: 0.0031315514352172613\n",
            "Iteration: 234/537, Loss: 0.00103485316503793\n",
            "Iteration: 235/537, Loss: 0.004291989840567112\n",
            "Iteration: 236/537, Loss: 0.0007614491623826325\n",
            "Iteration: 237/537, Loss: 0.657156229019165\n",
            "Iteration: 238/537, Loss: 0.0003404544841032475\n",
            "Iteration: 239/537, Loss: 0.0037803032901138067\n",
            "Iteration: 240/537, Loss: 0.0007634029607288539\n",
            "Iteration: 241/537, Loss: 0.002291740383952856\n",
            "Iteration: 242/537, Loss: 0.00282710837200284\n",
            "Iteration: 243/537, Loss: 0.0030516539700329304\n",
            "Iteration: 244/537, Loss: 0.0008294522413052619\n",
            "Iteration: 245/537, Loss: 0.0008666174253448844\n",
            "Iteration: 246/537, Loss: 0.0016320925205945969\n",
            "Iteration: 247/537, Loss: 0.0013896808959543705\n",
            "Iteration: 248/537, Loss: 0.0014399306382983923\n",
            "Iteration: 249/537, Loss: 0.03408192843198776\n",
            "Iteration: 250/537, Loss: 0.0019937697798013687\n",
            "Iteration: 251/537, Loss: 0.013446046970784664\n",
            "Iteration: 252/537, Loss: 0.20072442293167114\n",
            "Iteration: 253/537, Loss: 0.0024590701796114445\n",
            "Iteration: 254/537, Loss: 0.0016956464387476444\n",
            "Iteration: 255/537, Loss: 0.009286337532103062\n",
            "Iteration: 256/537, Loss: 0.000818430504295975\n",
            "Iteration: 257/537, Loss: 0.000830397242680192\n",
            "Iteration: 258/537, Loss: 0.00276316050440073\n",
            "Iteration: 259/537, Loss: 0.0017364149680361152\n",
            "Iteration: 260/537, Loss: 0.0004830645921174437\n",
            "Iteration: 261/537, Loss: 0.004576016217470169\n",
            "Iteration: 262/537, Loss: 0.0004889998817816377\n",
            "Iteration: 263/537, Loss: 0.0008860973175615072\n",
            "Iteration: 264/537, Loss: 0.0026658468414098024\n",
            "Iteration: 265/537, Loss: 0.001852643210440874\n",
            "Iteration: 266/537, Loss: 0.0044465865939855576\n",
            "Iteration: 267/537, Loss: 0.001654598512686789\n",
            "Iteration: 268/537, Loss: 0.002721828408539295\n",
            "Iteration: 269/537, Loss: 0.008516817353665829\n",
            "Iteration: 270/537, Loss: 0.042418692260980606\n",
            "Iteration: 271/537, Loss: 0.0051353974267840385\n",
            "Iteration: 272/537, Loss: 0.011902493424713612\n",
            "Iteration: 273/537, Loss: 0.0021688025444746017\n",
            "Iteration: 274/537, Loss: 0.0004342133179306984\n",
            "Iteration: 275/537, Loss: 0.0083216093480587\n",
            "Iteration: 276/537, Loss: 0.005668740253895521\n",
            "Iteration: 277/537, Loss: 0.0013219047104939818\n",
            "Iteration: 278/537, Loss: 0.002385214902460575\n",
            "Iteration: 279/537, Loss: 0.0037994105368852615\n",
            "Iteration: 280/537, Loss: 0.008868392556905746\n",
            "Iteration: 281/537, Loss: 0.0007032968569546938\n",
            "Iteration: 282/537, Loss: 0.0023432511370629072\n",
            "Iteration: 283/537, Loss: 0.001348312827758491\n",
            "Iteration: 284/537, Loss: 0.0008528103935532272\n",
            "Iteration: 285/537, Loss: 0.0005183628527447581\n",
            "Iteration: 286/537, Loss: 0.0013403126504272223\n",
            "Iteration: 287/537, Loss: 0.0007720349240116775\n",
            "Iteration: 288/537, Loss: 0.00842076726257801\n",
            "Iteration: 289/537, Loss: 0.006820830516517162\n",
            "Iteration: 290/537, Loss: 0.0011018923250958323\n",
            "Iteration: 291/537, Loss: 0.000992059474810958\n",
            "Iteration: 292/537, Loss: 0.0029780196491628885\n",
            "Iteration: 293/537, Loss: 0.020595738664269447\n",
            "Iteration: 294/537, Loss: 0.0005518807447515428\n",
            "Iteration: 295/537, Loss: 0.0008114714873954654\n",
            "Iteration: 296/537, Loss: 0.0017574166413396597\n",
            "Iteration: 297/537, Loss: 0.0020043521653860807\n",
            "Iteration: 298/537, Loss: 0.007453444879502058\n",
            "Iteration: 299/537, Loss: 0.00047418841859325767\n",
            "Iteration: 300/537, Loss: 0.0023456476628780365\n",
            "Iteration: 301/537, Loss: 0.06859494745731354\n",
            "Iteration: 302/537, Loss: 0.001493088435381651\n",
            "Iteration: 303/537, Loss: 0.00301791587844491\n",
            "Iteration: 304/537, Loss: 0.01192420069128275\n",
            "Iteration: 305/537, Loss: 0.002823254559189081\n",
            "Iteration: 306/537, Loss: 0.005186510272324085\n",
            "Iteration: 307/537, Loss: 0.0062360865995287895\n",
            "Iteration: 308/537, Loss: 0.00033588020596653223\n",
            "Iteration: 309/537, Loss: 0.0005468957824632525\n",
            "Iteration: 310/537, Loss: 0.00040124039514921606\n",
            "Iteration: 311/537, Loss: 0.002618475118651986\n",
            "Iteration: 312/537, Loss: 0.001705001574009657\n",
            "Iteration: 313/537, Loss: 0.002869111718609929\n",
            "Iteration: 314/537, Loss: 0.0007606917642988265\n",
            "Iteration: 315/537, Loss: 0.0008830533479340374\n",
            "Iteration: 316/537, Loss: 0.0016856397269293666\n",
            "Iteration: 317/537, Loss: 0.0013894623843953013\n",
            "Iteration: 318/537, Loss: 0.0003341791743878275\n",
            "Iteration: 319/537, Loss: 0.00015287323913071305\n",
            "Iteration: 320/537, Loss: 0.002034944947808981\n",
            "Iteration: 321/537, Loss: 0.002765064127743244\n",
            "Iteration: 322/537, Loss: 0.005540245212614536\n",
            "Iteration: 323/537, Loss: 0.0039841425605118275\n",
            "Iteration: 324/537, Loss: 0.0008282932685688138\n",
            "Iteration: 325/537, Loss: 0.0010983349056914449\n",
            "Iteration: 326/537, Loss: 0.04743492603302002\n",
            "Iteration: 327/537, Loss: 0.001816578907892108\n",
            "Iteration: 328/537, Loss: 0.005986935459077358\n",
            "Iteration: 329/537, Loss: 0.0004277365223970264\n",
            "Iteration: 330/537, Loss: 0.0005317671457305551\n",
            "Iteration: 331/537, Loss: 0.08515369892120361\n",
            "Iteration: 332/537, Loss: 0.02576039731502533\n",
            "Iteration: 333/537, Loss: 0.0007045987877063453\n",
            "Iteration: 334/537, Loss: 0.001377840293571353\n",
            "Iteration: 335/537, Loss: 0.0004797954752575606\n",
            "Iteration: 336/537, Loss: 0.1430729180574417\n",
            "Iteration: 337/537, Loss: 0.005543709732592106\n",
            "Iteration: 338/537, Loss: 0.00043363962322473526\n",
            "Iteration: 339/537, Loss: 0.00038723894977010787\n",
            "Iteration: 340/537, Loss: 0.001212205272167921\n",
            "Iteration: 341/537, Loss: 0.0006637213518843055\n",
            "Iteration: 342/537, Loss: 0.0018271227600052953\n",
            "Iteration: 343/537, Loss: 0.0014855689369142056\n",
            "Iteration: 344/537, Loss: 0.0015101402532309294\n",
            "Iteration: 345/537, Loss: 0.0050327046774327755\n",
            "Iteration: 346/537, Loss: 0.0022239151876419783\n",
            "Iteration: 347/537, Loss: 0.053913675248622894\n",
            "Iteration: 348/537, Loss: 0.0014146823668852448\n",
            "Iteration: 349/537, Loss: 0.0017073489725589752\n",
            "Iteration: 350/537, Loss: 0.0031863872427493334\n",
            "Iteration: 351/537, Loss: 0.00265663955360651\n",
            "Iteration: 352/537, Loss: 0.000532023492269218\n",
            "Iteration: 353/537, Loss: 0.013274755328893661\n",
            "Iteration: 354/537, Loss: 0.008512127213180065\n",
            "Iteration: 355/537, Loss: 0.0002560756984166801\n",
            "Iteration: 356/537, Loss: 0.005134172737598419\n",
            "Iteration: 357/537, Loss: 0.003075962420552969\n",
            "Iteration: 358/537, Loss: 0.007205129135400057\n",
            "Iteration: 359/537, Loss: 0.011152629740536213\n",
            "Iteration: 360/537, Loss: 0.0109573258087039\n",
            "Iteration: 361/537, Loss: 0.000707477331161499\n",
            "Iteration: 362/537, Loss: 0.006521319970488548\n",
            "Iteration: 363/537, Loss: 0.0015068788779899478\n",
            "Iteration: 364/537, Loss: 0.0010040290653705597\n",
            "Iteration: 365/537, Loss: 0.00046284834388643503\n",
            "Iteration: 366/537, Loss: 0.004453470930457115\n",
            "Iteration: 367/537, Loss: 0.00042793917236849666\n",
            "Iteration: 368/537, Loss: 0.009142959490418434\n",
            "Iteration: 369/537, Loss: 0.004733096808195114\n",
            "Iteration: 370/537, Loss: 0.0016865701181814075\n",
            "Iteration: 371/537, Loss: 0.008170781657099724\n",
            "Iteration: 372/537, Loss: 0.0021112868562340736\n",
            "Iteration: 373/537, Loss: 0.0003576777526177466\n",
            "Iteration: 374/537, Loss: 0.0006620015483349562\n",
            "Iteration: 375/537, Loss: 0.0005203401087783277\n",
            "Iteration: 376/537, Loss: 0.0005059081595391035\n",
            "Iteration: 377/537, Loss: 0.00018759455997496843\n",
            "Iteration: 378/537, Loss: 0.0005020388634875417\n",
            "Iteration: 379/537, Loss: 0.00013832749391440302\n",
            "Iteration: 380/537, Loss: 0.02975943312048912\n",
            "Iteration: 381/537, Loss: 0.0002453230554237962\n",
            "Iteration: 382/537, Loss: 0.13606177270412445\n",
            "Iteration: 383/537, Loss: 0.001141971442848444\n",
            "Iteration: 384/537, Loss: 0.0034233308397233486\n",
            "Iteration: 385/537, Loss: 0.0015838132239878178\n",
            "Iteration: 386/537, Loss: 0.013337131589651108\n",
            "Iteration: 387/537, Loss: 0.001627718098461628\n",
            "Iteration: 388/537, Loss: 0.005020556505769491\n",
            "Iteration: 389/537, Loss: 0.0014554597437381744\n",
            "Iteration: 390/537, Loss: 0.000838781357742846\n",
            "Iteration: 391/537, Loss: 0.000565112626645714\n",
            "Iteration: 392/537, Loss: 0.010580142959952354\n",
            "Iteration: 393/537, Loss: 0.0073210857808589935\n",
            "Iteration: 394/537, Loss: 0.0008880328387022018\n",
            "Iteration: 395/537, Loss: 0.049529917538166046\n",
            "Iteration: 396/537, Loss: 0.0022475654259324074\n",
            "Iteration: 397/537, Loss: 0.00084115524077788\n",
            "Iteration: 398/537, Loss: 0.0007019967306405306\n",
            "Iteration: 399/537, Loss: 0.0017766590462997556\n",
            "Iteration: 400/537, Loss: 0.0011898233788087964\n",
            "Iteration: 401/537, Loss: 0.08433804661035538\n",
            "Iteration: 402/537, Loss: 0.002434408525004983\n",
            "Iteration: 403/537, Loss: 0.01194959320127964\n",
            "Iteration: 404/537, Loss: 0.046856097877025604\n",
            "Iteration: 405/537, Loss: 0.0007756194099783897\n",
            "Iteration: 406/537, Loss: 0.0063904221169650555\n",
            "Iteration: 407/537, Loss: 0.0010017374297603965\n",
            "Iteration: 408/537, Loss: 0.009662298485636711\n",
            "Iteration: 409/537, Loss: 0.0007773889228701591\n",
            "Iteration: 410/537, Loss: 0.0005631615058518946\n",
            "Iteration: 411/537, Loss: 0.0027701365761458874\n",
            "Iteration: 412/537, Loss: 0.004567501600831747\n",
            "Iteration: 413/537, Loss: 0.00045949334162287414\n",
            "Iteration: 414/537, Loss: 0.0015440448187291622\n",
            "Iteration: 415/537, Loss: 0.0008331419667229056\n",
            "Iteration: 416/537, Loss: 0.002845694310963154\n",
            "Iteration: 417/537, Loss: 0.0007503144443035126\n",
            "Iteration: 418/537, Loss: 0.0027462318539619446\n",
            "Iteration: 419/537, Loss: 0.00031284772558137774\n",
            "Iteration: 420/537, Loss: 0.0007253281073644757\n",
            "Iteration: 421/537, Loss: 0.0017218322027474642\n",
            "Iteration: 422/537, Loss: 0.0006570625118911266\n",
            "Iteration: 423/537, Loss: 0.006047656759619713\n",
            "Iteration: 424/537, Loss: 0.00037955399602651596\n",
            "Iteration: 425/537, Loss: 0.005034137982875109\n",
            "Iteration: 426/537, Loss: 0.25284650921821594\n",
            "Iteration: 427/537, Loss: 0.0010226514423266053\n",
            "Iteration: 428/537, Loss: 0.00026475294725969434\n",
            "Iteration: 429/537, Loss: 0.6360012888908386\n",
            "Iteration: 430/537, Loss: 0.000432927772635594\n",
            "Iteration: 431/537, Loss: 0.0013913367874920368\n",
            "Iteration: 432/537, Loss: 0.0006499183364212513\n",
            "Iteration: 433/537, Loss: 0.0029484652914106846\n",
            "Iteration: 434/537, Loss: 0.006812342442572117\n",
            "Iteration: 435/537, Loss: 0.003956334199756384\n",
            "Iteration: 436/537, Loss: 0.002331430558115244\n",
            "Iteration: 437/537, Loss: 0.005316926632076502\n",
            "Iteration: 438/537, Loss: 0.0011423882097005844\n",
            "Iteration: 439/537, Loss: 0.0075551411136984825\n",
            "Iteration: 440/537, Loss: 0.0009694512700662017\n",
            "Iteration: 441/537, Loss: 0.0002740902127698064\n",
            "Iteration: 442/537, Loss: 0.003417917527258396\n",
            "Iteration: 443/537, Loss: 0.0007678342517465353\n",
            "Iteration: 444/537, Loss: 0.0015651013236492872\n",
            "Iteration: 445/537, Loss: 0.0009266516426578164\n",
            "Iteration: 446/537, Loss: 0.0004501093935687095\n",
            "Iteration: 447/537, Loss: 0.013204345479607582\n",
            "Iteration: 448/537, Loss: 0.0036158193834125996\n",
            "Iteration: 449/537, Loss: 0.0006328842719085515\n",
            "Iteration: 450/537, Loss: 0.003221106017008424\n",
            "Iteration: 451/537, Loss: 0.002800681395456195\n",
            "Iteration: 452/537, Loss: 0.0009191747521981597\n",
            "Iteration: 453/537, Loss: 0.001198125653900206\n",
            "Iteration: 454/537, Loss: 0.0007616478251293302\n",
            "Iteration: 455/537, Loss: 0.0005359193892218173\n",
            "Iteration: 456/537, Loss: 0.0001743203611113131\n",
            "Iteration: 457/537, Loss: 0.001197580248117447\n",
            "Iteration: 458/537, Loss: 0.0007272513466887176\n",
            "Iteration: 459/537, Loss: 0.004609314724802971\n",
            "Iteration: 460/537, Loss: 0.0015241429209709167\n",
            "Iteration: 461/537, Loss: 0.009519527666270733\n",
            "Iteration: 462/537, Loss: 0.0023212714586406946\n",
            "Iteration: 463/537, Loss: 0.0012770529137924314\n",
            "Iteration: 464/537, Loss: 0.006322275847196579\n",
            "Iteration: 465/537, Loss: 0.00039867748273536563\n",
            "Iteration: 466/537, Loss: 0.0014185064937919378\n",
            "Iteration: 467/537, Loss: 0.003948465920984745\n",
            "Iteration: 468/537, Loss: 0.0051065352745354176\n",
            "Iteration: 469/537, Loss: 0.00038331025280058384\n",
            "Iteration: 470/537, Loss: 0.004353317432105541\n",
            "Iteration: 471/537, Loss: 0.002914700424298644\n",
            "Iteration: 472/537, Loss: 0.0026958456728607416\n",
            "Iteration: 473/537, Loss: 0.004535301588475704\n",
            "Iteration: 474/537, Loss: 0.0014319721376523376\n",
            "Iteration: 475/537, Loss: 0.002961465623229742\n",
            "Iteration: 476/537, Loss: 0.0037252255715429783\n",
            "Iteration: 477/537, Loss: 0.00016856458387337625\n",
            "Iteration: 478/537, Loss: 0.004595802165567875\n",
            "Iteration: 479/537, Loss: 0.008464856073260307\n",
            "Iteration: 480/537, Loss: 0.004103322047740221\n",
            "Iteration: 481/537, Loss: 0.0008340341737493873\n",
            "Iteration: 482/537, Loss: 0.11189448833465576\n",
            "Iteration: 483/537, Loss: 0.0006059558363631368\n",
            "Iteration: 484/537, Loss: 0.03023334965109825\n",
            "Iteration: 485/537, Loss: 0.0006997663876973093\n",
            "Iteration: 486/537, Loss: 0.000468238111352548\n",
            "Iteration: 487/537, Loss: 0.01250129658728838\n",
            "Iteration: 488/537, Loss: 0.0022257952950894833\n",
            "Iteration: 489/537, Loss: 0.007009233348071575\n",
            "Iteration: 490/537, Loss: 0.0012881216825917363\n",
            "Iteration: 491/537, Loss: 0.003458451945334673\n",
            "Iteration: 492/537, Loss: 0.0004467825056053698\n",
            "Iteration: 493/537, Loss: 0.0002986201725434512\n",
            "Iteration: 494/537, Loss: 0.07375190407037735\n",
            "Iteration: 495/537, Loss: 0.003492501797154546\n",
            "Iteration: 496/537, Loss: 0.0016502703074365854\n",
            "Iteration: 497/537, Loss: 0.003780899802222848\n",
            "Iteration: 498/537, Loss: 0.0015272602904587984\n",
            "Iteration: 499/537, Loss: 0.003680011024698615\n",
            "Iteration: 500/537, Loss: 0.1197662353515625\n",
            "Iteration: 501/537, Loss: 0.00639664800837636\n",
            "Iteration: 502/537, Loss: 0.0006053353426977992\n",
            "Iteration: 503/537, Loss: 0.00041103450348600745\n",
            "Iteration: 504/537, Loss: 0.0011911203619092703\n",
            "Iteration: 505/537, Loss: 0.00024492511874996126\n",
            "Iteration: 506/537, Loss: 0.0874517485499382\n",
            "Iteration: 507/537, Loss: 0.0005143204471096396\n",
            "Iteration: 508/537, Loss: 0.0017062044935300946\n",
            "Iteration: 509/537, Loss: 0.0063726832158863544\n",
            "Iteration: 510/537, Loss: 0.0014262237818911672\n",
            "Iteration: 511/537, Loss: 0.01694091223180294\n",
            "Iteration: 512/537, Loss: 0.003790809540078044\n",
            "Iteration: 513/537, Loss: 0.0010549805592745543\n",
            "Iteration: 514/537, Loss: 0.0005110105848871171\n",
            "Iteration: 515/537, Loss: 0.003055005567148328\n",
            "Iteration: 516/537, Loss: 0.0025845617055892944\n",
            "Iteration: 517/537, Loss: 0.0032420684583485126\n",
            "Iteration: 518/537, Loss: 0.0045701125636696815\n",
            "Iteration: 519/537, Loss: 0.00030698871705681086\n",
            "Iteration: 520/537, Loss: 0.00044404977234080434\n",
            "Iteration: 521/537, Loss: 0.0006046888884156942\n",
            "Iteration: 522/537, Loss: 0.0560586079955101\n",
            "Iteration: 523/537, Loss: 0.0044463216327130795\n",
            "Iteration: 524/537, Loss: 0.0228196419775486\n",
            "Iteration: 525/537, Loss: 0.0008518880349583924\n",
            "Iteration: 526/537, Loss: 0.000975435774307698\n",
            "Iteration: 527/537, Loss: 0.0027367197908461094\n",
            "Iteration: 528/537, Loss: 0.0006392802461050451\n",
            "Iteration: 529/537, Loss: 0.024101046845316887\n",
            "Iteration: 530/537, Loss: 0.004134518094360828\n",
            "Iteration: 531/537, Loss: 0.02397756837308407\n",
            "Iteration: 532/537, Loss: 0.00048388782306574285\n",
            "Iteration: 533/537, Loss: 0.039514780044555664\n",
            "Iteration: 534/537, Loss: 0.0018927085911855102\n",
            "Iteration: 535/537, Loss: 0.002050188137218356\n",
            "Iteration: 536/537, Loss: 0.0028384127654135227\n",
            "Iteration: 537/537, Loss: 1.0169641971588135\n",
            "Epoch 43 train loss: 0.014716392869249119\n",
            "Epoch 43 test loss: 0.0715015933480269\n",
            "-------------------------------------\n",
            "Iteration: 1/537, Loss: 0.0006394737865775824\n",
            "Iteration: 2/537, Loss: 0.0013436067383736372\n",
            "Iteration: 3/537, Loss: 0.008413505740463734\n",
            "Iteration: 4/537, Loss: 0.0006975746364332736\n",
            "Iteration: 5/537, Loss: 0.027417074888944626\n",
            "Iteration: 6/537, Loss: 0.0037890395615249872\n",
            "Iteration: 7/537, Loss: 0.0017444519326090813\n",
            "Iteration: 8/537, Loss: 0.00032729044323787093\n",
            "Iteration: 9/537, Loss: 0.0002888050221372396\n",
            "Iteration: 10/537, Loss: 0.020740266889333725\n",
            "Iteration: 11/537, Loss: 0.0016581753734499216\n",
            "Iteration: 12/537, Loss: 0.00055839610286057\n",
            "Iteration: 13/537, Loss: 0.0033568746875971556\n",
            "Iteration: 14/537, Loss: 0.0008482735720463097\n",
            "Iteration: 15/537, Loss: 0.0002404230908723548\n",
            "Iteration: 16/537, Loss: 0.0012170056579634547\n",
            "Iteration: 17/537, Loss: 0.012015962973237038\n",
            "Iteration: 18/537, Loss: 0.0007628825260326266\n",
            "Iteration: 19/537, Loss: 0.0027982161846011877\n",
            "Iteration: 20/537, Loss: 0.09863066673278809\n",
            "Iteration: 21/537, Loss: 0.0011343925725668669\n",
            "Iteration: 22/537, Loss: 0.0029188760090619326\n",
            "Iteration: 23/537, Loss: 0.003159219864755869\n",
            "Iteration: 24/537, Loss: 0.00013478360779117793\n",
            "Iteration: 25/537, Loss: 0.003035595640540123\n",
            "Iteration: 26/537, Loss: 0.000961401907261461\n",
            "Iteration: 27/537, Loss: 0.00036094264942221344\n",
            "Iteration: 28/537, Loss: 0.0006596834864467382\n",
            "Iteration: 29/537, Loss: 0.0038712001405656338\n",
            "Iteration: 30/537, Loss: 0.0036567789502441883\n",
            "Iteration: 31/537, Loss: 0.0003331908374093473\n",
            "Iteration: 32/537, Loss: 0.001701192930340767\n",
            "Iteration: 33/537, Loss: 0.00810818001627922\n",
            "Iteration: 34/537, Loss: 0.0044760191813111305\n",
            "Iteration: 35/537, Loss: 0.0008786781691014767\n",
            "Iteration: 36/537, Loss: 0.0037788962945342064\n",
            "Iteration: 37/537, Loss: 0.005508812610059977\n",
            "Iteration: 38/537, Loss: 0.00013755360851064324\n",
            "Iteration: 39/537, Loss: 0.002062567276880145\n",
            "Iteration: 40/537, Loss: 0.0007255813688971102\n",
            "Iteration: 41/537, Loss: 0.10288740694522858\n",
            "Iteration: 42/537, Loss: 0.002281111665070057\n",
            "Iteration: 43/537, Loss: 0.0024389794562011957\n",
            "Iteration: 44/537, Loss: 0.0023791214916855097\n",
            "Iteration: 45/537, Loss: 0.00029870192520320415\n",
            "Iteration: 46/537, Loss: 0.0004840759211219847\n",
            "Iteration: 47/537, Loss: 0.0008036083308979869\n",
            "Iteration: 48/537, Loss: 0.0007450762786902487\n",
            "Iteration: 49/537, Loss: 0.0011799215571954846\n",
            "Iteration: 50/537, Loss: 0.09793397784233093\n",
            "Iteration: 51/537, Loss: 0.0006797608220949769\n",
            "Iteration: 52/537, Loss: 0.0003433201927691698\n",
            "Iteration: 53/537, Loss: 0.002564278431236744\n",
            "Iteration: 54/537, Loss: 0.0013907351531088352\n",
            "Iteration: 55/537, Loss: 0.0032915754709392786\n",
            "Iteration: 56/537, Loss: 0.000497062224894762\n",
            "Iteration: 57/537, Loss: 0.0043427650816738605\n",
            "Iteration: 58/537, Loss: 0.0023433680180460215\n",
            "Iteration: 59/537, Loss: 0.0034870347008109093\n",
            "Iteration: 60/537, Loss: 0.0009365095756947994\n",
            "Iteration: 61/537, Loss: 0.0019313165685161948\n",
            "Iteration: 62/537, Loss: 0.0001580529351485893\n",
            "Iteration: 63/537, Loss: 0.00019935554882977158\n",
            "Iteration: 64/537, Loss: 0.0003674843756016344\n",
            "Iteration: 65/537, Loss: 0.043737541884183884\n",
            "Iteration: 66/537, Loss: 0.001210667542181909\n",
            "Iteration: 67/537, Loss: 0.0015953562688082457\n",
            "Iteration: 68/537, Loss: 0.0001793563278624788\n",
            "Iteration: 69/537, Loss: 0.0010155048221349716\n",
            "Iteration: 70/537, Loss: 0.0035765001084655523\n",
            "Iteration: 71/537, Loss: 0.0003801662242040038\n",
            "Iteration: 72/537, Loss: 0.0006796644884161651\n",
            "Iteration: 73/537, Loss: 0.00619041733443737\n",
            "Iteration: 74/537, Loss: 0.004627365618944168\n",
            "Iteration: 75/537, Loss: 0.00041565403807908297\n",
            "Iteration: 76/537, Loss: 0.0030272030271589756\n",
            "Iteration: 77/537, Loss: 0.0001827866508392617\n",
            "Iteration: 78/537, Loss: 0.0001334975240752101\n",
            "Iteration: 79/537, Loss: 0.007171208038926125\n",
            "Iteration: 80/537, Loss: 0.000762749114073813\n",
            "Iteration: 81/537, Loss: 0.00542862992733717\n",
            "Iteration: 82/537, Loss: 0.00928148441016674\n",
            "Iteration: 83/537, Loss: 0.002509169280529022\n",
            "Iteration: 84/537, Loss: 0.0010684659937396646\n",
            "Iteration: 85/537, Loss: 0.029399393126368523\n",
            "Iteration: 86/537, Loss: 0.00019812185200862586\n",
            "Iteration: 87/537, Loss: 0.00041179469553753734\n",
            "Iteration: 88/537, Loss: 0.005389814265072346\n",
            "Iteration: 89/537, Loss: 0.008324231952428818\n",
            "Iteration: 90/537, Loss: 0.0004981823149137199\n",
            "Iteration: 91/537, Loss: 0.0012739819940179586\n",
            "Iteration: 92/537, Loss: 0.007393344305455685\n",
            "Iteration: 93/537, Loss: 0.0014814436435699463\n",
            "Iteration: 94/537, Loss: 0.00612161960452795\n",
            "Iteration: 95/537, Loss: 0.0012051465455442667\n",
            "Iteration: 96/537, Loss: 0.0008395523764193058\n",
            "Iteration: 97/537, Loss: 0.051559437066316605\n",
            "Iteration: 98/537, Loss: 0.0036014218349009752\n",
            "Iteration: 99/537, Loss: 0.001203725696541369\n",
            "Iteration: 100/537, Loss: 0.03183655068278313\n",
            "Iteration: 101/537, Loss: 0.0034877141006290913\n",
            "Iteration: 102/537, Loss: 0.05988023802638054\n",
            "Iteration: 103/537, Loss: 0.004547453951090574\n",
            "Iteration: 104/537, Loss: 0.0010852947598323226\n",
            "Iteration: 105/537, Loss: 0.00039157026913017035\n",
            "Iteration: 106/537, Loss: 0.0004839253379032016\n",
            "Iteration: 107/537, Loss: 0.0012545172357931733\n",
            "Iteration: 108/537, Loss: 0.0005836514756083488\n",
            "Iteration: 109/537, Loss: 0.006002466659992933\n",
            "Iteration: 110/537, Loss: 0.0009304468403570354\n",
            "Iteration: 111/537, Loss: 0.001078032306395471\n",
            "Iteration: 112/537, Loss: 0.0008663837797939777\n",
            "Iteration: 113/537, Loss: 0.006921831518411636\n",
            "Iteration: 114/537, Loss: 0.0006590806297026575\n",
            "Iteration: 115/537, Loss: 0.0017902490217238665\n",
            "Iteration: 116/537, Loss: 0.0033335937187075615\n",
            "Iteration: 117/537, Loss: 0.0010695054661482573\n",
            "Iteration: 118/537, Loss: 0.00028171544545330107\n",
            "Iteration: 119/537, Loss: 0.0014762644423171878\n",
            "Iteration: 120/537, Loss: 0.0032441806979477406\n",
            "Iteration: 121/537, Loss: 0.004309823736548424\n",
            "Iteration: 122/537, Loss: 0.0006130945403128862\n",
            "Iteration: 123/537, Loss: 0.006771810352802277\n",
            "Iteration: 124/537, Loss: 0.002231299877166748\n",
            "Iteration: 125/537, Loss: 0.0005249890964478254\n",
            "Iteration: 126/537, Loss: 0.0024675820022821426\n",
            "Iteration: 127/537, Loss: 0.0013467753306031227\n",
            "Iteration: 128/537, Loss: 0.006764417514204979\n",
            "Iteration: 129/537, Loss: 0.005007960367947817\n",
            "Iteration: 130/537, Loss: 0.0009426670148968697\n",
            "Iteration: 131/537, Loss: 0.006496787071228027\n",
            "Iteration: 132/537, Loss: 0.0002107707696268335\n",
            "Iteration: 133/537, Loss: 0.00027537811547517776\n",
            "Iteration: 134/537, Loss: 0.0009621583158150315\n",
            "Iteration: 135/537, Loss: 0.49033984541893005\n",
            "Iteration: 136/537, Loss: 0.0009767732117325068\n",
            "Iteration: 137/537, Loss: 0.334045946598053\n",
            "Iteration: 138/537, Loss: 0.0011602796148508787\n",
            "Iteration: 139/537, Loss: 0.0005248459056019783\n",
            "Iteration: 140/537, Loss: 0.0014225936029106379\n",
            "Iteration: 141/537, Loss: 0.00041446590330451727\n",
            "Iteration: 142/537, Loss: 0.004874422214925289\n",
            "Iteration: 143/537, Loss: 0.0012631407007575035\n",
            "Iteration: 144/537, Loss: 0.0043996889144182205\n",
            "Iteration: 145/537, Loss: 0.00015301168605219573\n",
            "Iteration: 146/537, Loss: 0.00015099091979209334\n",
            "Iteration: 147/537, Loss: 0.012560718692839146\n",
            "Iteration: 148/537, Loss: 0.02005493827164173\n",
            "Iteration: 149/537, Loss: 0.0023091447073966265\n",
            "Iteration: 150/537, Loss: 0.003102382877841592\n",
            "Iteration: 151/537, Loss: 0.0001740550360409543\n",
            "Iteration: 152/537, Loss: 0.0019518400076776743\n",
            "Iteration: 153/537, Loss: 0.0007884608930908144\n",
            "Iteration: 154/537, Loss: 0.00022565611288882792\n",
            "Iteration: 155/537, Loss: 0.001106626121327281\n",
            "Iteration: 156/537, Loss: 0.00031816144473850727\n",
            "Iteration: 157/537, Loss: 0.0010963553795590997\n",
            "Iteration: 158/537, Loss: 0.001525825122371316\n",
            "Iteration: 159/537, Loss: 0.0008340580388903618\n",
            "Iteration: 160/537, Loss: 0.016541818156838417\n",
            "Iteration: 161/537, Loss: 0.001153274904936552\n",
            "Iteration: 162/537, Loss: 0.018118424341082573\n",
            "Iteration: 163/537, Loss: 0.00026577984681352973\n",
            "Iteration: 164/537, Loss: 0.0001902637304738164\n",
            "Iteration: 165/537, Loss: 0.005184398498386145\n",
            "Iteration: 166/537, Loss: 0.00018189016554970294\n",
            "Iteration: 167/537, Loss: 0.00026269073714502156\n",
            "Iteration: 168/537, Loss: 0.03236008435487747\n",
            "Iteration: 169/537, Loss: 0.00027410691836848855\n",
            "Iteration: 170/537, Loss: 0.008563145995140076\n",
            "Iteration: 171/537, Loss: 0.04196833446621895\n",
            "Iteration: 172/537, Loss: 0.0006653565214946866\n",
            "Iteration: 173/537, Loss: 0.20497792959213257\n",
            "Iteration: 174/537, Loss: 0.0017027219291776419\n",
            "Iteration: 175/537, Loss: 0.0003174753801431507\n",
            "Iteration: 176/537, Loss: 0.00043209182331338525\n",
            "Iteration: 177/537, Loss: 0.000783742347266525\n",
            "Iteration: 178/537, Loss: 0.002066683489829302\n",
            "Iteration: 179/537, Loss: 0.003587262239307165\n",
            "Iteration: 180/537, Loss: 0.0006699244258925319\n",
            "Iteration: 181/537, Loss: 7.232623465824872e-05\n",
            "Iteration: 182/537, Loss: 0.004396817181259394\n",
            "Iteration: 183/537, Loss: 0.006501218304038048\n",
            "Iteration: 184/537, Loss: 0.0008549646008759737\n",
            "Iteration: 185/537, Loss: 0.006270269863307476\n",
            "Iteration: 186/537, Loss: 0.000988405547104776\n",
            "Iteration: 187/537, Loss: 0.008002047426998615\n",
            "Iteration: 188/537, Loss: 0.0002477823873050511\n",
            "Iteration: 189/537, Loss: 0.00017350720008835196\n",
            "Iteration: 190/537, Loss: 0.07107409834861755\n",
            "Iteration: 191/537, Loss: 0.001107575255446136\n",
            "Iteration: 192/537, Loss: 0.001583789591677487\n",
            "Iteration: 193/537, Loss: 0.0005488553433679044\n",
            "Iteration: 194/537, Loss: 0.0004448703839443624\n",
            "Iteration: 195/537, Loss: 0.14897949993610382\n",
            "Iteration: 196/537, Loss: 0.0004790941020473838\n",
            "Iteration: 197/537, Loss: 0.0004894011071883142\n",
            "Iteration: 198/537, Loss: 0.00115837377961725\n",
            "Iteration: 199/537, Loss: 0.007326451595872641\n",
            "Iteration: 200/537, Loss: 0.028006667271256447\n",
            "Iteration: 201/537, Loss: 0.0003265283885411918\n",
            "Iteration: 202/537, Loss: 0.00026745686773210764\n",
            "Iteration: 203/537, Loss: 0.00026552268536761403\n",
            "Iteration: 204/537, Loss: 0.0003486693894956261\n",
            "Iteration: 205/537, Loss: 0.003970205783843994\n",
            "Iteration: 206/537, Loss: 1.4717365503311157\n",
            "Iteration: 207/537, Loss: 0.0009868749184533954\n",
            "Iteration: 208/537, Loss: 0.0006499529117718339\n",
            "Iteration: 209/537, Loss: 0.0019388737855479121\n",
            "Iteration: 210/537, Loss: 0.026145216077566147\n",
            "Iteration: 211/537, Loss: 0.00042134037357755005\n",
            "Iteration: 212/537, Loss: 0.007464095950126648\n",
            "Iteration: 213/537, Loss: 0.0006300797685980797\n",
            "Iteration: 214/537, Loss: 0.0007023332873359323\n",
            "Iteration: 215/537, Loss: 0.0015088390791788697\n",
            "Iteration: 216/537, Loss: 0.01181129738688469\n",
            "Iteration: 217/537, Loss: 0.0017170647624880075\n",
            "Iteration: 218/537, Loss: 0.004369878675788641\n",
            "Iteration: 219/537, Loss: 0.0033450042828917503\n",
            "Iteration: 220/537, Loss: 0.0011521445121616125\n",
            "Iteration: 221/537, Loss: 0.00027311156736686826\n",
            "Iteration: 222/537, Loss: 0.0589948371052742\n",
            "Iteration: 223/537, Loss: 0.001272812718525529\n",
            "Iteration: 224/537, Loss: 0.04574800282716751\n",
            "Iteration: 225/537, Loss: 0.00022816080308984965\n",
            "Iteration: 226/537, Loss: 0.002498912625014782\n",
            "Iteration: 227/537, Loss: 0.0009313098271377385\n",
            "Iteration: 228/537, Loss: 0.003494392381981015\n",
            "Iteration: 229/537, Loss: 0.0829801931977272\n",
            "Iteration: 230/537, Loss: 0.006204140838235617\n",
            "Iteration: 231/537, Loss: 0.0004680693964473903\n",
            "Iteration: 232/537, Loss: 0.005375484470278025\n",
            "Iteration: 233/537, Loss: 0.0020614617969840765\n",
            "Iteration: 234/537, Loss: 0.0006443911115638912\n",
            "Iteration: 235/537, Loss: 0.29305195808410645\n",
            "Iteration: 236/537, Loss: 0.0012969216331839561\n",
            "Iteration: 237/537, Loss: 0.0013944834936410189\n",
            "Iteration: 238/537, Loss: 0.04467720910906792\n",
            "Iteration: 239/537, Loss: 0.004360945895314217\n",
            "Iteration: 240/537, Loss: 0.0007239787955768406\n",
            "Iteration: 241/537, Loss: 0.00028752017533406615\n",
            "Iteration: 242/537, Loss: 0.005832303315401077\n",
            "Iteration: 243/537, Loss: 0.0007467576651833951\n",
            "Iteration: 244/537, Loss: 0.0009877225384116173\n",
            "Iteration: 245/537, Loss: 0.01165517047047615\n",
            "Iteration: 246/537, Loss: 0.009207537397742271\n",
            "Iteration: 247/537, Loss: 0.0004076425393577665\n",
            "Iteration: 248/537, Loss: 0.0015754022169858217\n",
            "Iteration: 249/537, Loss: 0.0002792689483612776\n",
            "Iteration: 250/537, Loss: 0.0035064062103629112\n",
            "Iteration: 251/537, Loss: 0.0005286306841298938\n",
            "Iteration: 252/537, Loss: 0.0008463335689157248\n",
            "Iteration: 253/537, Loss: 0.0004290921497158706\n",
            "Iteration: 254/537, Loss: 0.0020854149479418993\n",
            "Iteration: 255/537, Loss: 0.05524779111146927\n",
            "Iteration: 256/537, Loss: 0.0006535279098898172\n",
            "Iteration: 257/537, Loss: 0.0019379862351343036\n",
            "Iteration: 258/537, Loss: 0.0012093143304809928\n",
            "Iteration: 259/537, Loss: 0.0015195343876257539\n",
            "Iteration: 260/537, Loss: 0.004596158396452665\n",
            "Iteration: 261/537, Loss: 0.010429766960442066\n",
            "Iteration: 262/537, Loss: 0.0007808898808434606\n",
            "Iteration: 263/537, Loss: 0.0012904978357255459\n",
            "Iteration: 264/537, Loss: 0.009761089459061623\n",
            "Iteration: 265/537, Loss: 0.0038850801065564156\n",
            "Iteration: 266/537, Loss: 0.000929267262108624\n",
            "Iteration: 267/537, Loss: 0.038219645619392395\n",
            "Iteration: 268/537, Loss: 0.002945807296782732\n",
            "Iteration: 269/537, Loss: 0.00036990345688536763\n",
            "Iteration: 270/537, Loss: 0.006020452827215195\n",
            "Iteration: 271/537, Loss: 0.3872128427028656\n",
            "Iteration: 272/537, Loss: 0.0019260714761912823\n",
            "Iteration: 273/537, Loss: 0.0004496806359384209\n",
            "Iteration: 274/537, Loss: 0.004291887395083904\n",
            "Iteration: 275/537, Loss: 0.003188490867614746\n",
            "Iteration: 276/537, Loss: 0.001602389384061098\n",
            "Iteration: 277/537, Loss: 0.005562592297792435\n",
            "Iteration: 278/537, Loss: 0.008666567504405975\n",
            "Iteration: 279/537, Loss: 0.003745605004951358\n",
            "Iteration: 280/537, Loss: 0.0005095440428704023\n",
            "Iteration: 281/537, Loss: 0.005895474925637245\n",
            "Iteration: 282/537, Loss: 0.0010801145108416677\n",
            "Iteration: 283/537, Loss: 0.0018327657599002123\n",
            "Iteration: 284/537, Loss: 0.0004379567690193653\n",
            "Iteration: 285/537, Loss: 0.007137003820389509\n",
            "Iteration: 286/537, Loss: 0.0027856016531586647\n",
            "Iteration: 287/537, Loss: 0.0014186828630045056\n",
            "Iteration: 288/537, Loss: 0.0036141371820122004\n",
            "Iteration: 289/537, Loss: 0.00158704596105963\n",
            "Iteration: 290/537, Loss: 0.027875525876879692\n",
            "Iteration: 291/537, Loss: 0.0005607786006294191\n",
            "Iteration: 292/537, Loss: 0.00017434952314943075\n",
            "Iteration: 293/537, Loss: 0.0004743155441246927\n",
            "Iteration: 294/537, Loss: 0.00035270710941404104\n",
            "Iteration: 295/537, Loss: 0.0019895131699740887\n",
            "Iteration: 296/537, Loss: 0.0019035916775465012\n",
            "Iteration: 297/537, Loss: 0.245514914393425\n",
            "Iteration: 298/537, Loss: 0.0005721524939872324\n",
            "Iteration: 299/537, Loss: 0.06498450040817261\n",
            "Iteration: 300/537, Loss: 0.001046257559210062\n",
            "Iteration: 301/537, Loss: 0.15428900718688965\n",
            "Iteration: 302/537, Loss: 0.004505285993218422\n",
            "Iteration: 303/537, Loss: 0.012078966945409775\n",
            "Iteration: 304/537, Loss: 0.0028955419547855854\n",
            "Iteration: 305/537, Loss: 0.004881595261394978\n",
            "Iteration: 306/537, Loss: 0.014577697962522507\n",
            "Iteration: 307/537, Loss: 0.0005283831851556897\n",
            "Iteration: 308/537, Loss: 0.006653617601841688\n",
            "Iteration: 309/537, Loss: 0.00396348163485527\n",
            "Iteration: 310/537, Loss: 0.006894826423376799\n",
            "Iteration: 311/537, Loss: 0.0013369518565014005\n",
            "Iteration: 312/537, Loss: 0.0027268370613455772\n",
            "Iteration: 313/537, Loss: 0.0006967378430999815\n",
            "Iteration: 314/537, Loss: 0.0055353459902107716\n",
            "Iteration: 315/537, Loss: 0.00014535037917084992\n",
            "Iteration: 316/537, Loss: 0.0008602889720350504\n",
            "Iteration: 317/537, Loss: 0.0055705467239022255\n",
            "Iteration: 318/537, Loss: 0.0015569553943350911\n",
            "Iteration: 319/537, Loss: 0.00018949012155644596\n",
            "Iteration: 320/537, Loss: 0.0011878340737894177\n",
            "Iteration: 321/537, Loss: 0.0025020360480993986\n",
            "Iteration: 322/537, Loss: 0.0036674763541668653\n",
            "Iteration: 323/537, Loss: 0.0008762908400967717\n",
            "Iteration: 324/537, Loss: 0.0013243748107925057\n",
            "Iteration: 325/537, Loss: 0.0011865277774631977\n",
            "Iteration: 326/537, Loss: 0.0031178793869912624\n",
            "Iteration: 327/537, Loss: 0.0010433385614305735\n",
            "Iteration: 328/537, Loss: 0.00038831986603327096\n",
            "Iteration: 329/537, Loss: 0.000498019449878484\n",
            "Iteration: 330/537, Loss: 0.000586346082855016\n",
            "Iteration: 331/537, Loss: 0.0008649713709019125\n",
            "Iteration: 332/537, Loss: 0.004130969755351543\n",
            "Iteration: 333/537, Loss: 0.002495438326150179\n",
            "Iteration: 334/537, Loss: 0.0005833778413943946\n",
            "Iteration: 335/537, Loss: 0.0015412282664328814\n",
            "Iteration: 336/537, Loss: 0.00046291560283862054\n",
            "Iteration: 337/537, Loss: 0.003542151302099228\n",
            "Iteration: 338/537, Loss: 0.00022273522336035967\n",
            "Iteration: 339/537, Loss: 0.004085111897438765\n",
            "Iteration: 340/537, Loss: 0.0003285529091954231\n",
            "Iteration: 341/537, Loss: 0.02176526188850403\n",
            "Iteration: 342/537, Loss: 0.005858869757503271\n",
            "Iteration: 343/537, Loss: 0.002527326112613082\n",
            "Iteration: 344/537, Loss: 0.002039717510342598\n",
            "Iteration: 345/537, Loss: 0.004430829081684351\n",
            "Iteration: 346/537, Loss: 0.03704932704567909\n",
            "Iteration: 347/537, Loss: 0.006332910619676113\n",
            "Iteration: 348/537, Loss: 0.0009205711539834738\n",
            "Iteration: 349/537, Loss: 0.0043067652732133865\n",
            "Iteration: 350/537, Loss: 0.003005304606631398\n",
            "Iteration: 351/537, Loss: 0.005724944174289703\n",
            "Iteration: 352/537, Loss: 0.0028977012261748314\n",
            "Iteration: 353/537, Loss: 0.0024057214614003897\n",
            "Iteration: 354/537, Loss: 0.5395169258117676\n",
            "Iteration: 355/537, Loss: 0.11527740955352783\n",
            "Iteration: 356/537, Loss: 0.001166352303698659\n",
            "Iteration: 357/537, Loss: 0.0015734591288492084\n",
            "Iteration: 358/537, Loss: 0.0024977773427963257\n",
            "Iteration: 359/537, Loss: 0.000561634951736778\n",
            "Iteration: 360/537, Loss: 0.0017072937916964293\n",
            "Iteration: 361/537, Loss: 0.003225640393793583\n",
            "Iteration: 362/537, Loss: 0.006552313454449177\n",
            "Iteration: 363/537, Loss: 0.010712534189224243\n",
            "Iteration: 364/537, Loss: 0.0013483488000929356\n",
            "Iteration: 365/537, Loss: 0.11781756579875946\n",
            "Iteration: 366/537, Loss: 0.05841692537069321\n",
            "Iteration: 367/537, Loss: 0.0016254032962024212\n",
            "Iteration: 368/537, Loss: 0.012824317440390587\n",
            "Iteration: 369/537, Loss: 0.0008295119041576982\n",
            "Iteration: 370/537, Loss: 0.002035803161561489\n",
            "Iteration: 371/537, Loss: 0.0028353682719171047\n",
            "Iteration: 372/537, Loss: 0.0036933235824108124\n",
            "Iteration: 373/537, Loss: 0.0014664737973362207\n",
            "Iteration: 374/537, Loss: 0.13306495547294617\n",
            "Iteration: 375/537, Loss: 0.01621987298130989\n",
            "Iteration: 376/537, Loss: 0.0026802318170666695\n",
            "Iteration: 377/537, Loss: 0.581973135471344\n",
            "Iteration: 378/537, Loss: 0.003833646420389414\n",
            "Iteration: 379/537, Loss: 0.0009694566251710057\n",
            "Iteration: 380/537, Loss: 0.009358037263154984\n",
            "Iteration: 381/537, Loss: 0.004968804307281971\n",
            "Iteration: 382/537, Loss: 0.005839393008500338\n",
            "Iteration: 383/537, Loss: 0.004570643417537212\n",
            "Iteration: 384/537, Loss: 0.005260851234197617\n",
            "Iteration: 385/537, Loss: 0.0008837617933750153\n",
            "Iteration: 386/537, Loss: 0.0028217327781021595\n",
            "Iteration: 387/537, Loss: 0.0013661531265825033\n",
            "Iteration: 388/537, Loss: 0.003208083100616932\n",
            "Iteration: 389/537, Loss: 0.34810441732406616\n",
            "Iteration: 390/537, Loss: 0.0007697059772908688\n",
            "Iteration: 391/537, Loss: 0.0018496719421818852\n",
            "Iteration: 392/537, Loss: 0.0016535494942218065\n",
            "Iteration: 393/537, Loss: 0.1253148764371872\n",
            "Iteration: 394/537, Loss: 0.004241191782057285\n",
            "Iteration: 395/537, Loss: 0.0015783820999786258\n",
            "Iteration: 396/537, Loss: 0.0009888001950457692\n",
            "Iteration: 397/537, Loss: 0.0015764296986162663\n",
            "Iteration: 398/537, Loss: 0.015424247831106186\n",
            "Iteration: 399/537, Loss: 0.0015698649222031236\n",
            "Iteration: 400/537, Loss: 0.0005079102702438831\n",
            "Iteration: 401/537, Loss: 0.0019422216573730111\n",
            "Iteration: 402/537, Loss: 0.0006232316372916102\n",
            "Iteration: 403/537, Loss: 0.01073057483881712\n",
            "Iteration: 404/537, Loss: 0.001351982937194407\n",
            "Iteration: 405/537, Loss: 0.001596857444383204\n",
            "Iteration: 406/537, Loss: 0.0029935124330222607\n",
            "Iteration: 407/537, Loss: 0.0008883080445230007\n",
            "Iteration: 408/537, Loss: 0.0025228606536984444\n",
            "Iteration: 409/537, Loss: 0.0013252891367301345\n",
            "Iteration: 410/537, Loss: 0.004810715559870005\n",
            "Iteration: 411/537, Loss: 0.0014653302496299148\n",
            "Iteration: 412/537, Loss: 0.11465225368738174\n",
            "Iteration: 413/537, Loss: 0.0006589558906853199\n",
            "Iteration: 414/537, Loss: 0.0004627534653991461\n",
            "Iteration: 415/537, Loss: 0.00048043642891570926\n",
            "Iteration: 416/537, Loss: 0.0003355408553034067\n",
            "Iteration: 417/537, Loss: 0.09515038132667542\n",
            "Iteration: 418/537, Loss: 0.2117067575454712\n",
            "Iteration: 419/537, Loss: 0.1322079449892044\n",
            "Iteration: 420/537, Loss: 0.004622809123247862\n",
            "Iteration: 421/537, Loss: 0.0003474392869975418\n",
            "Iteration: 422/537, Loss: 0.0015380708500742912\n",
            "Iteration: 423/537, Loss: 0.0014486389700323343\n",
            "Iteration: 424/537, Loss: 0.085761658847332\n",
            "Iteration: 425/537, Loss: 0.022634385153651237\n",
            "Iteration: 426/537, Loss: 0.0017867707647383213\n",
            "Iteration: 427/537, Loss: 0.0033542606979608536\n",
            "Iteration: 428/537, Loss: 0.0005787978880107403\n",
            "Iteration: 429/537, Loss: 0.0007546758861280978\n",
            "Iteration: 430/537, Loss: 0.003496758872643113\n",
            "Iteration: 431/537, Loss: 0.04285042732954025\n",
            "Iteration: 432/537, Loss: 0.004860215820372105\n",
            "Iteration: 433/537, Loss: 0.002551137236878276\n",
            "Iteration: 434/537, Loss: 0.005285197403281927\n",
            "Iteration: 435/537, Loss: 0.000258245796430856\n",
            "Iteration: 436/537, Loss: 0.00031589821446686983\n",
            "Iteration: 437/537, Loss: 0.0008910189499147236\n",
            "Iteration: 438/537, Loss: 0.0013788449577987194\n",
            "Iteration: 439/537, Loss: 0.0007440581684932113\n",
            "Iteration: 440/537, Loss: 0.004572414793074131\n",
            "Iteration: 441/537, Loss: 0.0019187022699043155\n",
            "Iteration: 442/537, Loss: 0.0009736443171277642\n",
            "Iteration: 443/537, Loss: 0.10279642790555954\n",
            "Iteration: 444/537, Loss: 0.007535010576248169\n",
            "Iteration: 445/537, Loss: 0.003586537204682827\n",
            "Iteration: 446/537, Loss: 0.002072062809020281\n",
            "Iteration: 447/537, Loss: 0.0011202723253518343\n",
            "Iteration: 448/537, Loss: 0.01778261736035347\n",
            "Iteration: 449/537, Loss: 0.0002626844507176429\n",
            "Iteration: 450/537, Loss: 0.0015298370271921158\n",
            "Iteration: 451/537, Loss: 0.0007058496121317148\n",
            "Iteration: 452/537, Loss: 0.0009871928486973047\n",
            "Iteration: 453/537, Loss: 0.0005270299152471125\n",
            "Iteration: 454/537, Loss: 0.00030771299498155713\n",
            "Iteration: 455/537, Loss: 0.0025839447043836117\n",
            "Iteration: 456/537, Loss: 0.0008654837147332728\n",
            "Iteration: 457/537, Loss: 0.002729033352807164\n",
            "Iteration: 458/537, Loss: 0.0016192891635000706\n",
            "Iteration: 459/537, Loss: 0.00026305863866582513\n",
            "Iteration: 460/537, Loss: 0.002803899347782135\n",
            "Iteration: 461/537, Loss: 0.0011051410110667348\n",
            "Iteration: 462/537, Loss: 0.0015983772464096546\n",
            "Iteration: 463/537, Loss: 0.0010613612830638885\n",
            "Iteration: 464/537, Loss: 0.0019741859287023544\n",
            "Iteration: 465/537, Loss: 0.0066395970061421394\n",
            "Iteration: 466/537, Loss: 0.0012790740001946688\n",
            "Iteration: 467/537, Loss: 0.0014163374435156584\n",
            "Iteration: 468/537, Loss: 0.0003010617510881275\n",
            "Iteration: 469/537, Loss: 0.0033191365655511618\n",
            "Iteration: 470/537, Loss: 0.00039609172381460667\n",
            "Iteration: 471/537, Loss: 0.0031746490858495235\n",
            "Iteration: 472/537, Loss: 0.0013844314962625504\n",
            "Iteration: 473/537, Loss: 0.0008209328516386449\n",
            "Iteration: 474/537, Loss: 0.0012336557265371084\n",
            "Iteration: 475/537, Loss: 0.007740667089819908\n",
            "Iteration: 476/537, Loss: 0.004153958987444639\n",
            "Iteration: 477/537, Loss: 0.0017872112803161144\n",
            "Iteration: 478/537, Loss: 0.000938708777539432\n",
            "Iteration: 479/537, Loss: 0.0056665195152163506\n",
            "Iteration: 480/537, Loss: 0.0007052231230773032\n",
            "Iteration: 481/537, Loss: 0.0004082103550899774\n",
            "Iteration: 482/537, Loss: 0.001969500444829464\n",
            "Iteration: 483/537, Loss: 0.0011186429765075445\n",
            "Iteration: 484/537, Loss: 0.0010264106094837189\n",
            "Iteration: 485/537, Loss: 0.26116806268692017\n",
            "Iteration: 486/537, Loss: 0.08097875863313675\n",
            "Iteration: 487/537, Loss: 0.0007010903209447861\n",
            "Iteration: 488/537, Loss: 0.002326112939044833\n",
            "Iteration: 489/537, Loss: 0.004063290078192949\n",
            "Iteration: 490/537, Loss: 0.006294225342571735\n",
            "Iteration: 491/537, Loss: 0.009059787727892399\n",
            "Iteration: 492/537, Loss: 0.0007430093246512115\n",
            "Iteration: 493/537, Loss: 0.0005194421391934156\n",
            "Iteration: 494/537, Loss: 0.0009974222630262375\n",
            "Iteration: 495/537, Loss: 0.0007221514824777842\n",
            "Iteration: 496/537, Loss: 0.0007803452317602932\n",
            "Iteration: 497/537, Loss: 0.002445450983941555\n",
            "Iteration: 498/537, Loss: 0.00047303157043643296\n",
            "Iteration: 499/537, Loss: 0.0032109476160258055\n",
            "Iteration: 500/537, Loss: 0.006983441300690174\n",
            "Iteration: 501/537, Loss: 0.03465140983462334\n",
            "Iteration: 502/537, Loss: 0.0016529314452782273\n",
            "Iteration: 503/537, Loss: 0.008764061145484447\n",
            "Iteration: 504/537, Loss: 0.00040533154970034957\n",
            "Iteration: 505/537, Loss: 0.0005379914655350149\n",
            "Iteration: 506/537, Loss: 0.0013614618219435215\n",
            "Iteration: 507/537, Loss: 0.007615872658789158\n",
            "Iteration: 508/537, Loss: 0.025140414014458656\n",
            "Iteration: 509/537, Loss: 0.002254403894767165\n",
            "Iteration: 510/537, Loss: 0.00039034715155139565\n",
            "Iteration: 511/537, Loss: 0.005531480070203543\n",
            "Iteration: 512/537, Loss: 0.00021685317915398628\n",
            "Iteration: 513/537, Loss: 0.006696333643049002\n",
            "Iteration: 514/537, Loss: 0.0014255823334679008\n",
            "Iteration: 515/537, Loss: 0.000662148289848119\n",
            "Iteration: 516/537, Loss: 0.006180212367326021\n",
            "Iteration: 517/537, Loss: 0.0004247631295584142\n",
            "Iteration: 518/537, Loss: 0.0004774856788571924\n",
            "Iteration: 519/537, Loss: 0.0003768668684642762\n",
            "Iteration: 520/537, Loss: 0.0032515577040612698\n",
            "Iteration: 521/537, Loss: 0.002635873854160309\n",
            "Iteration: 522/537, Loss: 0.0003057832655031234\n",
            "Iteration: 523/537, Loss: 0.005409630015492439\n",
            "Iteration: 524/537, Loss: 0.001306468271650374\n",
            "Iteration: 525/537, Loss: 0.0008350387797690928\n",
            "Iteration: 526/537, Loss: 0.0009726678836159408\n",
            "Iteration: 527/537, Loss: 0.1211235374212265\n",
            "Iteration: 528/537, Loss: 0.017049673944711685\n",
            "Iteration: 529/537, Loss: 0.0031551739666610956\n",
            "Iteration: 530/537, Loss: 0.001158570870757103\n",
            "Iteration: 531/537, Loss: 0.0031931933481246233\n",
            "Iteration: 532/537, Loss: 0.006958164740353823\n",
            "Iteration: 533/537, Loss: 0.010566352866590023\n",
            "Iteration: 534/537, Loss: 0.001408528070896864\n",
            "Iteration: 535/537, Loss: 0.10921789705753326\n",
            "Iteration: 536/537, Loss: 0.002040573861449957\n",
            "Iteration: 537/537, Loss: 0.04353521391749382\n",
            "Epoch 44 train loss: 0.018174238165927656\n",
            "Epoch 44 test loss: 0.06753486510122592\n",
            "-------------------------------------\n",
            "Iteration: 1/537, Loss: 0.0010330947116017342\n",
            "Iteration: 2/537, Loss: 0.0007621689583174884\n",
            "Iteration: 3/537, Loss: 0.002158928429707885\n",
            "Iteration: 4/537, Loss: 0.002528109587728977\n",
            "Iteration: 5/537, Loss: 0.001748979208059609\n",
            "Iteration: 6/537, Loss: 0.0023455906193703413\n",
            "Iteration: 7/537, Loss: 0.00042047238093800843\n",
            "Iteration: 8/537, Loss: 0.0007558488287031651\n",
            "Iteration: 9/537, Loss: 0.01790444552898407\n",
            "Iteration: 10/537, Loss: 0.0011125423479825258\n",
            "Iteration: 11/537, Loss: 0.0007802796317264438\n",
            "Iteration: 12/537, Loss: 0.044658053666353226\n",
            "Iteration: 13/537, Loss: 0.0008546277531422675\n",
            "Iteration: 14/537, Loss: 0.0065709431655704975\n",
            "Iteration: 15/537, Loss: 0.002896046731621027\n",
            "Iteration: 16/537, Loss: 0.0006604285445064306\n",
            "Iteration: 17/537, Loss: 0.012329156510531902\n",
            "Iteration: 18/537, Loss: 0.000369039160432294\n",
            "Iteration: 19/537, Loss: 0.0035158481914550066\n",
            "Iteration: 20/537, Loss: 0.002299209591001272\n",
            "Iteration: 21/537, Loss: 0.001881201984360814\n",
            "Iteration: 22/537, Loss: 0.00043286988511681557\n",
            "Iteration: 23/537, Loss: 0.0006272265454754233\n",
            "Iteration: 24/537, Loss: 0.0028042595367878675\n",
            "Iteration: 25/537, Loss: 0.0005938163958489895\n",
            "Iteration: 26/537, Loss: 0.009236915037035942\n",
            "Iteration: 27/537, Loss: 0.002183607080951333\n",
            "Iteration: 28/537, Loss: 0.0017301226034760475\n",
            "Iteration: 29/537, Loss: 0.0006885930197313428\n",
            "Iteration: 30/537, Loss: 0.013510769233107567\n",
            "Iteration: 31/537, Loss: 0.0007053085137158632\n",
            "Iteration: 32/537, Loss: 0.0011300942860543728\n",
            "Iteration: 33/537, Loss: 0.0011902564438059926\n",
            "Iteration: 34/537, Loss: 0.0033058887347579002\n",
            "Iteration: 35/537, Loss: 0.14509646594524384\n",
            "Iteration: 36/537, Loss: 0.011161117814481258\n",
            "Iteration: 37/537, Loss: 0.3006076216697693\n",
            "Iteration: 38/537, Loss: 0.0007048729457892478\n",
            "Iteration: 39/537, Loss: 0.0001860527554526925\n",
            "Iteration: 40/537, Loss: 0.0024714460596442223\n",
            "Iteration: 41/537, Loss: 0.0016939445631578565\n",
            "Iteration: 42/537, Loss: 0.0017191956285387278\n",
            "Iteration: 43/537, Loss: 0.005545883439481258\n",
            "Iteration: 44/537, Loss: 0.0004239986010361463\n",
            "Iteration: 45/537, Loss: 0.0031059281900525093\n",
            "Iteration: 46/537, Loss: 0.003971996251493692\n",
            "Iteration: 47/537, Loss: 0.0014241107273846865\n",
            "Iteration: 48/537, Loss: 0.08295983821153641\n",
            "Iteration: 49/537, Loss: 0.0014115457888692617\n",
            "Iteration: 50/537, Loss: 0.00047320747398771346\n",
            "Iteration: 51/537, Loss: 0.5570067167282104\n",
            "Iteration: 52/537, Loss: 0.004322309046983719\n",
            "Iteration: 53/537, Loss: 0.003381200600415468\n",
            "Iteration: 54/537, Loss: 0.0002981396100949496\n",
            "Iteration: 55/537, Loss: 0.0020165755413472652\n",
            "Iteration: 56/537, Loss: 0.02610328234732151\n",
            "Iteration: 57/537, Loss: 0.008513875305652618\n",
            "Iteration: 58/537, Loss: 0.0023818612098693848\n",
            "Iteration: 59/537, Loss: 0.0006601169588975608\n",
            "Iteration: 60/537, Loss: 0.00018485492910258472\n",
            "Iteration: 61/537, Loss: 0.0010061400244012475\n",
            "Iteration: 62/537, Loss: 0.0022399364970624447\n",
            "Iteration: 63/537, Loss: 0.00019476852321531624\n",
            "Iteration: 64/537, Loss: 0.0004762964090332389\n",
            "Iteration: 65/537, Loss: 0.003950150217860937\n",
            "Iteration: 66/537, Loss: 0.0008784430101513863\n",
            "Iteration: 67/537, Loss: 0.005269535351544619\n",
            "Iteration: 68/537, Loss: 0.0014315127627924085\n",
            "Iteration: 69/537, Loss: 0.002605295041576028\n",
            "Iteration: 70/537, Loss: 0.0011819174978882074\n",
            "Iteration: 71/537, Loss: 0.0004741785814985633\n",
            "Iteration: 72/537, Loss: 0.004292218014597893\n",
            "Iteration: 73/537, Loss: 0.0003815229865722358\n",
            "Iteration: 74/537, Loss: 0.0028556552715599537\n",
            "Iteration: 75/537, Loss: 0.0012096058344468474\n",
            "Iteration: 76/537, Loss: 0.0015819207765161991\n",
            "Iteration: 77/537, Loss: 0.000239783083088696\n",
            "Iteration: 78/537, Loss: 0.0017142186406999826\n",
            "Iteration: 79/537, Loss: 0.0021771928295493126\n",
            "Iteration: 80/537, Loss: 0.0018970902310684323\n",
            "Iteration: 81/537, Loss: 0.016421783715486526\n",
            "Iteration: 82/537, Loss: 0.0013486014213413\n",
            "Iteration: 83/537, Loss: 0.0005417472566477954\n",
            "Iteration: 84/537, Loss: 0.13808877766132355\n",
            "Iteration: 85/537, Loss: 0.0010337982093915343\n",
            "Iteration: 86/537, Loss: 0.0021135066635906696\n",
            "Iteration: 87/537, Loss: 0.0006929134833626449\n",
            "Iteration: 88/537, Loss: 0.002736513502895832\n",
            "Iteration: 89/537, Loss: 0.0034566312097012997\n",
            "Iteration: 90/537, Loss: 0.0007189773605205119\n",
            "Iteration: 91/537, Loss: 0.0033130445517599583\n",
            "Iteration: 92/537, Loss: 0.0015727321151643991\n",
            "Iteration: 93/537, Loss: 0.003982848022133112\n",
            "Iteration: 94/537, Loss: 0.2009618878364563\n",
            "Iteration: 95/537, Loss: 0.00040981313213706017\n",
            "Iteration: 96/537, Loss: 0.006061157677322626\n",
            "Iteration: 97/537, Loss: 0.00035576761001721025\n",
            "Iteration: 98/537, Loss: 0.0016870995750650764\n",
            "Iteration: 99/537, Loss: 0.017024049535393715\n",
            "Iteration: 100/537, Loss: 0.00033836212242022157\n",
            "Iteration: 101/537, Loss: 0.005283970385789871\n",
            "Iteration: 102/537, Loss: 0.0008246427751146257\n",
            "Iteration: 103/537, Loss: 0.0015661905054003\n",
            "Iteration: 104/537, Loss: 0.0005722364294342697\n",
            "Iteration: 105/537, Loss: 0.002499155467376113\n",
            "Iteration: 106/537, Loss: 0.003537749405950308\n",
            "Iteration: 107/537, Loss: 0.0008879508823156357\n",
            "Iteration: 108/537, Loss: 0.000346552231349051\n",
            "Iteration: 109/537, Loss: 0.0004679819685406983\n",
            "Iteration: 110/537, Loss: 0.010988228023052216\n",
            "Iteration: 111/537, Loss: 0.0005373707390390337\n",
            "Iteration: 112/537, Loss: 0.0034460891038179398\n",
            "Iteration: 113/537, Loss: 0.005443014204502106\n",
            "Iteration: 114/537, Loss: 0.000387747393688187\n",
            "Iteration: 115/537, Loss: 0.0018126687500625849\n",
            "Iteration: 116/537, Loss: 0.0007971492595970631\n",
            "Iteration: 117/537, Loss: 0.006619678344577551\n",
            "Iteration: 118/537, Loss: 0.0036746361292898655\n",
            "Iteration: 119/537, Loss: 0.002809676807373762\n",
            "Iteration: 120/537, Loss: 0.0005111513892188668\n",
            "Iteration: 121/537, Loss: 0.0003026160120498389\n",
            "Iteration: 122/537, Loss: 0.07638431340456009\n",
            "Iteration: 123/537, Loss: 0.004772636108100414\n",
            "Iteration: 124/537, Loss: 0.0003079686430282891\n",
            "Iteration: 125/537, Loss: 0.014526226557791233\n",
            "Iteration: 126/537, Loss: 0.0060248724184930325\n",
            "Iteration: 127/537, Loss: 0.026895031332969666\n",
            "Iteration: 128/537, Loss: 0.0005809303838759661\n",
            "Iteration: 129/537, Loss: 0.000832527584861964\n",
            "Iteration: 130/537, Loss: 0.001540790661238134\n",
            "Iteration: 131/537, Loss: 0.10988020151853561\n",
            "Iteration: 132/537, Loss: 0.0006276394124142826\n",
            "Iteration: 133/537, Loss: 0.013665702193975449\n",
            "Iteration: 134/537, Loss: 0.0009484580368734896\n",
            "Iteration: 135/537, Loss: 0.005881026387214661\n",
            "Iteration: 136/537, Loss: 0.00021160076721571386\n",
            "Iteration: 137/537, Loss: 0.00021234193991404027\n",
            "Iteration: 138/537, Loss: 0.007278812117874622\n",
            "Iteration: 139/537, Loss: 0.005891870707273483\n",
            "Iteration: 140/537, Loss: 0.0018211973365396261\n",
            "Iteration: 141/537, Loss: 0.0008168895146809518\n",
            "Iteration: 142/537, Loss: 0.0007624131394550204\n",
            "Iteration: 143/537, Loss: 0.0005619601579383016\n",
            "Iteration: 144/537, Loss: 0.10645097494125366\n",
            "Iteration: 145/537, Loss: 0.0009810053743422031\n",
            "Iteration: 146/537, Loss: 0.0008999177371151745\n",
            "Iteration: 147/537, Loss: 0.03244452551007271\n",
            "Iteration: 148/537, Loss: 0.0005371212610043585\n",
            "Iteration: 149/537, Loss: 0.001049649901688099\n",
            "Iteration: 150/537, Loss: 0.0001924586103996262\n",
            "Iteration: 151/537, Loss: 0.0011487302836030722\n",
            "Iteration: 152/537, Loss: 0.004606548696756363\n",
            "Iteration: 153/537, Loss: 0.0004216576926410198\n",
            "Iteration: 154/537, Loss: 0.001016982365399599\n",
            "Iteration: 155/537, Loss: 0.0019565036054700613\n",
            "Iteration: 156/537, Loss: 0.0025413355324417353\n",
            "Iteration: 157/537, Loss: 0.002208175603300333\n",
            "Iteration: 158/537, Loss: 0.0004669681948143989\n",
            "Iteration: 159/537, Loss: 0.00018215707677882165\n",
            "Iteration: 160/537, Loss: 0.0007584983832202852\n",
            "Iteration: 161/537, Loss: 0.0016174889169633389\n",
            "Iteration: 162/537, Loss: 0.0023593734949827194\n",
            "Iteration: 163/537, Loss: 0.0006742209661751986\n",
            "Iteration: 164/537, Loss: 0.0019962412770837545\n",
            "Iteration: 165/537, Loss: 0.0014592825900763273\n",
            "Iteration: 166/537, Loss: 0.00022281745623331517\n",
            "Iteration: 167/537, Loss: 0.003296460025012493\n",
            "Iteration: 168/537, Loss: 0.018182968720793724\n",
            "Iteration: 169/537, Loss: 0.0012778131058439612\n",
            "Iteration: 170/537, Loss: 0.0018763828556984663\n",
            "Iteration: 171/537, Loss: 0.0006016826955601573\n",
            "Iteration: 172/537, Loss: 0.0023528996389359236\n",
            "Iteration: 173/537, Loss: 0.007793218828737736\n",
            "Iteration: 174/537, Loss: 0.0009306574938818812\n",
            "Iteration: 175/537, Loss: 0.020351041108369827\n",
            "Iteration: 176/537, Loss: 0.003275142749771476\n",
            "Iteration: 177/537, Loss: 0.003592086723074317\n",
            "Iteration: 178/537, Loss: 0.0014317897148430347\n",
            "Iteration: 179/537, Loss: 0.0017685143975540996\n",
            "Iteration: 180/537, Loss: 1.0558854341506958\n",
            "Iteration: 181/537, Loss: 0.0006170195410959423\n",
            "Iteration: 182/537, Loss: 0.0025076987221837044\n",
            "Iteration: 183/537, Loss: 0.0024705124087631702\n",
            "Iteration: 184/537, Loss: 0.026305977255105972\n",
            "Iteration: 185/537, Loss: 0.0001872466818895191\n",
            "Iteration: 186/537, Loss: 0.024340761825442314\n",
            "Iteration: 187/537, Loss: 0.0028129257261753082\n",
            "Iteration: 188/537, Loss: 0.0012919239234179258\n",
            "Iteration: 189/537, Loss: 0.002949170069769025\n",
            "Iteration: 190/537, Loss: 0.0008217059075832367\n",
            "Iteration: 191/537, Loss: 0.002643701620399952\n",
            "Iteration: 192/537, Loss: 0.2522362768650055\n",
            "Iteration: 193/537, Loss: 0.0023428143467754126\n",
            "Iteration: 194/537, Loss: 0.04323532059788704\n",
            "Iteration: 195/537, Loss: 0.05298469588160515\n",
            "Iteration: 196/537, Loss: 0.00232090731151402\n",
            "Iteration: 197/537, Loss: 0.03680460527539253\n",
            "Iteration: 198/537, Loss: 0.0006451536319218576\n",
            "Iteration: 199/537, Loss: 0.0005008481675758958\n",
            "Iteration: 200/537, Loss: 0.01238402258604765\n",
            "Iteration: 201/537, Loss: 0.004221617244184017\n",
            "Iteration: 202/537, Loss: 0.0003622924559749663\n",
            "Iteration: 203/537, Loss: 0.0037387993652373552\n",
            "Iteration: 204/537, Loss: 0.0015648661646991968\n",
            "Iteration: 205/537, Loss: 0.0018272454617545009\n",
            "Iteration: 206/537, Loss: 0.01751522906124592\n",
            "Iteration: 207/537, Loss: 0.0004063732922077179\n",
            "Iteration: 208/537, Loss: 0.010378951206803322\n",
            "Iteration: 209/537, Loss: 0.0006657298072241247\n",
            "Iteration: 210/537, Loss: 0.001295103575102985\n",
            "Iteration: 211/537, Loss: 0.0009384901495650411\n",
            "Iteration: 212/537, Loss: 0.015469485893845558\n",
            "Iteration: 213/537, Loss: 0.0006800139090046287\n",
            "Iteration: 214/537, Loss: 0.003273243084549904\n",
            "Iteration: 215/537, Loss: 0.005638085305690765\n",
            "Iteration: 216/537, Loss: 0.45069190859794617\n",
            "Iteration: 217/537, Loss: 0.0005512703792192042\n",
            "Iteration: 218/537, Loss: 0.0006692582974210382\n",
            "Iteration: 219/537, Loss: 0.002690819324925542\n",
            "Iteration: 220/537, Loss: 0.004625796340405941\n",
            "Iteration: 221/537, Loss: 0.0026140641421079636\n",
            "Iteration: 222/537, Loss: 0.020796023309230804\n",
            "Iteration: 223/537, Loss: 0.00288798613473773\n",
            "Iteration: 224/537, Loss: 0.001249506021849811\n",
            "Iteration: 225/537, Loss: 0.0029768534004688263\n",
            "Iteration: 226/537, Loss: 0.0008027462754398584\n",
            "Iteration: 227/537, Loss: 0.0007075539906509221\n",
            "Iteration: 228/537, Loss: 0.0008516279049217701\n",
            "Iteration: 229/537, Loss: 0.008155676536262035\n",
            "Iteration: 230/537, Loss: 0.03118584305047989\n",
            "Iteration: 231/537, Loss: 0.006231571547687054\n",
            "Iteration: 232/537, Loss: 0.00012324631097726524\n",
            "Iteration: 233/537, Loss: 0.008203035220503807\n",
            "Iteration: 234/537, Loss: 0.002625059802085161\n",
            "Iteration: 235/537, Loss: 0.0004974756739102304\n",
            "Iteration: 236/537, Loss: 0.000601485138759017\n",
            "Iteration: 237/537, Loss: 0.0023739731404930353\n",
            "Iteration: 238/537, Loss: 0.0019084443338215351\n",
            "Iteration: 239/537, Loss: 0.0011396441841498017\n",
            "Iteration: 240/537, Loss: 0.028595564886927605\n",
            "Iteration: 241/537, Loss: 0.004950256086885929\n",
            "Iteration: 242/537, Loss: 0.01964484341442585\n",
            "Iteration: 243/537, Loss: 0.003183310618624091\n",
            "Iteration: 244/537, Loss: 0.0008837864734232426\n",
            "Iteration: 245/537, Loss: 0.07217974960803986\n",
            "Iteration: 246/537, Loss: 0.0016929465346038342\n",
            "Iteration: 247/537, Loss: 0.0004854337312281132\n",
            "Iteration: 248/537, Loss: 0.0019195645581930876\n",
            "Iteration: 249/537, Loss: 0.002771882340312004\n",
            "Iteration: 250/537, Loss: 0.036152731627225876\n",
            "Iteration: 251/537, Loss: 0.0043370602652430534\n",
            "Iteration: 252/537, Loss: 0.0014239021111279726\n",
            "Iteration: 253/537, Loss: 0.000685745500959456\n",
            "Iteration: 254/537, Loss: 0.0014541973359882832\n",
            "Iteration: 255/537, Loss: 0.004473252687603235\n",
            "Iteration: 256/537, Loss: 0.011007609777152538\n",
            "Iteration: 257/537, Loss: 0.002922412008047104\n",
            "Iteration: 258/537, Loss: 0.000780093134380877\n",
            "Iteration: 259/537, Loss: 0.0012382966233417392\n",
            "Iteration: 260/537, Loss: 0.0016350947553291917\n",
            "Iteration: 261/537, Loss: 0.0017256063874810934\n",
            "Iteration: 262/537, Loss: 0.0508512407541275\n",
            "Iteration: 263/537, Loss: 0.0007434310973621905\n",
            "Iteration: 264/537, Loss: 0.0017289514653384686\n",
            "Iteration: 265/537, Loss: 0.0007741398876532912\n",
            "Iteration: 266/537, Loss: 0.0009118909947574139\n",
            "Iteration: 267/537, Loss: 0.011857328936457634\n",
            "Iteration: 268/537, Loss: 0.0014311290578916669\n",
            "Iteration: 269/537, Loss: 0.0010636893566697836\n",
            "Iteration: 270/537, Loss: 0.017025645822286606\n",
            "Iteration: 271/537, Loss: 0.005264816340059042\n",
            "Iteration: 272/537, Loss: 0.0015797301894053817\n",
            "Iteration: 273/537, Loss: 0.002398229669779539\n",
            "Iteration: 274/537, Loss: 0.00024396883964072913\n",
            "Iteration: 275/537, Loss: 0.005750142503529787\n",
            "Iteration: 276/537, Loss: 0.002271578647196293\n",
            "Iteration: 277/537, Loss: 0.005173079669475555\n",
            "Iteration: 278/537, Loss: 0.0025724745355546474\n",
            "Iteration: 279/537, Loss: 0.15808334946632385\n",
            "Iteration: 280/537, Loss: 0.0008439107332378626\n",
            "Iteration: 281/537, Loss: 0.008020168170332909\n",
            "Iteration: 282/537, Loss: 0.0012403831351548433\n",
            "Iteration: 283/537, Loss: 0.0010903474176302552\n",
            "Iteration: 284/537, Loss: 0.0028151064179837704\n",
            "Iteration: 285/537, Loss: 0.0016187273431569338\n",
            "Iteration: 286/537, Loss: 0.00041991149191744626\n",
            "Iteration: 287/537, Loss: 0.0005546715692616999\n",
            "Iteration: 288/537, Loss: 0.0003964918723795563\n",
            "Iteration: 289/537, Loss: 0.0063865953125059605\n",
            "Iteration: 290/537, Loss: 0.0007155437488108873\n",
            "Iteration: 291/537, Loss: 0.004614632576704025\n",
            "Iteration: 292/537, Loss: 0.0012849237537011504\n",
            "Iteration: 293/537, Loss: 0.02700747549533844\n",
            "Iteration: 294/537, Loss: 0.003641905263066292\n",
            "Iteration: 295/537, Loss: 0.007058432325720787\n",
            "Iteration: 296/537, Loss: 0.0010546066332608461\n",
            "Iteration: 297/537, Loss: 0.0010253444779664278\n",
            "Iteration: 298/537, Loss: 0.0007554428884759545\n",
            "Iteration: 299/537, Loss: 0.006224868353456259\n",
            "Iteration: 300/537, Loss: 0.001203196938149631\n",
            "Iteration: 301/537, Loss: 0.002317936159670353\n",
            "Iteration: 302/537, Loss: 0.0010263381991535425\n",
            "Iteration: 303/537, Loss: 0.0011431683087721467\n",
            "Iteration: 304/537, Loss: 0.0017017030622810125\n",
            "Iteration: 305/537, Loss: 0.0032933347392827272\n",
            "Iteration: 306/537, Loss: 0.0008482924895361066\n",
            "Iteration: 307/537, Loss: 0.0022926011588424444\n",
            "Iteration: 308/537, Loss: 0.001425442984327674\n",
            "Iteration: 309/537, Loss: 0.00246731610968709\n",
            "Iteration: 310/537, Loss: 0.004193358588963747\n",
            "Iteration: 311/537, Loss: 0.0013568232534453273\n",
            "Iteration: 312/537, Loss: 0.0018169351387768984\n",
            "Iteration: 313/537, Loss: 0.0033574113622307777\n",
            "Iteration: 314/537, Loss: 0.0006937543512322009\n",
            "Iteration: 315/537, Loss: 0.006178184412419796\n",
            "Iteration: 316/537, Loss: 0.0005154706886969507\n",
            "Iteration: 317/537, Loss: 0.0005360579816624522\n",
            "Iteration: 318/537, Loss: 0.00024046457838267088\n",
            "Iteration: 319/537, Loss: 0.0005454880301840603\n",
            "Iteration: 320/537, Loss: 0.1459587961435318\n",
            "Iteration: 321/537, Loss: 0.0020559574477374554\n",
            "Iteration: 322/537, Loss: 0.0006749387248419225\n",
            "Iteration: 323/537, Loss: 0.0003326383884996176\n",
            "Iteration: 324/537, Loss: 0.00040467808139510453\n",
            "Iteration: 325/537, Loss: 0.001116825733333826\n",
            "Iteration: 326/537, Loss: 0.000752126332372427\n",
            "Iteration: 327/537, Loss: 0.0015331987524405122\n",
            "Iteration: 328/537, Loss: 0.0010519835632294416\n",
            "Iteration: 329/537, Loss: 0.015362723730504513\n",
            "Iteration: 330/537, Loss: 0.0006577071035280824\n",
            "Iteration: 331/537, Loss: 0.0007325322949327528\n",
            "Iteration: 332/537, Loss: 0.006272526457905769\n",
            "Iteration: 333/537, Loss: 0.006331659387797117\n",
            "Iteration: 334/537, Loss: 0.0011958801187574863\n",
            "Iteration: 335/537, Loss: 0.006881692446768284\n",
            "Iteration: 336/537, Loss: 0.0003035836271010339\n",
            "Iteration: 337/537, Loss: 0.002226355252787471\n",
            "Iteration: 338/537, Loss: 0.5401076078414917\n",
            "Iteration: 339/537, Loss: 0.0003861747100017965\n",
            "Iteration: 340/537, Loss: 0.0018785037100315094\n",
            "Iteration: 341/537, Loss: 0.002305397531017661\n",
            "Iteration: 342/537, Loss: 0.0014936255756765604\n",
            "Iteration: 343/537, Loss: 0.0009548624511808157\n",
            "Iteration: 344/537, Loss: 0.038421545177698135\n",
            "Iteration: 345/537, Loss: 0.004577852785587311\n",
            "Iteration: 346/537, Loss: 0.0004137652285862714\n",
            "Iteration: 347/537, Loss: 0.00036272555007599294\n",
            "Iteration: 348/537, Loss: 0.002306695096194744\n",
            "Iteration: 349/537, Loss: 0.007037006318569183\n",
            "Iteration: 350/537, Loss: 0.09626324474811554\n",
            "Iteration: 351/537, Loss: 0.0031193264294415712\n",
            "Iteration: 352/537, Loss: 0.001482458203099668\n",
            "Iteration: 353/537, Loss: 0.019736813381314278\n",
            "Iteration: 354/537, Loss: 0.0025118677876889706\n",
            "Iteration: 355/537, Loss: 0.0009706781129352748\n",
            "Iteration: 356/537, Loss: 0.0007360938470810652\n",
            "Iteration: 357/537, Loss: 0.00032761480542831123\n",
            "Iteration: 358/537, Loss: 0.04163234308362007\n",
            "Iteration: 359/537, Loss: 0.0004144725971855223\n",
            "Iteration: 360/537, Loss: 0.005959545262157917\n",
            "Iteration: 361/537, Loss: 0.0005487028392963111\n",
            "Iteration: 362/537, Loss: 0.0006990334368310869\n",
            "Iteration: 363/537, Loss: 0.0005220358143560588\n",
            "Iteration: 364/537, Loss: 0.0017963219434022903\n",
            "Iteration: 365/537, Loss: 0.0009656926267780364\n",
            "Iteration: 366/537, Loss: 0.0002277326420880854\n",
            "Iteration: 367/537, Loss: 0.0041056424379348755\n",
            "Iteration: 368/537, Loss: 0.006372104398906231\n",
            "Iteration: 369/537, Loss: 0.0006004509050399065\n",
            "Iteration: 370/537, Loss: 0.00035850240965373814\n",
            "Iteration: 371/537, Loss: 0.003655332373455167\n",
            "Iteration: 372/537, Loss: 0.0007625530706718564\n",
            "Iteration: 373/537, Loss: 0.011878359131515026\n",
            "Iteration: 374/537, Loss: 0.00015899393474683166\n",
            "Iteration: 375/537, Loss: 0.012347398325800896\n",
            "Iteration: 376/537, Loss: 0.00010981257946696132\n",
            "Iteration: 377/537, Loss: 0.0014754639705643058\n",
            "Iteration: 378/537, Loss: 0.0005346235702745616\n",
            "Iteration: 379/537, Loss: 0.0023387714754790068\n",
            "Iteration: 380/537, Loss: 0.0001487750851083547\n",
            "Iteration: 381/537, Loss: 0.0009254058240912855\n",
            "Iteration: 382/537, Loss: 0.0018468043999746442\n",
            "Iteration: 383/537, Loss: 0.001509317895397544\n",
            "Iteration: 384/537, Loss: 0.0007655881345272064\n",
            "Iteration: 385/537, Loss: 0.0073734973557293415\n",
            "Iteration: 386/537, Loss: 0.0014106057351455092\n",
            "Iteration: 387/537, Loss: 0.001608921797014773\n",
            "Iteration: 388/537, Loss: 0.3377498686313629\n",
            "Iteration: 389/537, Loss: 0.00039438658859580755\n",
            "Iteration: 390/537, Loss: 0.07869924604892731\n",
            "Iteration: 391/537, Loss: 0.00011604044993873686\n",
            "Iteration: 392/537, Loss: 0.0014812371227890253\n",
            "Iteration: 393/537, Loss: 0.0011641251621767879\n",
            "Iteration: 394/537, Loss: 0.00016669162141624838\n",
            "Iteration: 395/537, Loss: 0.0040350910276174545\n",
            "Iteration: 396/537, Loss: 0.00042066420428454876\n",
            "Iteration: 397/537, Loss: 0.004397905431687832\n",
            "Iteration: 398/537, Loss: 0.0016079097986221313\n",
            "Iteration: 399/537, Loss: 0.04832734167575836\n",
            "Iteration: 400/537, Loss: 0.0009396616369485855\n",
            "Iteration: 401/537, Loss: 0.0010354401310905814\n",
            "Iteration: 402/537, Loss: 0.005228008143603802\n",
            "Iteration: 403/537, Loss: 0.00041542729013599455\n",
            "Iteration: 404/537, Loss: 0.00023811009305063635\n",
            "Iteration: 405/537, Loss: 0.00032723305048421025\n",
            "Iteration: 406/537, Loss: 0.004991134162992239\n",
            "Iteration: 407/537, Loss: 0.0008800292853266001\n",
            "Iteration: 408/537, Loss: 0.00022836065909359604\n",
            "Iteration: 409/537, Loss: 0.0008879509987309575\n",
            "Iteration: 410/537, Loss: 0.0007425692747347057\n",
            "Iteration: 411/537, Loss: 0.0023622692096978426\n",
            "Iteration: 412/537, Loss: 0.00025114711024798453\n",
            "Iteration: 413/537, Loss: 0.011182289570569992\n",
            "Iteration: 414/537, Loss: 0.0003245270636398345\n",
            "Iteration: 415/537, Loss: 0.00013055354065727443\n",
            "Iteration: 416/537, Loss: 0.0002815143088810146\n",
            "Iteration: 417/537, Loss: 0.00022253126371651888\n",
            "Iteration: 418/537, Loss: 0.0037688317243009806\n",
            "Iteration: 419/537, Loss: 0.0010200607357546687\n",
            "Iteration: 420/537, Loss: 0.0022450885735452175\n",
            "Iteration: 421/537, Loss: 0.0017454810440540314\n",
            "Iteration: 422/537, Loss: 0.011226173490285873\n",
            "Iteration: 423/537, Loss: 0.008031303063035011\n",
            "Iteration: 424/537, Loss: 0.0008526860037818551\n",
            "Iteration: 425/537, Loss: 0.0006784778670407832\n",
            "Iteration: 426/537, Loss: 0.0002651337126735598\n",
            "Iteration: 427/537, Loss: 0.003612976986914873\n",
            "Iteration: 428/537, Loss: 0.011768829077482224\n",
            "Iteration: 429/537, Loss: 0.0007242562714964151\n",
            "Iteration: 430/537, Loss: 0.005081754177808762\n",
            "Iteration: 431/537, Loss: 0.005606877151876688\n",
            "Iteration: 432/537, Loss: 0.0013091230066493154\n",
            "Iteration: 433/537, Loss: 0.0014442019164562225\n",
            "Iteration: 434/537, Loss: 0.0017764612566679716\n",
            "Iteration: 435/537, Loss: 0.0009563460480421782\n",
            "Iteration: 436/537, Loss: 0.0013404699275270104\n",
            "Iteration: 437/537, Loss: 0.0014772076392546296\n",
            "Iteration: 438/537, Loss: 0.0002884064451791346\n",
            "Iteration: 439/537, Loss: 0.0006199186318553984\n",
            "Iteration: 440/537, Loss: 0.0006483963807113469\n",
            "Iteration: 441/537, Loss: 0.0006590508855879307\n",
            "Iteration: 442/537, Loss: 0.00031755369855090976\n",
            "Iteration: 443/537, Loss: 0.00031859902082942426\n",
            "Iteration: 444/537, Loss: 0.00018502253806218505\n",
            "Iteration: 445/537, Loss: 0.004570799879729748\n",
            "Iteration: 446/537, Loss: 0.0011565827298909426\n",
            "Iteration: 447/537, Loss: 0.00024652041611261666\n",
            "Iteration: 448/537, Loss: 6.359558028634638e-05\n",
            "Iteration: 449/537, Loss: 0.00018623191863298416\n",
            "Iteration: 450/537, Loss: 0.0027924326714128256\n",
            "Iteration: 451/537, Loss: 0.0005322639481164515\n",
            "Iteration: 452/537, Loss: 0.00021496022236533463\n",
            "Iteration: 453/537, Loss: 0.0006955891149118543\n",
            "Iteration: 454/537, Loss: 0.0007796564023010433\n",
            "Iteration: 455/537, Loss: 0.0007213957142084837\n",
            "Iteration: 456/537, Loss: 0.0006261005182750523\n",
            "Iteration: 457/537, Loss: 0.00011061746045015752\n",
            "Iteration: 458/537, Loss: 0.00047574457130394876\n",
            "Iteration: 459/537, Loss: 0.004922126419842243\n",
            "Iteration: 460/537, Loss: 0.00028555281460285187\n",
            "Iteration: 461/537, Loss: 0.00017405685503035784\n",
            "Iteration: 462/537, Loss: 0.00026711454847827554\n",
            "Iteration: 463/537, Loss: 0.23521670699119568\n",
            "Iteration: 464/537, Loss: 0.001985160168260336\n",
            "Iteration: 465/537, Loss: 0.0012374413199722767\n",
            "Iteration: 466/537, Loss: 8.078981045400724e-05\n",
            "Iteration: 467/537, Loss: 0.0049127619713544846\n",
            "Iteration: 468/537, Loss: 0.0005175542901270092\n",
            "Iteration: 469/537, Loss: 0.03492157161235809\n",
            "Iteration: 470/537, Loss: 0.00548288831487298\n",
            "Iteration: 471/537, Loss: 0.000820085930172354\n",
            "Iteration: 472/537, Loss: 0.0018694321624934673\n",
            "Iteration: 473/537, Loss: 0.0023418846540153027\n",
            "Iteration: 474/537, Loss: 0.0018340284004807472\n",
            "Iteration: 475/537, Loss: 0.07369042187929153\n",
            "Iteration: 476/537, Loss: 0.002577489707618952\n",
            "Iteration: 477/537, Loss: 0.0002730685519054532\n",
            "Iteration: 478/537, Loss: 0.001273800851777196\n",
            "Iteration: 479/537, Loss: 0.00017676310380920768\n",
            "Iteration: 480/537, Loss: 0.003155617043375969\n",
            "Iteration: 481/537, Loss: 0.0029514399357140064\n",
            "Iteration: 482/537, Loss: 0.00045082869473844767\n",
            "Iteration: 483/537, Loss: 0.00029402031213976443\n",
            "Iteration: 484/537, Loss: 0.2428334504365921\n",
            "Iteration: 485/537, Loss: 0.01206540409475565\n",
            "Iteration: 486/537, Loss: 0.0002693423011805862\n",
            "Iteration: 487/537, Loss: 0.0004850417608395219\n",
            "Iteration: 488/537, Loss: 0.0040338183753192425\n",
            "Iteration: 489/537, Loss: 0.003006149549037218\n",
            "Iteration: 490/537, Loss: 0.0002861707762349397\n",
            "Iteration: 491/537, Loss: 0.0023739361204206944\n",
            "Iteration: 492/537, Loss: 0.0029367150273174047\n",
            "Iteration: 493/537, Loss: 0.00032319597085006535\n",
            "Iteration: 494/537, Loss: 0.0008235748973675072\n",
            "Iteration: 495/537, Loss: 0.008049321360886097\n",
            "Iteration: 496/537, Loss: 0.001609413418918848\n",
            "Iteration: 497/537, Loss: 0.001847648061811924\n",
            "Iteration: 498/537, Loss: 0.00043423479655757546\n",
            "Iteration: 499/537, Loss: 0.0006494595436379313\n",
            "Iteration: 500/537, Loss: 0.0016624410636723042\n",
            "Iteration: 501/537, Loss: 0.000256187777267769\n",
            "Iteration: 502/537, Loss: 0.0002399285149294883\n",
            "Iteration: 503/537, Loss: 0.003443355206400156\n",
            "Iteration: 504/537, Loss: 0.0013957718620076776\n",
            "Iteration: 505/537, Loss: 0.0007778534782119095\n",
            "Iteration: 506/537, Loss: 0.0004725413746200502\n",
            "Iteration: 507/537, Loss: 0.0011495498474687338\n",
            "Iteration: 508/537, Loss: 0.0008423095569014549\n",
            "Iteration: 509/537, Loss: 0.0015617822064086795\n",
            "Iteration: 510/537, Loss: 0.0021846743766218424\n",
            "Iteration: 511/537, Loss: 0.0003922624164260924\n",
            "Iteration: 512/537, Loss: 0.0015966786304488778\n",
            "Iteration: 513/537, Loss: 0.0030311932787299156\n",
            "Iteration: 514/537, Loss: 0.001944260555319488\n",
            "Iteration: 515/537, Loss: 0.0012281137751415372\n",
            "Iteration: 516/537, Loss: 0.0035857013426721096\n",
            "Iteration: 517/537, Loss: 0.0012993433047086\n",
            "Iteration: 518/537, Loss: 0.00302533688955009\n",
            "Iteration: 519/537, Loss: 0.003499955404549837\n",
            "Iteration: 520/537, Loss: 0.0001878480688901618\n",
            "Iteration: 521/537, Loss: 0.0007688474142923951\n",
            "Iteration: 522/537, Loss: 0.9772788286209106\n",
            "Iteration: 523/537, Loss: 0.0019739638082683086\n",
            "Iteration: 524/537, Loss: 0.00022217913647182286\n",
            "Iteration: 525/537, Loss: 0.002031406620517373\n",
            "Iteration: 526/537, Loss: 0.0002663268824107945\n",
            "Iteration: 527/537, Loss: 0.0004979664226993918\n",
            "Iteration: 528/537, Loss: 0.0014791011344641447\n",
            "Iteration: 529/537, Loss: 0.0004302807792555541\n",
            "Iteration: 530/537, Loss: 0.0011935137445107102\n",
            "Iteration: 531/537, Loss: 0.00021973421098664403\n",
            "Iteration: 532/537, Loss: 0.000251901539741084\n",
            "Iteration: 533/537, Loss: 0.006892767734825611\n",
            "Iteration: 534/537, Loss: 0.0002545876777730882\n",
            "Iteration: 535/537, Loss: 0.0024867509491741657\n",
            "Iteration: 536/537, Loss: 6.487645441666245e-05\n",
            "Iteration: 537/537, Loss: 0.3393922448158264\n",
            "Epoch 45 train loss: 0.01622389992342139\n",
            "Epoch 45 test loss: 0.07363599311491406\n",
            "-------------------------------------\n",
            "Iteration: 1/537, Loss: 0.0002387083222856745\n",
            "Iteration: 2/537, Loss: 0.0037177149206399918\n",
            "Iteration: 3/537, Loss: 0.001003027893602848\n",
            "Iteration: 4/537, Loss: 0.000546549039427191\n",
            "Iteration: 5/537, Loss: 0.0027396923396736383\n",
            "Iteration: 6/537, Loss: 0.8169688582420349\n",
            "Iteration: 7/537, Loss: 0.00042040960397571325\n",
            "Iteration: 8/537, Loss: 0.00021943470346741378\n",
            "Iteration: 9/537, Loss: 0.0003065832715947181\n",
            "Iteration: 10/537, Loss: 0.00284271826967597\n",
            "Iteration: 11/537, Loss: 0.001744585344567895\n",
            "Iteration: 12/537, Loss: 0.003202875843271613\n",
            "Iteration: 13/537, Loss: 0.019037866964936256\n",
            "Iteration: 14/537, Loss: 0.002022385597229004\n",
            "Iteration: 15/537, Loss: 0.004213986452668905\n",
            "Iteration: 16/537, Loss: 0.019845575094223022\n",
            "Iteration: 17/537, Loss: 0.0031233688350766897\n",
            "Iteration: 18/537, Loss: 0.001742114662192762\n",
            "Iteration: 19/537, Loss: 0.012482265010476112\n",
            "Iteration: 20/537, Loss: 0.000864522298797965\n",
            "Iteration: 21/537, Loss: 0.0008432921604253352\n",
            "Iteration: 22/537, Loss: 0.01999550126492977\n",
            "Iteration: 23/537, Loss: 0.0014175615506246686\n",
            "Iteration: 24/537, Loss: 0.017841286957263947\n",
            "Iteration: 25/537, Loss: 0.1366361379623413\n",
            "Iteration: 26/537, Loss: 0.0005144680617377162\n",
            "Iteration: 27/537, Loss: 0.10332466661930084\n",
            "Iteration: 28/537, Loss: 0.0066384547390043736\n",
            "Iteration: 29/537, Loss: 0.00597795844078064\n",
            "Iteration: 30/537, Loss: 0.17062318325042725\n",
            "Iteration: 31/537, Loss: 0.0017325880471616983\n",
            "Iteration: 32/537, Loss: 0.0014533729990944266\n",
            "Iteration: 33/537, Loss: 0.0012923217145726085\n",
            "Iteration: 34/537, Loss: 0.005188025068491697\n",
            "Iteration: 35/537, Loss: 0.005177504848688841\n",
            "Iteration: 36/537, Loss: 0.03164992481470108\n",
            "Iteration: 37/537, Loss: 0.010536541230976582\n",
            "Iteration: 38/537, Loss: 0.0015909860376268625\n",
            "Iteration: 39/537, Loss: 0.005174693185836077\n",
            "Iteration: 40/537, Loss: 0.0004677778924815357\n",
            "Iteration: 41/537, Loss: 0.0036544734612107277\n",
            "Iteration: 42/537, Loss: 0.006403103470802307\n",
            "Iteration: 43/537, Loss: 0.00023596514074597508\n",
            "Iteration: 44/537, Loss: 0.0002184683980885893\n",
            "Iteration: 45/537, Loss: 0.43676820397377014\n",
            "Iteration: 46/537, Loss: 0.007247000001370907\n",
            "Iteration: 47/537, Loss: 0.0031784335151314735\n",
            "Iteration: 48/537, Loss: 0.007851258851587772\n",
            "Iteration: 49/537, Loss: 0.006315228994935751\n",
            "Iteration: 50/537, Loss: 0.0007390179089270532\n",
            "Iteration: 51/537, Loss: 0.00033733242889866233\n",
            "Iteration: 52/537, Loss: 0.0015606045490130782\n",
            "Iteration: 53/537, Loss: 0.0030303222592920065\n",
            "Iteration: 54/537, Loss: 0.018364518880844116\n",
            "Iteration: 55/537, Loss: 0.0009184939553961158\n",
            "Iteration: 56/537, Loss: 0.00975547544658184\n",
            "Iteration: 57/537, Loss: 0.0003823903389275074\n",
            "Iteration: 58/537, Loss: 0.0004668597539421171\n",
            "Iteration: 59/537, Loss: 0.00023113840143196285\n",
            "Iteration: 60/537, Loss: 0.002046463778242469\n",
            "Iteration: 61/537, Loss: 0.0017521012341603637\n",
            "Iteration: 62/537, Loss: 0.007930540479719639\n",
            "Iteration: 63/537, Loss: 0.001934147090651095\n",
            "Iteration: 64/537, Loss: 0.27527838945388794\n",
            "Iteration: 65/537, Loss: 0.0008157329866662621\n",
            "Iteration: 66/537, Loss: 0.00104793650098145\n",
            "Iteration: 67/537, Loss: 0.008075399324297905\n",
            "Iteration: 68/537, Loss: 0.0006108623929321766\n",
            "Iteration: 69/537, Loss: 0.10038665682077408\n",
            "Iteration: 70/537, Loss: 0.0015584431821480393\n",
            "Iteration: 71/537, Loss: 0.0012079613516107202\n",
            "Iteration: 72/537, Loss: 0.0023014997132122517\n",
            "Iteration: 73/537, Loss: 0.008157514967024326\n",
            "Iteration: 74/537, Loss: 0.0046914853155612946\n",
            "Iteration: 75/537, Loss: 0.0012410263298079371\n",
            "Iteration: 76/537, Loss: 0.019856151193380356\n",
            "Iteration: 77/537, Loss: 0.004102263133972883\n",
            "Iteration: 78/537, Loss: 0.003438470186665654\n",
            "Iteration: 79/537, Loss: 0.001264586579054594\n",
            "Iteration: 80/537, Loss: 0.003674017731100321\n",
            "Iteration: 81/537, Loss: 0.002711961744353175\n",
            "Iteration: 82/537, Loss: 0.014956515282392502\n",
            "Iteration: 83/537, Loss: 0.0050257728435099125\n",
            "Iteration: 84/537, Loss: 0.005525144282728434\n",
            "Iteration: 85/537, Loss: 0.0033844257704913616\n",
            "Iteration: 86/537, Loss: 0.02584994025528431\n",
            "Iteration: 87/537, Loss: 0.0011112510692328215\n",
            "Iteration: 88/537, Loss: 0.002250612946227193\n",
            "Iteration: 89/537, Loss: 0.003300778102129698\n",
            "Iteration: 90/537, Loss: 0.006934008095413446\n",
            "Iteration: 91/537, Loss: 0.007151507772505283\n",
            "Iteration: 92/537, Loss: 0.006009707227349281\n",
            "Iteration: 93/537, Loss: 0.0192024577409029\n",
            "Iteration: 94/537, Loss: 0.0019165149424225092\n",
            "Iteration: 95/537, Loss: 0.010841047391295433\n",
            "Iteration: 96/537, Loss: 0.019116051495075226\n",
            "Iteration: 97/537, Loss: 0.003968163393437862\n",
            "Iteration: 98/537, Loss: 0.002105269581079483\n",
            "Iteration: 99/537, Loss: 0.0012400252744555473\n",
            "Iteration: 100/537, Loss: 0.004362619947642088\n",
            "Iteration: 101/537, Loss: 0.0032195548992604017\n",
            "Iteration: 102/537, Loss: 0.005762893706560135\n",
            "Iteration: 103/537, Loss: 0.0021878534462302923\n",
            "Iteration: 104/537, Loss: 0.038345787674188614\n",
            "Iteration: 105/537, Loss: 0.0011913430644199252\n",
            "Iteration: 106/537, Loss: 0.006657466292381287\n",
            "Iteration: 107/537, Loss: 0.0016354713588953018\n",
            "Iteration: 108/537, Loss: 0.003523876192048192\n",
            "Iteration: 109/537, Loss: 0.00195335759781301\n",
            "Iteration: 110/537, Loss: 0.0034966932144016027\n",
            "Iteration: 111/537, Loss: 0.0004284376627765596\n",
            "Iteration: 112/537, Loss: 0.0005991736543364823\n",
            "Iteration: 113/537, Loss: 0.008848237805068493\n",
            "Iteration: 114/537, Loss: 0.0011615095427259803\n",
            "Iteration: 115/537, Loss: 0.03732583299279213\n",
            "Iteration: 116/537, Loss: 0.005013160407543182\n",
            "Iteration: 117/537, Loss: 0.003446762217208743\n",
            "Iteration: 118/537, Loss: 0.05624973773956299\n",
            "Iteration: 119/537, Loss: 0.002812613034620881\n",
            "Iteration: 120/537, Loss: 0.0008407254936173558\n",
            "Iteration: 121/537, Loss: 0.004858060739934444\n",
            "Iteration: 122/537, Loss: 0.00036565426853485405\n",
            "Iteration: 123/537, Loss: 0.0021761374082416296\n",
            "Iteration: 124/537, Loss: 0.0027023544535040855\n",
            "Iteration: 125/537, Loss: 0.009343822486698627\n",
            "Iteration: 126/537, Loss: 0.0012159408070147038\n",
            "Iteration: 127/537, Loss: 0.0006753932684659958\n",
            "Iteration: 128/537, Loss: 0.0029713378753513098\n",
            "Iteration: 129/537, Loss: 0.0038636967074126005\n",
            "Iteration: 130/537, Loss: 0.007187570445239544\n",
            "Iteration: 131/537, Loss: 0.0012765972642228007\n",
            "Iteration: 132/537, Loss: 0.0004579001688398421\n",
            "Iteration: 133/537, Loss: 0.003843811107799411\n",
            "Iteration: 134/537, Loss: 0.00023957794473972172\n",
            "Iteration: 135/537, Loss: 0.0005366567638702691\n",
            "Iteration: 136/537, Loss: 0.004247828386723995\n",
            "Iteration: 137/537, Loss: 0.0009387573227286339\n",
            "Iteration: 138/537, Loss: 0.01018464844673872\n",
            "Iteration: 139/537, Loss: 0.002405757550150156\n",
            "Iteration: 140/537, Loss: 0.0007062003714963794\n",
            "Iteration: 141/537, Loss: 0.0026599420234560966\n",
            "Iteration: 142/537, Loss: 0.018980829045176506\n",
            "Iteration: 143/537, Loss: 0.003489785362035036\n",
            "Iteration: 144/537, Loss: 0.0004104897379875183\n",
            "Iteration: 145/537, Loss: 0.009441538713872433\n",
            "Iteration: 146/537, Loss: 0.04589433595538139\n",
            "Iteration: 147/537, Loss: 0.0005484531284309924\n",
            "Iteration: 148/537, Loss: 0.0001120745437219739\n",
            "Iteration: 149/537, Loss: 0.00132137315813452\n",
            "Iteration: 150/537, Loss: 0.005224611610174179\n",
            "Iteration: 151/537, Loss: 0.004600942600518465\n",
            "Iteration: 152/537, Loss: 0.26095256209373474\n",
            "Iteration: 153/537, Loss: 0.015833115205168724\n",
            "Iteration: 154/537, Loss: 0.009983561933040619\n",
            "Iteration: 155/537, Loss: 0.0027735712938010693\n",
            "Iteration: 156/537, Loss: 0.09031055867671967\n",
            "Iteration: 157/537, Loss: 0.0017412924207746983\n",
            "Iteration: 158/537, Loss: 0.0010654840152710676\n",
            "Iteration: 159/537, Loss: 0.000808308192063123\n",
            "Iteration: 160/537, Loss: 0.0003421811561565846\n",
            "Iteration: 161/537, Loss: 0.0037310863845050335\n",
            "Iteration: 162/537, Loss: 0.024967266246676445\n",
            "Iteration: 163/537, Loss: 0.005402709357440472\n",
            "Iteration: 164/537, Loss: 0.0007109175203368068\n",
            "Iteration: 165/537, Loss: 0.004301414359360933\n",
            "Iteration: 166/537, Loss: 0.1774136871099472\n",
            "Iteration: 167/537, Loss: 0.02268189564347267\n",
            "Iteration: 168/537, Loss: 0.007502335589379072\n",
            "Iteration: 169/537, Loss: 0.025915030390024185\n",
            "Iteration: 170/537, Loss: 0.021930580958724022\n",
            "Iteration: 171/537, Loss: 0.240387424826622\n",
            "Iteration: 172/537, Loss: 0.011649897322058678\n",
            "Iteration: 173/537, Loss: 0.33044588565826416\n",
            "Iteration: 174/537, Loss: 0.0010558173526078463\n",
            "Iteration: 175/537, Loss: 0.002864215290173888\n",
            "Iteration: 176/537, Loss: 0.009248300455510616\n",
            "Iteration: 177/537, Loss: 0.0004661823622882366\n",
            "Iteration: 178/537, Loss: 0.02340974286198616\n",
            "Iteration: 179/537, Loss: 0.0002644816122483462\n",
            "Iteration: 180/537, Loss: 0.00043323764111846685\n",
            "Iteration: 181/537, Loss: 0.0008594626560807228\n",
            "Iteration: 182/537, Loss: 0.0006009271019138396\n",
            "Iteration: 183/537, Loss: 0.0010318644344806671\n",
            "Iteration: 184/537, Loss: 0.0008341375505551696\n",
            "Iteration: 185/537, Loss: 0.008059028536081314\n",
            "Iteration: 186/537, Loss: 0.000497890985570848\n",
            "Iteration: 187/537, Loss: 0.0011860023951157928\n",
            "Iteration: 188/537, Loss: 0.0020322026684880257\n",
            "Iteration: 189/537, Loss: 0.0016509270062670112\n",
            "Iteration: 190/537, Loss: 0.0008041404653340578\n",
            "Iteration: 191/537, Loss: 0.0004400651669129729\n",
            "Iteration: 192/537, Loss: 0.12953868508338928\n",
            "Iteration: 193/537, Loss: 0.0026133849751204252\n",
            "Iteration: 194/537, Loss: 0.06484611332416534\n",
            "Iteration: 195/537, Loss: 0.0031835860572755337\n",
            "Iteration: 196/537, Loss: 0.0001697004772722721\n",
            "Iteration: 197/537, Loss: 0.0029529607854783535\n",
            "Iteration: 198/537, Loss: 0.0003805953892879188\n",
            "Iteration: 199/537, Loss: 0.001906532677821815\n",
            "Iteration: 200/537, Loss: 0.002058509737253189\n",
            "Iteration: 201/537, Loss: 0.00037130480632185936\n",
            "Iteration: 202/537, Loss: 0.000483447132864967\n",
            "Iteration: 203/537, Loss: 1.016937494277954\n",
            "Iteration: 204/537, Loss: 0.0003047561040148139\n",
            "Iteration: 205/537, Loss: 0.00561924371868372\n",
            "Iteration: 206/537, Loss: 0.004134634975343943\n",
            "Iteration: 207/537, Loss: 0.0007626028964295983\n",
            "Iteration: 208/537, Loss: 0.003020739648491144\n",
            "Iteration: 209/537, Loss: 0.0035397179890424013\n",
            "Iteration: 210/537, Loss: 0.012989918701350689\n",
            "Iteration: 211/537, Loss: 0.0005626690108329058\n",
            "Iteration: 212/537, Loss: 0.003437446430325508\n",
            "Iteration: 213/537, Loss: 0.036958325654268265\n",
            "Iteration: 214/537, Loss: 0.0027358271181583405\n",
            "Iteration: 215/537, Loss: 0.0030146148055791855\n",
            "Iteration: 216/537, Loss: 0.0001463998924009502\n",
            "Iteration: 217/537, Loss: 0.009762611240148544\n",
            "Iteration: 218/537, Loss: 0.07023994624614716\n",
            "Iteration: 219/537, Loss: 0.004224251490086317\n",
            "Iteration: 220/537, Loss: 0.0011788301635533571\n",
            "Iteration: 221/537, Loss: 0.2208241969347\n",
            "Iteration: 222/537, Loss: 0.0006358316168189049\n",
            "Iteration: 223/537, Loss: 0.004745875019580126\n",
            "Iteration: 224/537, Loss: 0.0014376535546034575\n",
            "Iteration: 225/537, Loss: 0.00036204728530719876\n",
            "Iteration: 226/537, Loss: 0.00520635349676013\n",
            "Iteration: 227/537, Loss: 0.0004272151563782245\n",
            "Iteration: 228/537, Loss: 0.04253832623362541\n",
            "Iteration: 229/537, Loss: 0.0018772813491523266\n",
            "Iteration: 230/537, Loss: 0.0021719778887927532\n",
            "Iteration: 231/537, Loss: 0.07574827969074249\n",
            "Iteration: 232/537, Loss: 0.0013935191091150045\n",
            "Iteration: 233/537, Loss: 0.017508573830127716\n",
            "Iteration: 234/537, Loss: 0.008046790026128292\n",
            "Iteration: 235/537, Loss: 0.0011878976365551353\n",
            "Iteration: 236/537, Loss: 0.5253008604049683\n",
            "Iteration: 237/537, Loss: 0.0023583637084811926\n",
            "Iteration: 238/537, Loss: 0.0005526533932425082\n",
            "Iteration: 239/537, Loss: 0.004244388546794653\n",
            "Iteration: 240/537, Loss: 0.002734706038609147\n",
            "Iteration: 241/537, Loss: 0.000558643601834774\n",
            "Iteration: 242/537, Loss: 0.004561219830065966\n",
            "Iteration: 243/537, Loss: 0.0012762609403580427\n",
            "Iteration: 244/537, Loss: 0.0036731832660734653\n",
            "Iteration: 245/537, Loss: 0.0006248945137485862\n",
            "Iteration: 246/537, Loss: 0.0012166707310825586\n",
            "Iteration: 247/537, Loss: 0.0004108640714548528\n",
            "Iteration: 248/537, Loss: 0.0004004104994237423\n",
            "Iteration: 249/537, Loss: 0.1504932940006256\n",
            "Iteration: 250/537, Loss: 0.011483656242489815\n",
            "Iteration: 251/537, Loss: 0.00040443497709929943\n",
            "Iteration: 252/537, Loss: 0.019136233255267143\n",
            "Iteration: 253/537, Loss: 0.0009826882742345333\n",
            "Iteration: 254/537, Loss: 0.0019319584826007485\n",
            "Iteration: 255/537, Loss: 0.00366642652079463\n",
            "Iteration: 256/537, Loss: 0.0003283303522039205\n",
            "Iteration: 257/537, Loss: 0.0009335509384982288\n",
            "Iteration: 258/537, Loss: 0.0016164984554052353\n",
            "Iteration: 259/537, Loss: 0.0017733055865392089\n",
            "Iteration: 260/537, Loss: 0.0004361109167803079\n",
            "Iteration: 261/537, Loss: 0.0013366187922656536\n",
            "Iteration: 262/537, Loss: 0.0003249158035032451\n",
            "Iteration: 263/537, Loss: 0.0023976534139364958\n",
            "Iteration: 264/537, Loss: 0.001301977434195578\n",
            "Iteration: 265/537, Loss: 0.00042635531281121075\n",
            "Iteration: 266/537, Loss: 0.07669299840927124\n",
            "Iteration: 267/537, Loss: 0.00042054452933371067\n",
            "Iteration: 268/537, Loss: 0.0005237918812781572\n",
            "Iteration: 269/537, Loss: 0.006481382995843887\n",
            "Iteration: 270/537, Loss: 0.002677076729014516\n",
            "Iteration: 271/537, Loss: 0.002188483253121376\n",
            "Iteration: 272/537, Loss: 0.0026160674169659615\n",
            "Iteration: 273/537, Loss: 0.003740211483091116\n",
            "Iteration: 274/537, Loss: 0.0005104836891405284\n",
            "Iteration: 275/537, Loss: 0.0017005292465910316\n",
            "Iteration: 276/537, Loss: 0.002839460736140609\n",
            "Iteration: 277/537, Loss: 0.026849757879972458\n",
            "Iteration: 278/537, Loss: 0.0006231358856894076\n",
            "Iteration: 279/537, Loss: 0.0007393737323582172\n",
            "Iteration: 280/537, Loss: 0.00026881194207817316\n",
            "Iteration: 281/537, Loss: 0.0012272364692762494\n",
            "Iteration: 282/537, Loss: 0.007084110286086798\n",
            "Iteration: 283/537, Loss: 0.006486488040536642\n",
            "Iteration: 284/537, Loss: 0.0010890847770497203\n",
            "Iteration: 285/537, Loss: 0.000556449347641319\n",
            "Iteration: 286/537, Loss: 0.004476483445614576\n",
            "Iteration: 287/537, Loss: 0.0020866424310952425\n",
            "Iteration: 288/537, Loss: 0.002746023703366518\n",
            "Iteration: 289/537, Loss: 0.0004972894676029682\n",
            "Iteration: 290/537, Loss: 0.0017557610990479589\n",
            "Iteration: 291/537, Loss: 0.002054939977824688\n",
            "Iteration: 292/537, Loss: 0.0032258727587759495\n",
            "Iteration: 293/537, Loss: 0.00034695424255914986\n",
            "Iteration: 294/537, Loss: 0.0034534328151494265\n",
            "Iteration: 295/537, Loss: 0.01650659739971161\n",
            "Iteration: 296/537, Loss: 0.001042639371007681\n",
            "Iteration: 297/537, Loss: 0.004186486825346947\n",
            "Iteration: 298/537, Loss: 0.002069174312055111\n",
            "Iteration: 299/537, Loss: 0.03841528668999672\n",
            "Iteration: 300/537, Loss: 0.0012795224320143461\n",
            "Iteration: 301/537, Loss: 0.03395462781190872\n",
            "Iteration: 302/537, Loss: 0.04007476568222046\n",
            "Iteration: 303/537, Loss: 0.0016279390547424555\n",
            "Iteration: 304/537, Loss: 0.000482510105939582\n",
            "Iteration: 305/537, Loss: 0.016709722578525543\n",
            "Iteration: 306/537, Loss: 0.005086157470941544\n",
            "Iteration: 307/537, Loss: 0.00037472773692570627\n",
            "Iteration: 308/537, Loss: 0.01083114929497242\n",
            "Iteration: 309/537, Loss: 0.0005299479234963655\n",
            "Iteration: 310/537, Loss: 0.08383682370185852\n",
            "Iteration: 311/537, Loss: 0.0002574136306066066\n",
            "Iteration: 312/537, Loss: 0.0020725259091705084\n",
            "Iteration: 313/537, Loss: 0.06506785750389099\n",
            "Iteration: 314/537, Loss: 0.00242931442335248\n",
            "Iteration: 315/537, Loss: 0.0007066222024150193\n",
            "Iteration: 316/537, Loss: 0.02403399720788002\n",
            "Iteration: 317/537, Loss: 0.028640253469347954\n",
            "Iteration: 318/537, Loss: 0.0004721896257251501\n",
            "Iteration: 319/537, Loss: 0.0007536823977716267\n",
            "Iteration: 320/537, Loss: 0.00023647343914490193\n",
            "Iteration: 321/537, Loss: 0.05291341245174408\n",
            "Iteration: 322/537, Loss: 0.00029451039154082537\n",
            "Iteration: 323/537, Loss: 0.00038893448072485626\n",
            "Iteration: 324/537, Loss: 0.002718717325478792\n",
            "Iteration: 325/537, Loss: 0.0023107491433620453\n",
            "Iteration: 326/537, Loss: 0.0010662737768143415\n",
            "Iteration: 327/537, Loss: 0.009966246783733368\n",
            "Iteration: 328/537, Loss: 0.000620081031229347\n",
            "Iteration: 329/537, Loss: 0.0008626357303000987\n",
            "Iteration: 330/537, Loss: 0.00018177115998696536\n",
            "Iteration: 331/537, Loss: 0.00032327297958545387\n",
            "Iteration: 332/537, Loss: 0.445136696100235\n",
            "Iteration: 333/537, Loss: 0.012251464650034904\n",
            "Iteration: 334/537, Loss: 0.0012511486420407891\n",
            "Iteration: 335/537, Loss: 0.0011804839596152306\n",
            "Iteration: 336/537, Loss: 0.0009240671060979366\n",
            "Iteration: 337/537, Loss: 0.0006524843629449606\n",
            "Iteration: 338/537, Loss: 0.0006654874887317419\n",
            "Iteration: 339/537, Loss: 0.0006409484194591641\n",
            "Iteration: 340/537, Loss: 0.0021064882166683674\n",
            "Iteration: 341/537, Loss: 0.001224560895934701\n",
            "Iteration: 342/537, Loss: 0.0038362345658242702\n",
            "Iteration: 343/537, Loss: 0.002992702182382345\n",
            "Iteration: 344/537, Loss: 0.0005575153045356274\n",
            "Iteration: 345/537, Loss: 0.42730802297592163\n",
            "Iteration: 346/537, Loss: 0.0015242906520143151\n",
            "Iteration: 347/537, Loss: 0.010119832120835781\n",
            "Iteration: 348/537, Loss: 0.0030936426483094692\n",
            "Iteration: 349/537, Loss: 0.0035420567728579044\n",
            "Iteration: 350/537, Loss: 0.0012365896254777908\n",
            "Iteration: 351/537, Loss: 0.001741450047120452\n",
            "Iteration: 352/537, Loss: 0.0006546232616528869\n",
            "Iteration: 353/537, Loss: 0.002680581295862794\n",
            "Iteration: 354/537, Loss: 0.005182524211704731\n",
            "Iteration: 355/537, Loss: 0.002981743076816201\n",
            "Iteration: 356/537, Loss: 0.006158571224659681\n",
            "Iteration: 357/537, Loss: 0.0008277525193989277\n",
            "Iteration: 358/537, Loss: 0.0024890496861189604\n",
            "Iteration: 359/537, Loss: 0.0022372547537088394\n",
            "Iteration: 360/537, Loss: 0.016361303627490997\n",
            "Iteration: 361/537, Loss: 0.0013636962976306677\n",
            "Iteration: 362/537, Loss: 0.010675547644495964\n",
            "Iteration: 363/537, Loss: 0.0013331985101103783\n",
            "Iteration: 364/537, Loss: 0.0005477187805809081\n",
            "Iteration: 365/537, Loss: 0.0006288308068178594\n",
            "Iteration: 366/537, Loss: 0.012402776628732681\n",
            "Iteration: 367/537, Loss: 0.0007213216740638018\n",
            "Iteration: 368/537, Loss: 0.010481525212526321\n",
            "Iteration: 369/537, Loss: 0.0006555747240781784\n",
            "Iteration: 370/537, Loss: 0.004912403412163258\n",
            "Iteration: 371/537, Loss: 0.0032088288571685553\n",
            "Iteration: 372/537, Loss: 0.001958877081051469\n",
            "Iteration: 373/537, Loss: 0.0023964534047991037\n",
            "Iteration: 374/537, Loss: 0.0017183631425723433\n",
            "Iteration: 375/537, Loss: 0.040363602340221405\n",
            "Iteration: 376/537, Loss: 0.002871481003239751\n",
            "Iteration: 377/537, Loss: 0.0005776898469775915\n",
            "Iteration: 378/537, Loss: 0.0013180153910070658\n",
            "Iteration: 379/537, Loss: 0.0018571808468550444\n",
            "Iteration: 380/537, Loss: 0.0013979823561385274\n",
            "Iteration: 381/537, Loss: 0.0031740888953208923\n",
            "Iteration: 382/537, Loss: 0.0037215598858892918\n",
            "Iteration: 383/537, Loss: 0.0005830024019815028\n",
            "Iteration: 384/537, Loss: 0.0048894924111664295\n",
            "Iteration: 385/537, Loss: 0.0016916641034185886\n",
            "Iteration: 386/537, Loss: 0.0002657609584275633\n",
            "Iteration: 387/537, Loss: 0.004330961965024471\n",
            "Iteration: 388/537, Loss: 0.0017587164184078574\n",
            "Iteration: 389/537, Loss: 0.001275394344702363\n",
            "Iteration: 390/537, Loss: 0.001018741400912404\n",
            "Iteration: 391/537, Loss: 0.0007309273350983858\n",
            "Iteration: 392/537, Loss: 0.0037101092748343945\n",
            "Iteration: 393/537, Loss: 0.012922165915369987\n",
            "Iteration: 394/537, Loss: 0.0009375609224662185\n",
            "Iteration: 395/537, Loss: 0.0005982780130580068\n",
            "Iteration: 396/537, Loss: 0.0065607307478785515\n",
            "Iteration: 397/537, Loss: 0.011595171876251698\n",
            "Iteration: 398/537, Loss: 0.0006026178016327322\n",
            "Iteration: 399/537, Loss: 0.00018606209778226912\n",
            "Iteration: 400/537, Loss: 0.0006135082803666592\n",
            "Iteration: 401/537, Loss: 0.000510127458255738\n",
            "Iteration: 402/537, Loss: 0.0005718756001442671\n",
            "Iteration: 403/537, Loss: 0.0005552866496145725\n",
            "Iteration: 404/537, Loss: 0.0036865444853901863\n",
            "Iteration: 405/537, Loss: 0.00040667090797796845\n",
            "Iteration: 406/537, Loss: 0.0006261308444663882\n",
            "Iteration: 407/537, Loss: 0.0027175741270184517\n",
            "Iteration: 408/537, Loss: 0.010321317240595818\n",
            "Iteration: 409/537, Loss: 0.012877246364951134\n",
            "Iteration: 410/537, Loss: 0.010711325332522392\n",
            "Iteration: 411/537, Loss: 0.0021434519439935684\n",
            "Iteration: 412/537, Loss: 0.0015017727855592966\n",
            "Iteration: 413/537, Loss: 0.0008917521918192506\n",
            "Iteration: 414/537, Loss: 0.0016518938355147839\n",
            "Iteration: 415/537, Loss: 0.01906011812388897\n",
            "Iteration: 416/537, Loss: 0.0018601950723677874\n",
            "Iteration: 417/537, Loss: 0.008841919712722301\n",
            "Iteration: 418/537, Loss: 0.0033794865012168884\n",
            "Iteration: 419/537, Loss: 0.013205062597990036\n",
            "Iteration: 420/537, Loss: 0.2249612808227539\n",
            "Iteration: 421/537, Loss: 0.0020621982403099537\n",
            "Iteration: 422/537, Loss: 0.0011230258969590068\n",
            "Iteration: 423/537, Loss: 0.0010943334782496095\n",
            "Iteration: 424/537, Loss: 0.0017098680837079883\n",
            "Iteration: 425/537, Loss: 0.0006077004363760352\n",
            "Iteration: 426/537, Loss: 0.0004439688054844737\n",
            "Iteration: 427/537, Loss: 0.10257858037948608\n",
            "Iteration: 428/537, Loss: 0.0024279796052724123\n",
            "Iteration: 429/537, Loss: 0.00031083551584742963\n",
            "Iteration: 430/537, Loss: 0.0015041115693747997\n",
            "Iteration: 431/537, Loss: 0.000816884683445096\n",
            "Iteration: 432/537, Loss: 0.010239729657769203\n",
            "Iteration: 433/537, Loss: 0.00033251126296818256\n",
            "Iteration: 434/537, Loss: 0.0007864031940698624\n",
            "Iteration: 435/537, Loss: 0.0005130480276420712\n",
            "Iteration: 436/537, Loss: 0.005953945219516754\n",
            "Iteration: 437/537, Loss: 0.000876345788128674\n",
            "Iteration: 438/537, Loss: 0.005939073860645294\n",
            "Iteration: 439/537, Loss: 0.0057092150673270226\n",
            "Iteration: 440/537, Loss: 0.00021483715681824833\n",
            "Iteration: 441/537, Loss: 0.001059223897755146\n",
            "Iteration: 442/537, Loss: 0.0007548584835603833\n",
            "Iteration: 443/537, Loss: 0.03553812578320503\n",
            "Iteration: 444/537, Loss: 0.0003042629105038941\n",
            "Iteration: 445/537, Loss: 0.0005638442235067487\n",
            "Iteration: 446/537, Loss: 0.005728746764361858\n",
            "Iteration: 447/537, Loss: 0.003913136199116707\n",
            "Iteration: 448/537, Loss: 0.009611603803932667\n",
            "Iteration: 449/537, Loss: 0.01829625479876995\n",
            "Iteration: 450/537, Loss: 0.00019900168990716338\n",
            "Iteration: 451/537, Loss: 0.003269956912845373\n",
            "Iteration: 452/537, Loss: 0.0003813028451986611\n",
            "Iteration: 453/537, Loss: 0.0037287715822458267\n",
            "Iteration: 454/537, Loss: 0.002581082284450531\n",
            "Iteration: 455/537, Loss: 0.002431489061564207\n",
            "Iteration: 456/537, Loss: 0.0019298200495541096\n",
            "Iteration: 457/537, Loss: 0.021854281425476074\n",
            "Iteration: 458/537, Loss: 0.002454204950481653\n",
            "Iteration: 459/537, Loss: 0.002198284026235342\n",
            "Iteration: 460/537, Loss: 0.005026520695537329\n",
            "Iteration: 461/537, Loss: 0.0009000976569950581\n",
            "Iteration: 462/537, Loss: 0.00023930476163513958\n",
            "Iteration: 463/537, Loss: 0.0005142337176948786\n",
            "Iteration: 464/537, Loss: 0.000777861219830811\n",
            "Iteration: 465/537, Loss: 0.00043829603237099946\n",
            "Iteration: 466/537, Loss: 0.0021449970081448555\n",
            "Iteration: 467/537, Loss: 0.0007389192469418049\n",
            "Iteration: 468/537, Loss: 0.02218068763613701\n",
            "Iteration: 469/537, Loss: 0.016915448009967804\n",
            "Iteration: 470/537, Loss: 0.000918362638913095\n",
            "Iteration: 471/537, Loss: 0.0006324400892481208\n",
            "Iteration: 472/537, Loss: 0.022647064179182053\n",
            "Iteration: 473/537, Loss: 0.07336719334125519\n",
            "Iteration: 474/537, Loss: 0.00011139182606711984\n",
            "Iteration: 475/537, Loss: 0.0015349937602877617\n",
            "Iteration: 476/537, Loss: 0.0005891439504921436\n",
            "Iteration: 477/537, Loss: 0.00742940790951252\n",
            "Iteration: 478/537, Loss: 0.0014500501565635204\n",
            "Iteration: 479/537, Loss: 0.00030457606771960855\n",
            "Iteration: 480/537, Loss: 0.0002117701224051416\n",
            "Iteration: 481/537, Loss: 0.0016367727657780051\n",
            "Iteration: 482/537, Loss: 0.0008588616037741303\n",
            "Iteration: 483/537, Loss: 0.001889401930384338\n",
            "Iteration: 484/537, Loss: 0.0006135327857919037\n",
            "Iteration: 485/537, Loss: 0.00027160748140886426\n",
            "Iteration: 486/537, Loss: 0.0010450759436935186\n",
            "Iteration: 487/537, Loss: 0.00015662205987609923\n",
            "Iteration: 488/537, Loss: 0.000165233708685264\n",
            "Iteration: 489/537, Loss: 0.0007509866263717413\n",
            "Iteration: 490/537, Loss: 0.000977354939095676\n",
            "Iteration: 491/537, Loss: 0.0007065343670547009\n",
            "Iteration: 492/537, Loss: 0.01324793417006731\n",
            "Iteration: 493/537, Loss: 0.06108882278203964\n",
            "Iteration: 494/537, Loss: 0.0007960643852129579\n",
            "Iteration: 495/537, Loss: 0.0005914784269407392\n",
            "Iteration: 496/537, Loss: 0.0003932699910365045\n",
            "Iteration: 497/537, Loss: 0.0005873299669474363\n",
            "Iteration: 498/537, Loss: 0.002668128814548254\n",
            "Iteration: 499/537, Loss: 0.0004615577054210007\n",
            "Iteration: 500/537, Loss: 0.0029847014229744673\n",
            "Iteration: 501/537, Loss: 0.0005127352196723223\n",
            "Iteration: 502/537, Loss: 0.000567166309338063\n",
            "Iteration: 503/537, Loss: 0.000349161826306954\n",
            "Iteration: 504/537, Loss: 0.017440374940633774\n",
            "Iteration: 505/537, Loss: 0.0003680902882479131\n",
            "Iteration: 506/537, Loss: 0.0004915199242532253\n",
            "Iteration: 507/537, Loss: 0.00025226178695447743\n",
            "Iteration: 508/537, Loss: 0.0019427824299782515\n",
            "Iteration: 509/537, Loss: 0.00038145779399201274\n",
            "Iteration: 510/537, Loss: 0.0019281120039522648\n",
            "Iteration: 511/537, Loss: 0.005422633141279221\n",
            "Iteration: 512/537, Loss: 0.007423516362905502\n",
            "Iteration: 513/537, Loss: 0.0003366494202055037\n",
            "Iteration: 514/537, Loss: 0.0004784769844263792\n",
            "Iteration: 515/537, Loss: 0.0005642662872560322\n",
            "Iteration: 516/537, Loss: 0.0001775760465534404\n",
            "Iteration: 517/537, Loss: 0.0013424308272078633\n",
            "Iteration: 518/537, Loss: 0.0003377360408194363\n",
            "Iteration: 519/537, Loss: 0.00017004141409415752\n",
            "Iteration: 520/537, Loss: 0.0010325186885893345\n",
            "Iteration: 521/537, Loss: 0.0003949414531234652\n",
            "Iteration: 522/537, Loss: 0.07369759678840637\n",
            "Iteration: 523/537, Loss: 0.001469533541239798\n",
            "Iteration: 524/537, Loss: 0.10138770937919617\n",
            "Iteration: 525/537, Loss: 0.00020011747255921364\n",
            "Iteration: 526/537, Loss: 0.0001704421010799706\n",
            "Iteration: 527/537, Loss: 0.1800365447998047\n",
            "Iteration: 528/537, Loss: 0.0009820277336984873\n",
            "Iteration: 529/537, Loss: 0.010248650796711445\n",
            "Iteration: 530/537, Loss: 0.0006286670104600489\n",
            "Iteration: 531/537, Loss: 0.0037947488017380238\n",
            "Iteration: 532/537, Loss: 0.00025662744883447886\n",
            "Iteration: 533/537, Loss: 0.00043167563853785396\n",
            "Iteration: 534/537, Loss: 0.00039719988126307726\n",
            "Iteration: 535/537, Loss: 0.0007122708484530449\n",
            "Iteration: 536/537, Loss: 0.0016835550777614117\n",
            "Iteration: 537/537, Loss: 0.8272678852081299\n",
            "Epoch 46 train loss: 0.019602919221728663\n",
            "Epoch 46 test loss: 0.08100563144506412\n",
            "-------------------------------------\n",
            "Iteration: 1/537, Loss: 0.0016877749003469944\n",
            "Iteration: 2/537, Loss: 0.0015454316744580865\n",
            "Iteration: 3/537, Loss: 0.0005463279085233808\n",
            "Iteration: 4/537, Loss: 0.0031176230404525995\n",
            "Iteration: 5/537, Loss: 0.00040232203900814056\n",
            "Iteration: 6/537, Loss: 0.0015200590714812279\n",
            "Iteration: 7/537, Loss: 0.0003298471274320036\n",
            "Iteration: 8/537, Loss: 0.00018742641259450465\n",
            "Iteration: 9/537, Loss: 0.0009305902640335262\n",
            "Iteration: 10/537, Loss: 0.0002634986594785005\n",
            "Iteration: 11/537, Loss: 0.002059321152046323\n",
            "Iteration: 12/537, Loss: 0.0005155117250978947\n",
            "Iteration: 13/537, Loss: 0.006276234518736601\n",
            "Iteration: 14/537, Loss: 0.003995669074356556\n",
            "Iteration: 15/537, Loss: 0.0012784457067027688\n",
            "Iteration: 16/537, Loss: 0.0007995358901098371\n",
            "Iteration: 17/537, Loss: 0.0004950090078637004\n",
            "Iteration: 18/537, Loss: 0.000760856200940907\n",
            "Iteration: 19/537, Loss: 0.009529635310173035\n",
            "Iteration: 20/537, Loss: 0.0018881686264649034\n",
            "Iteration: 21/537, Loss: 0.0034139645285904408\n",
            "Iteration: 22/537, Loss: 0.0008875514031387866\n",
            "Iteration: 23/537, Loss: 0.003644558833912015\n",
            "Iteration: 24/537, Loss: 0.000309159979224205\n",
            "Iteration: 25/537, Loss: 0.00023675996635574847\n",
            "Iteration: 26/537, Loss: 0.00042108926572836936\n",
            "Iteration: 27/537, Loss: 0.00010325367475161329\n",
            "Iteration: 28/537, Loss: 0.0043424866162240505\n",
            "Iteration: 29/537, Loss: 0.0010597608052194118\n",
            "Iteration: 30/537, Loss: 0.0022236350923776627\n",
            "Iteration: 31/537, Loss: 0.0002855024649761617\n",
            "Iteration: 32/537, Loss: 0.0013008267851546407\n",
            "Iteration: 33/537, Loss: 0.00994922686368227\n",
            "Iteration: 34/537, Loss: 0.00015878889826126397\n",
            "Iteration: 35/537, Loss: 0.0004807944060303271\n",
            "Iteration: 36/537, Loss: 0.0004416580777615309\n",
            "Iteration: 37/537, Loss: 0.00039117870619520545\n",
            "Iteration: 38/537, Loss: 0.0001876235328381881\n",
            "Iteration: 39/537, Loss: 0.00027632308774627745\n",
            "Iteration: 40/537, Loss: 0.007598558906465769\n",
            "Iteration: 41/537, Loss: 0.0005330982967279851\n",
            "Iteration: 42/537, Loss: 0.00143843749538064\n",
            "Iteration: 43/537, Loss: 0.0004293430829420686\n",
            "Iteration: 44/537, Loss: 0.006327854935079813\n",
            "Iteration: 45/537, Loss: 0.00041748344665393233\n",
            "Iteration: 46/537, Loss: 0.003626757999882102\n",
            "Iteration: 47/537, Loss: 0.0007280481513589621\n",
            "Iteration: 48/537, Loss: 0.00020206303452141583\n",
            "Iteration: 49/537, Loss: 0.003648055251687765\n",
            "Iteration: 50/537, Loss: 0.002063992666080594\n",
            "Iteration: 51/537, Loss: 0.00018411265045870095\n",
            "Iteration: 52/537, Loss: 0.001121940673328936\n",
            "Iteration: 53/537, Loss: 0.0015424463199451566\n",
            "Iteration: 54/537, Loss: 0.0003635512257460505\n",
            "Iteration: 55/537, Loss: 0.0002885909634642303\n",
            "Iteration: 56/537, Loss: 0.00042746798135340214\n",
            "Iteration: 57/537, Loss: 0.00030702707590535283\n",
            "Iteration: 58/537, Loss: 0.001142184715718031\n",
            "Iteration: 59/537, Loss: 0.0015568756498396397\n",
            "Iteration: 60/537, Loss: 0.0005774945602752268\n",
            "Iteration: 61/537, Loss: 0.0007857168675400317\n",
            "Iteration: 62/537, Loss: 0.001064297161065042\n",
            "Iteration: 63/537, Loss: 0.0008330052951350808\n",
            "Iteration: 64/537, Loss: 0.0014663218753412366\n",
            "Iteration: 65/537, Loss: 0.0016906248638406396\n",
            "Iteration: 66/537, Loss: 0.0075443643145263195\n",
            "Iteration: 67/537, Loss: 0.0002147719351341948\n",
            "Iteration: 68/537, Loss: 0.0003100563189946115\n",
            "Iteration: 69/537, Loss: 0.06070207431912422\n",
            "Iteration: 70/537, Loss: 0.001601262018084526\n",
            "Iteration: 71/537, Loss: 0.0017143047880381346\n",
            "Iteration: 72/537, Loss: 0.00024004017177503556\n",
            "Iteration: 73/537, Loss: 0.0011519311228767037\n",
            "Iteration: 74/537, Loss: 0.002183755161240697\n",
            "Iteration: 75/537, Loss: 0.00023593239893671125\n",
            "Iteration: 76/537, Loss: 0.007436005398631096\n",
            "Iteration: 77/537, Loss: 0.006014486309140921\n",
            "Iteration: 78/537, Loss: 0.00013624393614009023\n",
            "Iteration: 79/537, Loss: 0.0004917805199511349\n",
            "Iteration: 80/537, Loss: 0.00024381976982112974\n",
            "Iteration: 81/537, Loss: 0.00048594531835988164\n",
            "Iteration: 82/537, Loss: 0.0009246128611266613\n",
            "Iteration: 83/537, Loss: 0.00018427500617690384\n",
            "Iteration: 84/537, Loss: 0.0004098046338185668\n",
            "Iteration: 85/537, Loss: 0.004347700159996748\n",
            "Iteration: 86/537, Loss: 0.000157391739776358\n",
            "Iteration: 87/537, Loss: 0.006665036082267761\n",
            "Iteration: 88/537, Loss: 0.05744863674044609\n",
            "Iteration: 89/537, Loss: 0.00042059950646944344\n",
            "Iteration: 90/537, Loss: 0.0003023167373612523\n",
            "Iteration: 91/537, Loss: 0.0005083668511360884\n",
            "Iteration: 92/537, Loss: 0.0037770725321024656\n",
            "Iteration: 93/537, Loss: 0.000979319796897471\n",
            "Iteration: 94/537, Loss: 0.0073771183378994465\n",
            "Iteration: 95/537, Loss: 0.0024043910671025515\n",
            "Iteration: 96/537, Loss: 0.00024342445249203593\n",
            "Iteration: 97/537, Loss: 0.003263121237978339\n",
            "Iteration: 98/537, Loss: 0.001371223945170641\n",
            "Iteration: 99/537, Loss: 0.00027028610929846764\n",
            "Iteration: 100/537, Loss: 0.003691037418320775\n",
            "Iteration: 101/537, Loss: 0.0019718753173947334\n",
            "Iteration: 102/537, Loss: 0.000993270892649889\n",
            "Iteration: 103/537, Loss: 0.0015601698542013764\n",
            "Iteration: 104/537, Loss: 0.0009039912256412208\n",
            "Iteration: 105/537, Loss: 0.00046182970982044935\n",
            "Iteration: 106/537, Loss: 0.0002459996030665934\n",
            "Iteration: 107/537, Loss: 0.00040462223114445806\n",
            "Iteration: 108/537, Loss: 0.003314166096970439\n",
            "Iteration: 109/537, Loss: 0.23267762362957\n",
            "Iteration: 110/537, Loss: 0.0006609827978536487\n",
            "Iteration: 111/537, Loss: 0.005292054265737534\n",
            "Iteration: 112/537, Loss: 0.000778631423600018\n",
            "Iteration: 113/537, Loss: 0.0003159629995934665\n",
            "Iteration: 114/537, Loss: 0.0025997476186603308\n",
            "Iteration: 115/537, Loss: 0.057639025151729584\n",
            "Iteration: 116/537, Loss: 0.0035649400670081377\n",
            "Iteration: 117/537, Loss: 0.0003391205973457545\n",
            "Iteration: 118/537, Loss: 0.00017815333558246493\n",
            "Iteration: 119/537, Loss: 0.001181089086458087\n",
            "Iteration: 120/537, Loss: 0.0015218229964375496\n",
            "Iteration: 121/537, Loss: 0.00045257684541866183\n",
            "Iteration: 122/537, Loss: 0.000743403856176883\n",
            "Iteration: 123/537, Loss: 0.0003469883813522756\n",
            "Iteration: 124/537, Loss: 0.0008824387914501131\n",
            "Iteration: 125/537, Loss: 0.0002569590578787029\n",
            "Iteration: 126/537, Loss: 0.0001794675481505692\n",
            "Iteration: 127/537, Loss: 0.04597761109471321\n",
            "Iteration: 128/537, Loss: 0.044337544590234756\n",
            "Iteration: 129/537, Loss: 0.00017753378779161721\n",
            "Iteration: 130/537, Loss: 0.002051752060651779\n",
            "Iteration: 131/537, Loss: 0.0006955962744541466\n",
            "Iteration: 132/537, Loss: 0.006437725853174925\n",
            "Iteration: 133/537, Loss: 0.0009809958282858133\n",
            "Iteration: 134/537, Loss: 0.0020760835614055395\n",
            "Iteration: 135/537, Loss: 0.0008414795156568289\n",
            "Iteration: 136/537, Loss: 0.0005537187098525465\n",
            "Iteration: 137/537, Loss: 0.00044535892084240913\n",
            "Iteration: 138/537, Loss: 0.0008313264115713537\n",
            "Iteration: 139/537, Loss: 0.0001798936864361167\n",
            "Iteration: 140/537, Loss: 0.00017726542137097567\n",
            "Iteration: 141/537, Loss: 0.00011380520300008357\n",
            "Iteration: 142/537, Loss: 0.0016882147174328566\n",
            "Iteration: 143/537, Loss: 0.00023717540898360312\n",
            "Iteration: 144/537, Loss: 0.030863290652632713\n",
            "Iteration: 145/537, Loss: 0.0002597908896859735\n",
            "Iteration: 146/537, Loss: 0.0005306001985445619\n",
            "Iteration: 147/537, Loss: 0.1528800129890442\n",
            "Iteration: 148/537, Loss: 0.14426438510417938\n",
            "Iteration: 149/537, Loss: 0.00011901484685949981\n",
            "Iteration: 150/537, Loss: 0.001909080077894032\n",
            "Iteration: 151/537, Loss: 0.0005799101199954748\n",
            "Iteration: 152/537, Loss: 0.0013844168279320002\n",
            "Iteration: 153/537, Loss: 0.0011280449107289314\n",
            "Iteration: 154/537, Loss: 0.0028374982066452503\n",
            "Iteration: 155/537, Loss: 0.0010489942505955696\n",
            "Iteration: 156/537, Loss: 0.0008702100603841245\n",
            "Iteration: 157/537, Loss: 0.0009002579026855528\n",
            "Iteration: 158/537, Loss: 0.00036363513208925724\n",
            "Iteration: 159/537, Loss: 0.0024475741665810347\n",
            "Iteration: 160/537, Loss: 0.005698210559785366\n",
            "Iteration: 161/537, Loss: 0.0005075391381978989\n",
            "Iteration: 162/537, Loss: 0.0016032501589506865\n",
            "Iteration: 163/537, Loss: 0.0009303869446739554\n",
            "Iteration: 164/537, Loss: 0.0019046971574425697\n",
            "Iteration: 165/537, Loss: 0.0006377249956130981\n",
            "Iteration: 166/537, Loss: 0.0011603499297052622\n",
            "Iteration: 167/537, Loss: 0.0007504434906877577\n",
            "Iteration: 168/537, Loss: 0.0016872669802978635\n",
            "Iteration: 169/537, Loss: 0.0001517996861366555\n",
            "Iteration: 170/537, Loss: 0.0005409930599853396\n",
            "Iteration: 171/537, Loss: 0.0018426957540214062\n",
            "Iteration: 172/537, Loss: 0.0004098645586054772\n",
            "Iteration: 173/537, Loss: 0.002794593805447221\n",
            "Iteration: 174/537, Loss: 0.0005763453664258122\n",
            "Iteration: 175/537, Loss: 0.00032499257940799\n",
            "Iteration: 176/537, Loss: 0.0028350581414997578\n",
            "Iteration: 177/537, Loss: 0.005291754845529795\n",
            "Iteration: 178/537, Loss: 0.0006301288958638906\n",
            "Iteration: 179/537, Loss: 0.0008209942025132477\n",
            "Iteration: 180/537, Loss: 0.0006773497443646193\n",
            "Iteration: 181/537, Loss: 0.0010685451561585069\n",
            "Iteration: 182/537, Loss: 0.0009378996910527349\n",
            "Iteration: 183/537, Loss: 0.002109831664711237\n",
            "Iteration: 184/537, Loss: 0.0006606593960896134\n",
            "Iteration: 185/537, Loss: 0.046410512179136276\n",
            "Iteration: 186/537, Loss: 0.0002238643355667591\n",
            "Iteration: 187/537, Loss: 0.0011017558863386512\n",
            "Iteration: 188/537, Loss: 0.00043145986273884773\n",
            "Iteration: 189/537, Loss: 0.004925181623548269\n",
            "Iteration: 190/537, Loss: 0.000948683824390173\n",
            "Iteration: 191/537, Loss: 0.00782211683690548\n",
            "Iteration: 192/537, Loss: 0.005164365749806166\n",
            "Iteration: 193/537, Loss: 0.0003742043918464333\n",
            "Iteration: 194/537, Loss: 0.0012531750835478306\n",
            "Iteration: 195/537, Loss: 0.004546868149191141\n",
            "Iteration: 196/537, Loss: 0.0008342104265466332\n",
            "Iteration: 197/537, Loss: 0.0015121165197342634\n",
            "Iteration: 198/537, Loss: 0.0004339581064414233\n",
            "Iteration: 199/537, Loss: 0.00139830750413239\n",
            "Iteration: 200/537, Loss: 0.0009798386599868536\n",
            "Iteration: 201/537, Loss: 0.0034333954099565744\n",
            "Iteration: 202/537, Loss: 0.00025364072644151747\n",
            "Iteration: 203/537, Loss: 0.06330998986959457\n",
            "Iteration: 204/537, Loss: 0.002682978753000498\n",
            "Iteration: 205/537, Loss: 0.0007822648622095585\n",
            "Iteration: 206/537, Loss: 0.0004653336654882878\n",
            "Iteration: 207/537, Loss: 0.013391758315265179\n",
            "Iteration: 208/537, Loss: 0.012576593086123466\n",
            "Iteration: 209/537, Loss: 0.0003950917744077742\n",
            "Iteration: 210/537, Loss: 0.0005621100426651537\n",
            "Iteration: 211/537, Loss: 0.008907247334718704\n",
            "Iteration: 212/537, Loss: 0.011401299387216568\n",
            "Iteration: 213/537, Loss: 0.0021759073715656996\n",
            "Iteration: 214/537, Loss: 0.0008050110773183405\n",
            "Iteration: 215/537, Loss: 0.11319025605916977\n",
            "Iteration: 216/537, Loss: 0.1662595272064209\n",
            "Iteration: 217/537, Loss: 0.0003813152143266052\n",
            "Iteration: 218/537, Loss: 0.0007910963613539934\n",
            "Iteration: 219/537, Loss: 0.0003613170702010393\n",
            "Iteration: 220/537, Loss: 0.012977367267012596\n",
            "Iteration: 221/537, Loss: 0.00022081515635363758\n",
            "Iteration: 222/537, Loss: 0.0007728856871835887\n",
            "Iteration: 223/537, Loss: 0.0035682660527527332\n",
            "Iteration: 224/537, Loss: 0.0003349699545651674\n",
            "Iteration: 225/537, Loss: 0.018078498542308807\n",
            "Iteration: 226/537, Loss: 0.00037200615042820573\n",
            "Iteration: 227/537, Loss: 0.003479650942608714\n",
            "Iteration: 228/537, Loss: 0.0014851901214569807\n",
            "Iteration: 229/537, Loss: 0.0008755080052651465\n",
            "Iteration: 230/537, Loss: 0.0009703137911856174\n",
            "Iteration: 231/537, Loss: 0.004639489110559225\n",
            "Iteration: 232/537, Loss: 0.04172151908278465\n",
            "Iteration: 233/537, Loss: 0.0006549570243805647\n",
            "Iteration: 234/537, Loss: 0.00076559919398278\n",
            "Iteration: 235/537, Loss: 0.0024007298052310944\n",
            "Iteration: 236/537, Loss: 0.0013474118895828724\n",
            "Iteration: 237/537, Loss: 0.0006388654001057148\n",
            "Iteration: 238/537, Loss: 0.000401672616135329\n",
            "Iteration: 239/537, Loss: 0.00015825466834940016\n",
            "Iteration: 240/537, Loss: 0.0015197026077657938\n",
            "Iteration: 241/537, Loss: 0.0007365815108641982\n",
            "Iteration: 242/537, Loss: 0.0011471963953226805\n",
            "Iteration: 243/537, Loss: 0.0018542602192610502\n",
            "Iteration: 244/537, Loss: 0.0013028536923229694\n",
            "Iteration: 245/537, Loss: 0.0008090208866633475\n",
            "Iteration: 246/537, Loss: 0.0030532698146998882\n",
            "Iteration: 247/537, Loss: 0.0007163268746808171\n",
            "Iteration: 248/537, Loss: 0.0007050992571748793\n",
            "Iteration: 249/537, Loss: 0.0004293454985599965\n",
            "Iteration: 250/537, Loss: 0.000545667193364352\n",
            "Iteration: 251/537, Loss: 0.00039553249371238053\n",
            "Iteration: 252/537, Loss: 0.0011083076242357492\n",
            "Iteration: 253/537, Loss: 0.0007977536879479885\n",
            "Iteration: 254/537, Loss: 0.0008709392859600484\n",
            "Iteration: 255/537, Loss: 0.0014802180230617523\n",
            "Iteration: 256/537, Loss: 0.0012793132336810231\n",
            "Iteration: 257/537, Loss: 0.001665793126448989\n",
            "Iteration: 258/537, Loss: 0.0035937014035880566\n",
            "Iteration: 259/537, Loss: 0.0031746590975672007\n",
            "Iteration: 260/537, Loss: 0.0004592657496687025\n",
            "Iteration: 261/537, Loss: 0.0015116191934794188\n",
            "Iteration: 262/537, Loss: 0.0003418757114559412\n",
            "Iteration: 263/537, Loss: 0.001282643061131239\n",
            "Iteration: 264/537, Loss: 0.0002809697180055082\n",
            "Iteration: 265/537, Loss: 0.004277077037841082\n",
            "Iteration: 266/537, Loss: 0.00024703540839254856\n",
            "Iteration: 267/537, Loss: 0.00013865306391380727\n",
            "Iteration: 268/537, Loss: 0.0004530903825070709\n",
            "Iteration: 269/537, Loss: 0.00010012507846113294\n",
            "Iteration: 270/537, Loss: 0.0001297162816626951\n",
            "Iteration: 271/537, Loss: 0.0009553979616612196\n",
            "Iteration: 272/537, Loss: 0.00266129313968122\n",
            "Iteration: 273/537, Loss: 0.009072271175682545\n",
            "Iteration: 274/537, Loss: 0.00044821889605373144\n",
            "Iteration: 275/537, Loss: 0.000466269557364285\n",
            "Iteration: 276/537, Loss: 0.003327186219394207\n",
            "Iteration: 277/537, Loss: 0.0001382393529638648\n",
            "Iteration: 278/537, Loss: 0.0003922943724319339\n",
            "Iteration: 279/537, Loss: 0.0020684453193098307\n",
            "Iteration: 280/537, Loss: 0.00023799519112799317\n",
            "Iteration: 281/537, Loss: 0.0005691515980288386\n",
            "Iteration: 282/537, Loss: 0.0017212139209732413\n",
            "Iteration: 283/537, Loss: 0.000659100420307368\n",
            "Iteration: 284/537, Loss: 0.00028409709921106696\n",
            "Iteration: 285/537, Loss: 0.006589085329324007\n",
            "Iteration: 286/537, Loss: 0.0023422082886099815\n",
            "Iteration: 287/537, Loss: 0.0012873898958787322\n",
            "Iteration: 288/537, Loss: 0.11333657056093216\n",
            "Iteration: 289/537, Loss: 0.00039050457417033613\n",
            "Iteration: 290/537, Loss: 0.0036057441029697657\n",
            "Iteration: 291/537, Loss: 0.0020869362633675337\n",
            "Iteration: 292/537, Loss: 0.0005442174151539803\n",
            "Iteration: 293/537, Loss: 0.027341045439243317\n",
            "Iteration: 294/537, Loss: 0.00037805025931447744\n",
            "Iteration: 295/537, Loss: 0.0008433585171587765\n",
            "Iteration: 296/537, Loss: 0.00013517157640308142\n",
            "Iteration: 297/537, Loss: 0.0039856284856796265\n",
            "Iteration: 298/537, Loss: 0.01210022158920765\n",
            "Iteration: 299/537, Loss: 0.00039451170596294105\n",
            "Iteration: 300/537, Loss: 0.00016662789857946336\n",
            "Iteration: 301/537, Loss: 0.0009015966206789017\n",
            "Iteration: 302/537, Loss: 0.002256096340715885\n",
            "Iteration: 303/537, Loss: 0.0002808499557431787\n",
            "Iteration: 304/537, Loss: 0.0010425008367747068\n",
            "Iteration: 305/537, Loss: 0.003145385067909956\n",
            "Iteration: 306/537, Loss: 0.003689919365569949\n",
            "Iteration: 307/537, Loss: 0.0011443886905908585\n",
            "Iteration: 308/537, Loss: 0.0032503732945770025\n",
            "Iteration: 309/537, Loss: 0.00011928786261705682\n",
            "Iteration: 310/537, Loss: 0.0512768030166626\n",
            "Iteration: 311/537, Loss: 0.04967542365193367\n",
            "Iteration: 312/537, Loss: 0.0006325758877210319\n",
            "Iteration: 313/537, Loss: 0.005013585556298494\n",
            "Iteration: 314/537, Loss: 0.004334191791713238\n",
            "Iteration: 315/537, Loss: 0.0031300722621381283\n",
            "Iteration: 316/537, Loss: 0.12034256756305695\n",
            "Iteration: 317/537, Loss: 0.00040773817454464734\n",
            "Iteration: 318/537, Loss: 0.00488306162878871\n",
            "Iteration: 319/537, Loss: 0.0003176484606228769\n",
            "Iteration: 320/537, Loss: 0.0002579810097813606\n",
            "Iteration: 321/537, Loss: 0.034083832055330276\n",
            "Iteration: 322/537, Loss: 0.0007956494810059667\n",
            "Iteration: 323/537, Loss: 0.007037132512778044\n",
            "Iteration: 324/537, Loss: 0.0004254775703884661\n",
            "Iteration: 325/537, Loss: 0.004217205569148064\n",
            "Iteration: 326/537, Loss: 0.010488966479897499\n",
            "Iteration: 327/537, Loss: 0.0005755966412834823\n",
            "Iteration: 328/537, Loss: 0.007148692850023508\n",
            "Iteration: 329/537, Loss: 0.00012753132614307106\n",
            "Iteration: 330/537, Loss: 0.0003202125371899456\n",
            "Iteration: 331/537, Loss: 0.0001416905433870852\n",
            "Iteration: 332/537, Loss: 0.005624239332973957\n",
            "Iteration: 333/537, Loss: 0.0008023394038900733\n",
            "Iteration: 334/537, Loss: 0.005235832650214434\n",
            "Iteration: 335/537, Loss: 0.00035573571221902966\n",
            "Iteration: 336/537, Loss: 0.000594831130001694\n",
            "Iteration: 337/537, Loss: 0.0012065772898495197\n",
            "Iteration: 338/537, Loss: 0.00012825729208998382\n",
            "Iteration: 339/537, Loss: 0.0007218577084131539\n",
            "Iteration: 340/537, Loss: 0.002277481835335493\n",
            "Iteration: 341/537, Loss: 0.00100608984939754\n",
            "Iteration: 342/537, Loss: 0.0009311696048825979\n",
            "Iteration: 343/537, Loss: 0.0012842556461691856\n",
            "Iteration: 344/537, Loss: 0.0036411976907402277\n",
            "Iteration: 345/537, Loss: 0.0003621925716288388\n",
            "Iteration: 346/537, Loss: 0.0006616648170165718\n",
            "Iteration: 347/537, Loss: 0.000729586579836905\n",
            "Iteration: 348/537, Loss: 0.0003245436237193644\n",
            "Iteration: 349/537, Loss: 0.004147075116634369\n",
            "Iteration: 350/537, Loss: 0.013010888360440731\n",
            "Iteration: 351/537, Loss: 0.0002217002329416573\n",
            "Iteration: 352/537, Loss: 0.00011744307266781107\n",
            "Iteration: 353/537, Loss: 0.00033502926817163825\n",
            "Iteration: 354/537, Loss: 0.0005834660842083395\n",
            "Iteration: 355/537, Loss: 0.0009805744048207998\n",
            "Iteration: 356/537, Loss: 0.00029180574347265065\n",
            "Iteration: 357/537, Loss: 0.0020676804706454277\n",
            "Iteration: 358/537, Loss: 0.0009244051179848611\n",
            "Iteration: 359/537, Loss: 0.0012106213252991438\n",
            "Iteration: 360/537, Loss: 0.0005501155974343419\n",
            "Iteration: 361/537, Loss: 0.000547385192476213\n",
            "Iteration: 362/537, Loss: 0.040030598640441895\n",
            "Iteration: 363/537, Loss: 0.0015351591864600778\n",
            "Iteration: 364/537, Loss: 0.002961029764264822\n",
            "Iteration: 365/537, Loss: 0.18974563479423523\n",
            "Iteration: 366/537, Loss: 0.00015751195314805955\n",
            "Iteration: 367/537, Loss: 0.0005002832622267306\n",
            "Iteration: 368/537, Loss: 0.001426768722012639\n",
            "Iteration: 369/537, Loss: 0.0010135392658412457\n",
            "Iteration: 370/537, Loss: 0.0005450373282656074\n",
            "Iteration: 371/537, Loss: 0.0007042039651423693\n",
            "Iteration: 372/537, Loss: 0.001637832261621952\n",
            "Iteration: 373/537, Loss: 0.0027870754711329937\n",
            "Iteration: 374/537, Loss: 0.00042774167377501726\n",
            "Iteration: 375/537, Loss: 0.001912785810418427\n",
            "Iteration: 376/537, Loss: 0.06575056910514832\n",
            "Iteration: 377/537, Loss: 0.0006111375987529755\n",
            "Iteration: 378/537, Loss: 0.0012574766296893358\n",
            "Iteration: 379/537, Loss: 0.0003167038084939122\n",
            "Iteration: 380/537, Loss: 0.001712133176624775\n",
            "Iteration: 381/537, Loss: 0.09825696796178818\n",
            "Iteration: 382/537, Loss: 0.0006477911956608295\n",
            "Iteration: 383/537, Loss: 0.0010524741373956203\n",
            "Iteration: 384/537, Loss: 0.0010326140327379107\n",
            "Iteration: 385/537, Loss: 0.04035048186779022\n",
            "Iteration: 386/537, Loss: 0.001097547821700573\n",
            "Iteration: 387/537, Loss: 0.001311996835283935\n",
            "Iteration: 388/537, Loss: 0.0003703886177390814\n",
            "Iteration: 389/537, Loss: 0.000116068942588754\n",
            "Iteration: 390/537, Loss: 0.017219366505742073\n",
            "Iteration: 391/537, Loss: 0.003531461814418435\n",
            "Iteration: 392/537, Loss: 0.00042870163451880217\n",
            "Iteration: 393/537, Loss: 0.0005193192628212273\n",
            "Iteration: 394/537, Loss: 0.0007036712486296892\n",
            "Iteration: 395/537, Loss: 0.0016471551498398185\n",
            "Iteration: 396/537, Loss: 0.0009241495281457901\n",
            "Iteration: 397/537, Loss: 0.00045712655992247164\n",
            "Iteration: 398/537, Loss: 0.0007461982895620167\n",
            "Iteration: 399/537, Loss: 0.021951060742139816\n",
            "Iteration: 400/537, Loss: 0.014828660525381565\n",
            "Iteration: 401/537, Loss: 0.0027616939041763544\n",
            "Iteration: 402/537, Loss: 0.0004976952914148569\n",
            "Iteration: 403/537, Loss: 0.0010025427909567952\n",
            "Iteration: 404/537, Loss: 0.0017725229263305664\n",
            "Iteration: 405/537, Loss: 0.00019168992002960294\n",
            "Iteration: 406/537, Loss: 0.002577373990789056\n",
            "Iteration: 407/537, Loss: 0.002714184345677495\n",
            "Iteration: 408/537, Loss: 0.0010581252863630652\n",
            "Iteration: 409/537, Loss: 0.0005888214218430221\n",
            "Iteration: 410/537, Loss: 0.00045123862219043076\n",
            "Iteration: 411/537, Loss: 0.0013120922958478332\n",
            "Iteration: 412/537, Loss: 0.0016404148191213608\n",
            "Iteration: 413/537, Loss: 0.00030296359909698367\n",
            "Iteration: 414/537, Loss: 0.0004879743210040033\n",
            "Iteration: 415/537, Loss: 0.003304550424218178\n",
            "Iteration: 416/537, Loss: 0.023803086951375008\n",
            "Iteration: 417/537, Loss: 0.000906246539670974\n",
            "Iteration: 418/537, Loss: 0.0010860930196940899\n",
            "Iteration: 419/537, Loss: 0.012520126067101955\n",
            "Iteration: 420/537, Loss: 0.0003316832007840276\n",
            "Iteration: 421/537, Loss: 0.0013653411297127604\n",
            "Iteration: 422/537, Loss: 0.03500731289386749\n",
            "Iteration: 423/537, Loss: 0.0007279831916093826\n",
            "Iteration: 424/537, Loss: 9.22318795346655e-05\n",
            "Iteration: 425/537, Loss: 0.00021415708761196584\n",
            "Iteration: 426/537, Loss: 0.0010078835766762495\n",
            "Iteration: 427/537, Loss: 0.0016763197490945458\n",
            "Iteration: 428/537, Loss: 0.027042342349886894\n",
            "Iteration: 429/537, Loss: 0.0004376500437501818\n",
            "Iteration: 430/537, Loss: 9.959255839930847e-05\n",
            "Iteration: 431/537, Loss: 0.0006383380386978388\n",
            "Iteration: 432/537, Loss: 0.00024317616771440953\n",
            "Iteration: 433/537, Loss: 0.0009591348934918642\n",
            "Iteration: 434/537, Loss: 0.00023740548931527883\n",
            "Iteration: 435/537, Loss: 0.0031131647992879152\n",
            "Iteration: 436/537, Loss: 0.0005977793480269611\n",
            "Iteration: 437/537, Loss: 0.009615461342036724\n",
            "Iteration: 438/537, Loss: 0.0009212786098942161\n",
            "Iteration: 439/537, Loss: 0.032529499381780624\n",
            "Iteration: 440/537, Loss: 0.0001950896839844063\n",
            "Iteration: 441/537, Loss: 0.0020076169166713953\n",
            "Iteration: 442/537, Loss: 9.56542935455218e-05\n",
            "Iteration: 443/537, Loss: 0.0007212729542516172\n",
            "Iteration: 444/537, Loss: 0.0029495819471776485\n",
            "Iteration: 445/537, Loss: 0.0017186516197398305\n",
            "Iteration: 446/537, Loss: 0.0006195995956659317\n",
            "Iteration: 447/537, Loss: 0.0007858689059503376\n",
            "Iteration: 448/537, Loss: 0.0004116014461033046\n",
            "Iteration: 449/537, Loss: 0.0003235838084947318\n",
            "Iteration: 450/537, Loss: 0.09612880647182465\n",
            "Iteration: 451/537, Loss: 0.0004084952815901488\n",
            "Iteration: 452/537, Loss: 0.003647284349426627\n",
            "Iteration: 453/537, Loss: 0.0004188456805422902\n",
            "Iteration: 454/537, Loss: 0.002611824544146657\n",
            "Iteration: 455/537, Loss: 0.002146200742572546\n",
            "Iteration: 456/537, Loss: 0.0013131096493452787\n",
            "Iteration: 457/537, Loss: 0.003921797499060631\n",
            "Iteration: 458/537, Loss: 0.001836538314819336\n",
            "Iteration: 459/537, Loss: 0.03037065453827381\n",
            "Iteration: 460/537, Loss: 0.000331531569827348\n",
            "Iteration: 461/537, Loss: 0.0036043671425431967\n",
            "Iteration: 462/537, Loss: 0.0012312686303630471\n",
            "Iteration: 463/537, Loss: 0.00036116380942985415\n",
            "Iteration: 464/537, Loss: 0.00018769045709632337\n",
            "Iteration: 465/537, Loss: 0.00044261018047109246\n",
            "Iteration: 466/537, Loss: 0.0019249534234404564\n",
            "Iteration: 467/537, Loss: 0.0004079166683368385\n",
            "Iteration: 468/537, Loss: 0.0002625065390020609\n",
            "Iteration: 469/537, Loss: 0.0005052426713518798\n",
            "Iteration: 470/537, Loss: 0.0005665648495778441\n",
            "Iteration: 471/537, Loss: 0.0026323699858039618\n",
            "Iteration: 472/537, Loss: 0.0030885611195117235\n",
            "Iteration: 473/537, Loss: 0.00027866786695085466\n",
            "Iteration: 474/537, Loss: 0.0006415528478100896\n",
            "Iteration: 475/537, Loss: 0.00457321060821414\n",
            "Iteration: 476/537, Loss: 0.005717915017157793\n",
            "Iteration: 477/537, Loss: 0.0012566453078761697\n",
            "Iteration: 478/537, Loss: 0.00015484036703128368\n",
            "Iteration: 479/537, Loss: 0.006311014294624329\n",
            "Iteration: 480/537, Loss: 0.0006593959406018257\n",
            "Iteration: 481/537, Loss: 0.00018043410091195256\n",
            "Iteration: 482/537, Loss: 0.0019354061223566532\n",
            "Iteration: 483/537, Loss: 0.0003775956283789128\n",
            "Iteration: 484/537, Loss: 0.004593447782099247\n",
            "Iteration: 485/537, Loss: 0.015454915352165699\n",
            "Iteration: 486/537, Loss: 0.00034512291313149035\n",
            "Iteration: 487/537, Loss: 0.0009737758082337677\n",
            "Iteration: 488/537, Loss: 0.0019669097382575274\n",
            "Iteration: 489/537, Loss: 0.0014330564299598336\n",
            "Iteration: 490/537, Loss: 0.0003888891951646656\n",
            "Iteration: 491/537, Loss: 0.0009156308369711041\n",
            "Iteration: 492/537, Loss: 0.0017073322087526321\n",
            "Iteration: 493/537, Loss: 0.0005112714134156704\n",
            "Iteration: 494/537, Loss: 0.0012934940168634057\n",
            "Iteration: 495/537, Loss: 0.4771287739276886\n",
            "Iteration: 496/537, Loss: 0.0007903309888206422\n",
            "Iteration: 497/537, Loss: 0.0021305198315531015\n",
            "Iteration: 498/537, Loss: 0.003641290357336402\n",
            "Iteration: 499/537, Loss: 0.00022636642097495496\n",
            "Iteration: 500/537, Loss: 0.38291624188423157\n",
            "Iteration: 501/537, Loss: 0.0003194681485183537\n",
            "Iteration: 502/537, Loss: 0.0015677128685638309\n",
            "Iteration: 503/537, Loss: 0.001759431092068553\n",
            "Iteration: 504/537, Loss: 0.09636609256267548\n",
            "Iteration: 505/537, Loss: 0.0012162986677139997\n",
            "Iteration: 506/537, Loss: 0.00012902478920295835\n",
            "Iteration: 507/537, Loss: 0.0028402949683368206\n",
            "Iteration: 508/537, Loss: 0.05929078161716461\n",
            "Iteration: 509/537, Loss: 0.001845206948928535\n",
            "Iteration: 510/537, Loss: 0.004915057681500912\n",
            "Iteration: 511/537, Loss: 0.0060341935604810715\n",
            "Iteration: 512/537, Loss: 0.022223390638828278\n",
            "Iteration: 513/537, Loss: 0.002639282960444689\n",
            "Iteration: 514/537, Loss: 0.0007093496969901025\n",
            "Iteration: 515/537, Loss: 0.0010674773948267102\n",
            "Iteration: 516/537, Loss: 0.05930408835411072\n",
            "Iteration: 517/537, Loss: 0.0024243067018687725\n",
            "Iteration: 518/537, Loss: 0.0005792467854917049\n",
            "Iteration: 519/537, Loss: 0.0014973602956160903\n",
            "Iteration: 520/537, Loss: 0.0030472520738840103\n",
            "Iteration: 521/537, Loss: 0.0010679216356948018\n",
            "Iteration: 522/537, Loss: 0.0003919517039321363\n",
            "Iteration: 523/537, Loss: 0.0005587410996668041\n",
            "Iteration: 524/537, Loss: 0.00538815651088953\n",
            "Iteration: 525/537, Loss: 0.0008625895716249943\n",
            "Iteration: 526/537, Loss: 0.0007427041418850422\n",
            "Iteration: 527/537, Loss: 0.003459818195551634\n",
            "Iteration: 528/537, Loss: 0.00016314961249008775\n",
            "Iteration: 529/537, Loss: 0.000673557398840785\n",
            "Iteration: 530/537, Loss: 0.027732163667678833\n",
            "Iteration: 531/537, Loss: 0.006391425617039204\n",
            "Iteration: 532/537, Loss: 0.004999932833015919\n",
            "Iteration: 533/537, Loss: 0.00040972771239466965\n",
            "Iteration: 534/537, Loss: 0.0005326210521161556\n",
            "Iteration: 535/537, Loss: 0.0027123033069074154\n",
            "Iteration: 536/537, Loss: 0.005348470993340015\n",
            "Iteration: 537/537, Loss: 0.07416027039289474\n",
            "Epoch 47 train loss: 0.00833150005679167\n",
            "Epoch 47 test loss: 0.07657962553019859\n",
            "-------------------------------------\n",
            "Iteration: 1/537, Loss: 0.0010576779022812843\n",
            "Iteration: 2/537, Loss: 0.0007751142838969827\n",
            "Iteration: 3/537, Loss: 0.00028157461201772094\n",
            "Iteration: 4/537, Loss: 0.0007013159338384867\n",
            "Iteration: 5/537, Loss: 0.0010386462090536952\n",
            "Iteration: 6/537, Loss: 0.0011555503588169813\n",
            "Iteration: 7/537, Loss: 0.009176963940262794\n",
            "Iteration: 8/537, Loss: 0.010836943052709103\n",
            "Iteration: 9/537, Loss: 0.002089476678520441\n",
            "Iteration: 10/537, Loss: 0.0006547179073095322\n",
            "Iteration: 11/537, Loss: 0.0012678316561505198\n",
            "Iteration: 12/537, Loss: 0.0012485279003158212\n",
            "Iteration: 13/537, Loss: 0.0002935021766461432\n",
            "Iteration: 14/537, Loss: 0.00241533643566072\n",
            "Iteration: 15/537, Loss: 0.004584707319736481\n",
            "Iteration: 16/537, Loss: 0.0002282705099787563\n",
            "Iteration: 17/537, Loss: 0.00038756587309762836\n",
            "Iteration: 18/537, Loss: 0.0016714719822630286\n",
            "Iteration: 19/537, Loss: 0.009994220919907093\n",
            "Iteration: 20/537, Loss: 0.0002191948879044503\n",
            "Iteration: 21/537, Loss: 0.00041371845873072743\n",
            "Iteration: 22/537, Loss: 0.0004771905951201916\n",
            "Iteration: 23/537, Loss: 0.0011124634183943272\n",
            "Iteration: 24/537, Loss: 0.0002632882969919592\n",
            "Iteration: 25/537, Loss: 0.0026008300483226776\n",
            "Iteration: 26/537, Loss: 0.00021591676340904087\n",
            "Iteration: 27/537, Loss: 0.001430917764082551\n",
            "Iteration: 28/537, Loss: 0.00014029636804480106\n",
            "Iteration: 29/537, Loss: 0.0028032041154801846\n",
            "Iteration: 30/537, Loss: 0.0008553783409297466\n",
            "Iteration: 31/537, Loss: 0.0026026000268757343\n",
            "Iteration: 32/537, Loss: 0.0007193580386228859\n",
            "Iteration: 33/537, Loss: 0.0010322458110749722\n",
            "Iteration: 34/537, Loss: 0.00026050707674585283\n",
            "Iteration: 35/537, Loss: 0.0005177263519726694\n",
            "Iteration: 36/537, Loss: 0.010210047475993633\n",
            "Iteration: 37/537, Loss: 0.007289435714483261\n",
            "Iteration: 38/537, Loss: 0.000734904722776264\n",
            "Iteration: 39/537, Loss: 0.02548777125775814\n",
            "Iteration: 40/537, Loss: 0.0020800547208637\n",
            "Iteration: 41/537, Loss: 0.0004220704431645572\n",
            "Iteration: 42/537, Loss: 0.002209280850365758\n",
            "Iteration: 43/537, Loss: 0.0010075389873236418\n",
            "Iteration: 44/537, Loss: 0.0022498266771435738\n",
            "Iteration: 45/537, Loss: 0.0015136393485590816\n",
            "Iteration: 46/537, Loss: 0.0024196351878345013\n",
            "Iteration: 47/537, Loss: 0.0004779128939844668\n",
            "Iteration: 48/537, Loss: 0.003142704488709569\n",
            "Iteration: 49/537, Loss: 0.0017188896890729666\n",
            "Iteration: 50/537, Loss: 0.0020889570005238056\n",
            "Iteration: 51/537, Loss: 0.0001540348312119022\n",
            "Iteration: 52/537, Loss: 0.109688900411129\n",
            "Iteration: 53/537, Loss: 0.0054108393378555775\n",
            "Iteration: 54/537, Loss: 0.0006631143041886389\n",
            "Iteration: 55/537, Loss: 0.0012504992773756385\n",
            "Iteration: 56/537, Loss: 0.0012639493215829134\n",
            "Iteration: 57/537, Loss: 0.013153351843357086\n",
            "Iteration: 58/537, Loss: 0.00030430458718910813\n",
            "Iteration: 59/537, Loss: 0.005589008331298828\n",
            "Iteration: 60/537, Loss: 0.000243395654251799\n",
            "Iteration: 61/537, Loss: 0.0019417706644162536\n",
            "Iteration: 62/537, Loss: 0.0001260696881217882\n",
            "Iteration: 63/537, Loss: 0.0018270460423082113\n",
            "Iteration: 64/537, Loss: 0.000180338611244224\n",
            "Iteration: 65/537, Loss: 0.0008318496402353048\n",
            "Iteration: 66/537, Loss: 0.00034927157685160637\n",
            "Iteration: 67/537, Loss: 0.0005126215983182192\n",
            "Iteration: 68/537, Loss: 0.0009978723246604204\n",
            "Iteration: 69/537, Loss: 0.00029232498491182923\n",
            "Iteration: 70/537, Loss: 0.0007241214043460786\n",
            "Iteration: 71/537, Loss: 0.005185725167393684\n",
            "Iteration: 72/537, Loss: 0.0001515846379334107\n",
            "Iteration: 73/537, Loss: 0.003779833437874913\n",
            "Iteration: 74/537, Loss: 0.1705104112625122\n",
            "Iteration: 75/537, Loss: 0.0015733108157292008\n",
            "Iteration: 76/537, Loss: 0.0005613931571133435\n",
            "Iteration: 77/537, Loss: 0.001060008304193616\n",
            "Iteration: 78/537, Loss: 0.0010782665340229869\n",
            "Iteration: 79/537, Loss: 0.002419712720438838\n",
            "Iteration: 80/537, Loss: 0.005277236457914114\n",
            "Iteration: 81/537, Loss: 0.00145138637162745\n",
            "Iteration: 82/537, Loss: 0.0007809823146089911\n",
            "Iteration: 83/537, Loss: 0.0009742632391862571\n",
            "Iteration: 84/537, Loss: 0.02402939647436142\n",
            "Iteration: 85/537, Loss: 0.0004444885707926005\n",
            "Iteration: 86/537, Loss: 0.0006355188088491559\n",
            "Iteration: 87/537, Loss: 0.0015437518013641238\n",
            "Iteration: 88/537, Loss: 0.0029412037692964077\n",
            "Iteration: 89/537, Loss: 0.0005768952541984618\n",
            "Iteration: 90/537, Loss: 0.0023205841425806284\n",
            "Iteration: 91/537, Loss: 0.000287150323856622\n",
            "Iteration: 92/537, Loss: 0.0004076557233929634\n",
            "Iteration: 93/537, Loss: 0.01607377640902996\n",
            "Iteration: 94/537, Loss: 0.00041200206032954156\n",
            "Iteration: 95/537, Loss: 0.0005437535000964999\n",
            "Iteration: 96/537, Loss: 0.0009539367165416479\n",
            "Iteration: 97/537, Loss: 0.000351919763488695\n",
            "Iteration: 98/537, Loss: 0.0005812287563458085\n",
            "Iteration: 99/537, Loss: 0.00026956640067510307\n",
            "Iteration: 100/537, Loss: 0.00047475352766923606\n",
            "Iteration: 101/537, Loss: 0.0007173531921580434\n",
            "Iteration: 102/537, Loss: 0.0007140191155485809\n",
            "Iteration: 103/537, Loss: 0.0002190358645748347\n",
            "Iteration: 104/537, Loss: 0.0008883406408131123\n",
            "Iteration: 105/537, Loss: 0.003834642469882965\n",
            "Iteration: 106/537, Loss: 0.0006084829801693559\n",
            "Iteration: 107/537, Loss: 0.002947819884866476\n",
            "Iteration: 108/537, Loss: 0.0033668270334601402\n",
            "Iteration: 109/537, Loss: 0.0692949965596199\n",
            "Iteration: 110/537, Loss: 0.7560732364654541\n",
            "Iteration: 111/537, Loss: 0.0003807941684499383\n",
            "Iteration: 112/537, Loss: 0.002073936630040407\n",
            "Iteration: 113/537, Loss: 0.00040854711551219225\n",
            "Iteration: 114/537, Loss: 0.0027152346447110176\n",
            "Iteration: 115/537, Loss: 0.07869947701692581\n",
            "Iteration: 116/537, Loss: 0.004298733547329903\n",
            "Iteration: 117/537, Loss: 0.20720010995864868\n",
            "Iteration: 118/537, Loss: 0.03540783002972603\n",
            "Iteration: 119/537, Loss: 0.004104339051991701\n",
            "Iteration: 120/537, Loss: 0.0010144960833713412\n",
            "Iteration: 121/537, Loss: 0.005772004369646311\n",
            "Iteration: 122/537, Loss: 0.0017208444187417626\n",
            "Iteration: 123/537, Loss: 0.0007630932959727943\n",
            "Iteration: 124/537, Loss: 0.213453471660614\n",
            "Iteration: 125/537, Loss: 0.002744420664384961\n",
            "Iteration: 126/537, Loss: 0.0002619498409330845\n",
            "Iteration: 127/537, Loss: 0.024758467450737953\n",
            "Iteration: 128/537, Loss: 0.048469386994838715\n",
            "Iteration: 129/537, Loss: 0.0007155202329158783\n",
            "Iteration: 130/537, Loss: 0.003177529899403453\n",
            "Iteration: 131/537, Loss: 0.007198143284767866\n",
            "Iteration: 132/537, Loss: 0.002402759622782469\n",
            "Iteration: 133/537, Loss: 0.0010115817422047257\n",
            "Iteration: 134/537, Loss: 0.0020004711113870144\n",
            "Iteration: 135/537, Loss: 0.0023111954797059298\n",
            "Iteration: 136/537, Loss: 0.016835372895002365\n",
            "Iteration: 137/537, Loss: 0.004495017230510712\n",
            "Iteration: 138/537, Loss: 0.03981317952275276\n",
            "Iteration: 139/537, Loss: 0.0015152508858591318\n",
            "Iteration: 140/537, Loss: 0.0003177211037836969\n",
            "Iteration: 141/537, Loss: 0.000373794260667637\n",
            "Iteration: 142/537, Loss: 0.06928456574678421\n",
            "Iteration: 143/537, Loss: 0.0630502849817276\n",
            "Iteration: 144/537, Loss: 0.0007869424298405647\n",
            "Iteration: 145/537, Loss: 0.014576777815818787\n",
            "Iteration: 146/537, Loss: 0.0006502342876046896\n",
            "Iteration: 147/537, Loss: 0.010288184508681297\n",
            "Iteration: 148/537, Loss: 0.003111955476924777\n",
            "Iteration: 149/537, Loss: 0.002222518203780055\n",
            "Iteration: 150/537, Loss: 0.0035541236866265535\n",
            "Iteration: 151/537, Loss: 0.002845186274498701\n",
            "Iteration: 152/537, Loss: 0.0010439823381602764\n",
            "Iteration: 153/537, Loss: 0.0017596186371520162\n",
            "Iteration: 154/537, Loss: 0.0015814169310033321\n",
            "Iteration: 155/537, Loss: 0.0005883770063519478\n",
            "Iteration: 156/537, Loss: 0.0010566406417638063\n",
            "Iteration: 157/537, Loss: 0.0003104037023149431\n",
            "Iteration: 158/537, Loss: 0.001337299938313663\n",
            "Iteration: 159/537, Loss: 0.0043585821986198425\n",
            "Iteration: 160/537, Loss: 0.0011205073678866029\n",
            "Iteration: 161/537, Loss: 0.0011731586419045925\n",
            "Iteration: 162/537, Loss: 0.004706572275608778\n",
            "Iteration: 163/537, Loss: 0.00014315622684080154\n",
            "Iteration: 164/537, Loss: 0.000699758471455425\n",
            "Iteration: 165/537, Loss: 0.00048152205999940634\n",
            "Iteration: 166/537, Loss: 4.2586099880281836e-05\n",
            "Iteration: 167/537, Loss: 0.00261759664863348\n",
            "Iteration: 168/537, Loss: 0.004764403682202101\n",
            "Iteration: 169/537, Loss: 0.00019661565602291375\n",
            "Iteration: 170/537, Loss: 0.003259956371039152\n",
            "Iteration: 171/537, Loss: 0.008430909365415573\n",
            "Iteration: 172/537, Loss: 0.0006769488099962473\n",
            "Iteration: 173/537, Loss: 0.0006734425551258028\n",
            "Iteration: 174/537, Loss: 0.006060220301151276\n",
            "Iteration: 175/537, Loss: 0.0030439202673733234\n",
            "Iteration: 176/537, Loss: 0.0006755829090252519\n",
            "Iteration: 177/537, Loss: 0.005602160468697548\n",
            "Iteration: 178/537, Loss: 0.00047954823821783066\n",
            "Iteration: 179/537, Loss: 0.006354799028486013\n",
            "Iteration: 180/537, Loss: 0.002090724417939782\n",
            "Iteration: 181/537, Loss: 0.04043377935886383\n",
            "Iteration: 182/537, Loss: 0.0011286342050880194\n",
            "Iteration: 183/537, Loss: 0.008937784470617771\n",
            "Iteration: 184/537, Loss: 0.003199739847332239\n",
            "Iteration: 185/537, Loss: 1.060543179512024\n",
            "Iteration: 186/537, Loss: 0.0024570811074227095\n",
            "Iteration: 187/537, Loss: 0.0004172619082964957\n",
            "Iteration: 188/537, Loss: 0.0010546231642365456\n",
            "Iteration: 189/537, Loss: 0.1010504812002182\n",
            "Iteration: 190/537, Loss: 0.004274128936231136\n",
            "Iteration: 191/537, Loss: 0.0015626875683665276\n",
            "Iteration: 192/537, Loss: 0.01783730275928974\n",
            "Iteration: 193/537, Loss: 0.004408080596476793\n",
            "Iteration: 194/537, Loss: 0.0008962037973105907\n",
            "Iteration: 195/537, Loss: 0.025187604129314423\n",
            "Iteration: 196/537, Loss: 0.00285732070915401\n",
            "Iteration: 197/537, Loss: 0.03588397428393364\n",
            "Iteration: 198/537, Loss: 0.00290890340693295\n",
            "Iteration: 199/537, Loss: 0.007622122764587402\n",
            "Iteration: 200/537, Loss: 0.000573975732550025\n",
            "Iteration: 201/537, Loss: 0.000829662021715194\n",
            "Iteration: 202/537, Loss: 0.01010779570788145\n",
            "Iteration: 203/537, Loss: 0.03596112132072449\n",
            "Iteration: 204/537, Loss: 0.0008019290398806334\n",
            "Iteration: 205/537, Loss: 0.0012536098947748542\n",
            "Iteration: 206/537, Loss: 0.003973714541643858\n",
            "Iteration: 207/537, Loss: 0.002476351335644722\n",
            "Iteration: 208/537, Loss: 0.002921924227848649\n",
            "Iteration: 209/537, Loss: 0.028555985540151596\n",
            "Iteration: 210/537, Loss: 0.001234652241691947\n",
            "Iteration: 211/537, Loss: 0.0029591317288577557\n",
            "Iteration: 212/537, Loss: 0.007059059105813503\n",
            "Iteration: 213/537, Loss: 0.005489313043653965\n",
            "Iteration: 214/537, Loss: 0.0021265968680381775\n",
            "Iteration: 215/537, Loss: 0.0032224170863628387\n",
            "Iteration: 216/537, Loss: 0.0024169355165213346\n",
            "Iteration: 217/537, Loss: 0.0016974606551229954\n",
            "Iteration: 218/537, Loss: 0.0027514880057424307\n",
            "Iteration: 219/537, Loss: 0.0003260272496845573\n",
            "Iteration: 220/537, Loss: 0.00894565787166357\n",
            "Iteration: 221/537, Loss: 0.0011715278960764408\n",
            "Iteration: 222/537, Loss: 0.05823677405714989\n",
            "Iteration: 223/537, Loss: 0.018052008002996445\n",
            "Iteration: 224/537, Loss: 0.0009129169047810137\n",
            "Iteration: 225/537, Loss: 0.00043650995939970016\n",
            "Iteration: 226/537, Loss: 0.0004442620265763253\n",
            "Iteration: 227/537, Loss: 1.7859570980072021\n",
            "Iteration: 228/537, Loss: 0.018829811364412308\n",
            "Iteration: 229/537, Loss: 0.0741066038608551\n",
            "Iteration: 230/537, Loss: 0.014158356934785843\n",
            "Iteration: 231/537, Loss: 0.007299596443772316\n",
            "Iteration: 232/537, Loss: 0.011693799868226051\n",
            "Iteration: 233/537, Loss: 0.0008588923956267536\n",
            "Iteration: 234/537, Loss: 0.005012001842260361\n",
            "Iteration: 235/537, Loss: 0.0016197225777432323\n",
            "Iteration: 236/537, Loss: 0.0007499372586607933\n",
            "Iteration: 237/537, Loss: 0.0028110085986554623\n",
            "Iteration: 238/537, Loss: 0.0033249957486987114\n",
            "Iteration: 239/537, Loss: 0.0021626059897243977\n",
            "Iteration: 240/537, Loss: 0.12939070165157318\n",
            "Iteration: 241/537, Loss: 0.04817432910203934\n",
            "Iteration: 242/537, Loss: 0.0030579306185245514\n",
            "Iteration: 243/537, Loss: 0.015471049584448338\n",
            "Iteration: 244/537, Loss: 0.01678672805428505\n",
            "Iteration: 245/537, Loss: 0.0009069534717127681\n",
            "Iteration: 246/537, Loss: 0.003822311758995056\n",
            "Iteration: 247/537, Loss: 0.0008181512239389122\n",
            "Iteration: 248/537, Loss: 0.0007436424493789673\n",
            "Iteration: 249/537, Loss: 0.0019116573967039585\n",
            "Iteration: 250/537, Loss: 0.0018267265986651182\n",
            "Iteration: 251/537, Loss: 0.0006986487424001098\n",
            "Iteration: 252/537, Loss: 0.001160891493782401\n",
            "Iteration: 253/537, Loss: 0.0032140235416591167\n",
            "Iteration: 254/537, Loss: 0.002786366268992424\n",
            "Iteration: 255/537, Loss: 0.00138485140632838\n",
            "Iteration: 256/537, Loss: 0.003594235749915242\n",
            "Iteration: 257/537, Loss: 0.017603952437639236\n",
            "Iteration: 258/537, Loss: 0.002284853020682931\n",
            "Iteration: 259/537, Loss: 0.0014937882078811526\n",
            "Iteration: 260/537, Loss: 0.008293543942272663\n",
            "Iteration: 261/537, Loss: 0.0009490965749137104\n",
            "Iteration: 262/537, Loss: 0.0017959086690098047\n",
            "Iteration: 263/537, Loss: 0.0018234933959320188\n",
            "Iteration: 264/537, Loss: 0.0006256020278669894\n",
            "Iteration: 265/537, Loss: 0.0012494584079831839\n",
            "Iteration: 266/537, Loss: 0.000293724617222324\n",
            "Iteration: 267/537, Loss: 0.006194086279720068\n",
            "Iteration: 268/537, Loss: 0.005473124794661999\n",
            "Iteration: 269/537, Loss: 0.009497348219156265\n",
            "Iteration: 270/537, Loss: 0.000366723514162004\n",
            "Iteration: 271/537, Loss: 0.00033979094587266445\n",
            "Iteration: 272/537, Loss: 0.0027044902089983225\n",
            "Iteration: 273/537, Loss: 0.001193871023133397\n",
            "Iteration: 274/537, Loss: 0.005768112372606993\n",
            "Iteration: 275/537, Loss: 0.013407669961452484\n",
            "Iteration: 276/537, Loss: 0.00032184284646064043\n",
            "Iteration: 277/537, Loss: 0.0029114726930856705\n",
            "Iteration: 278/537, Loss: 0.0006155293667688966\n",
            "Iteration: 279/537, Loss: 0.0003658477507997304\n",
            "Iteration: 280/537, Loss: 0.003239089623093605\n",
            "Iteration: 281/537, Loss: 0.0001985598646569997\n",
            "Iteration: 282/537, Loss: 0.011043189093470573\n",
            "Iteration: 283/537, Loss: 0.0003525133361108601\n",
            "Iteration: 284/537, Loss: 0.0017869933508336544\n",
            "Iteration: 285/537, Loss: 0.0009406181052327156\n",
            "Iteration: 286/537, Loss: 0.16034768521785736\n",
            "Iteration: 287/537, Loss: 0.007273097522556782\n",
            "Iteration: 288/537, Loss: 0.005258024204522371\n",
            "Iteration: 289/537, Loss: 0.06836746633052826\n",
            "Iteration: 290/537, Loss: 0.024588651955127716\n",
            "Iteration: 291/537, Loss: 0.002326421905308962\n",
            "Iteration: 292/537, Loss: 0.0005684923962689936\n",
            "Iteration: 293/537, Loss: 0.0007035795133560896\n",
            "Iteration: 294/537, Loss: 0.00031884395866654813\n",
            "Iteration: 295/537, Loss: 0.000589301809668541\n",
            "Iteration: 296/537, Loss: 0.007311184424906969\n",
            "Iteration: 297/537, Loss: 0.11134427785873413\n",
            "Iteration: 298/537, Loss: 0.0013297094264999032\n",
            "Iteration: 299/537, Loss: 0.0003203389351256192\n",
            "Iteration: 300/537, Loss: 0.0007945756078697741\n",
            "Iteration: 301/537, Loss: 0.0007003274513408542\n",
            "Iteration: 302/537, Loss: 0.00198262557387352\n",
            "Iteration: 303/537, Loss: 0.003243254730477929\n",
            "Iteration: 304/537, Loss: 0.0006046956405043602\n",
            "Iteration: 305/537, Loss: 0.002678194548934698\n",
            "Iteration: 306/537, Loss: 0.0018218089826405048\n",
            "Iteration: 307/537, Loss: 0.0038322960026562214\n",
            "Iteration: 308/537, Loss: 0.00042541284346953034\n",
            "Iteration: 309/537, Loss: 0.0011347499676048756\n",
            "Iteration: 310/537, Loss: 0.008258255198597908\n",
            "Iteration: 311/537, Loss: 0.0010382175678387284\n",
            "Iteration: 312/537, Loss: 0.0002643734042067081\n",
            "Iteration: 313/537, Loss: 0.0004893605364486575\n",
            "Iteration: 314/537, Loss: 0.0003836285031866282\n",
            "Iteration: 315/537, Loss: 0.00026851598522625864\n",
            "Iteration: 316/537, Loss: 0.0006340238614939153\n",
            "Iteration: 317/537, Loss: 0.0017395524773746729\n",
            "Iteration: 318/537, Loss: 0.004643079359084368\n",
            "Iteration: 319/537, Loss: 0.001214682008139789\n",
            "Iteration: 320/537, Loss: 0.0025627564173191786\n",
            "Iteration: 321/537, Loss: 0.0006005945615470409\n",
            "Iteration: 322/537, Loss: 0.006541555747389793\n",
            "Iteration: 323/537, Loss: 0.0007688573095947504\n",
            "Iteration: 324/537, Loss: 0.006105823442339897\n",
            "Iteration: 325/537, Loss: 0.000730409286916256\n",
            "Iteration: 326/537, Loss: 0.000930659705772996\n",
            "Iteration: 327/537, Loss: 0.000444302917458117\n",
            "Iteration: 328/537, Loss: 0.0024911724030971527\n",
            "Iteration: 329/537, Loss: 0.0018040462164208293\n",
            "Iteration: 330/537, Loss: 0.00039377977373078465\n",
            "Iteration: 331/537, Loss: 0.0017044516280293465\n",
            "Iteration: 332/537, Loss: 0.1922401487827301\n",
            "Iteration: 333/537, Loss: 0.000310982926748693\n",
            "Iteration: 334/537, Loss: 0.0026721497997641563\n",
            "Iteration: 335/537, Loss: 0.00048802042147144675\n",
            "Iteration: 336/537, Loss: 0.009939956478774548\n",
            "Iteration: 337/537, Loss: 0.0002841946261469275\n",
            "Iteration: 338/537, Loss: 0.0003657292400021106\n",
            "Iteration: 339/537, Loss: 0.002605010289698839\n",
            "Iteration: 340/537, Loss: 0.00039231247501447797\n",
            "Iteration: 341/537, Loss: 0.010089313611388206\n",
            "Iteration: 342/537, Loss: 0.0008918290259316564\n",
            "Iteration: 343/537, Loss: 0.0006081791943870485\n",
            "Iteration: 344/537, Loss: 0.00022203286061994731\n",
            "Iteration: 345/537, Loss: 0.00043651601299643517\n",
            "Iteration: 346/537, Loss: 0.0015242156805470586\n",
            "Iteration: 347/537, Loss: 0.000684511789586395\n",
            "Iteration: 348/537, Loss: 0.00819089263677597\n",
            "Iteration: 349/537, Loss: 0.00025253870990127325\n",
            "Iteration: 350/537, Loss: 0.002359882229939103\n",
            "Iteration: 351/537, Loss: 0.0032084493432193995\n",
            "Iteration: 352/537, Loss: 0.0006983846542425454\n",
            "Iteration: 353/537, Loss: 0.000435718335211277\n",
            "Iteration: 354/537, Loss: 0.0009202200453728437\n",
            "Iteration: 355/537, Loss: 0.0005045430152677\n",
            "Iteration: 356/537, Loss: 0.004901846870779991\n",
            "Iteration: 357/537, Loss: 0.11753547191619873\n",
            "Iteration: 358/537, Loss: 0.04460940510034561\n",
            "Iteration: 359/537, Loss: 0.0030970191583037376\n",
            "Iteration: 360/537, Loss: 0.0010429220274090767\n",
            "Iteration: 361/537, Loss: 0.005463980138301849\n",
            "Iteration: 362/537, Loss: 0.01850329153239727\n",
            "Iteration: 363/537, Loss: 0.0021443525329232216\n",
            "Iteration: 364/537, Loss: 0.045620694756507874\n",
            "Iteration: 365/537, Loss: 0.0018249398563057184\n",
            "Iteration: 366/537, Loss: 0.0013799031730741262\n",
            "Iteration: 367/537, Loss: 0.0010956398909911513\n",
            "Iteration: 368/537, Loss: 0.0005782450316473842\n",
            "Iteration: 369/537, Loss: 0.00023573287762701511\n",
            "Iteration: 370/537, Loss: 0.0006673639290966094\n",
            "Iteration: 371/537, Loss: 0.010770861990749836\n",
            "Iteration: 372/537, Loss: 0.0023169140331447124\n",
            "Iteration: 373/537, Loss: 0.00011547006579348817\n",
            "Iteration: 374/537, Loss: 0.002271135337650776\n",
            "Iteration: 375/537, Loss: 0.0011390518629923463\n",
            "Iteration: 376/537, Loss: 0.003034994937479496\n",
            "Iteration: 377/537, Loss: 0.00043616179027594626\n",
            "Iteration: 378/537, Loss: 0.000860302709043026\n",
            "Iteration: 379/537, Loss: 0.02823084406554699\n",
            "Iteration: 380/537, Loss: 0.024561647325754166\n",
            "Iteration: 381/537, Loss: 0.00046406686305999756\n",
            "Iteration: 382/537, Loss: 0.007640843279659748\n",
            "Iteration: 383/537, Loss: 0.00011490346514619887\n",
            "Iteration: 384/537, Loss: 0.0009650239371694624\n",
            "Iteration: 385/537, Loss: 0.0022135854233056307\n",
            "Iteration: 386/537, Loss: 0.000375789066310972\n",
            "Iteration: 387/537, Loss: 0.0020217590499669313\n",
            "Iteration: 388/537, Loss: 0.007018266711384058\n",
            "Iteration: 389/537, Loss: 0.0006974173011258245\n",
            "Iteration: 390/537, Loss: 0.0019046596717089415\n",
            "Iteration: 391/537, Loss: 0.001052834209986031\n",
            "Iteration: 392/537, Loss: 0.004126759245991707\n",
            "Iteration: 393/537, Loss: 0.0003260006196796894\n",
            "Iteration: 394/537, Loss: 0.0006721081444993615\n",
            "Iteration: 395/537, Loss: 0.0017455333145335317\n",
            "Iteration: 396/537, Loss: 0.0005705271032638848\n",
            "Iteration: 397/537, Loss: 0.0002193248365074396\n",
            "Iteration: 398/537, Loss: 0.0015753540210425854\n",
            "Iteration: 399/537, Loss: 0.0008035579812712967\n",
            "Iteration: 400/537, Loss: 0.0019945064559578896\n",
            "Iteration: 401/537, Loss: 0.0005823008250445127\n",
            "Iteration: 402/537, Loss: 0.0006969435489736497\n",
            "Iteration: 403/537, Loss: 0.0015129185048863292\n",
            "Iteration: 404/537, Loss: 0.0006266431300900877\n",
            "Iteration: 405/537, Loss: 0.0003774993238039315\n",
            "Iteration: 406/537, Loss: 0.0004909827839583158\n",
            "Iteration: 407/537, Loss: 0.0010951682925224304\n",
            "Iteration: 408/537, Loss: 0.0006221100920811296\n",
            "Iteration: 409/537, Loss: 0.005908225663006306\n",
            "Iteration: 410/537, Loss: 0.0013073146110400558\n",
            "Iteration: 411/537, Loss: 0.000363803788786754\n",
            "Iteration: 412/537, Loss: 0.0024630632251501083\n",
            "Iteration: 413/537, Loss: 0.0004110957379452884\n",
            "Iteration: 414/537, Loss: 0.0005779971252195537\n",
            "Iteration: 415/537, Loss: 0.0004955718759447336\n",
            "Iteration: 416/537, Loss: 0.002880290849134326\n",
            "Iteration: 417/537, Loss: 0.09428763389587402\n",
            "Iteration: 418/537, Loss: 0.0008745769155211747\n",
            "Iteration: 419/537, Loss: 0.00013251900963950902\n",
            "Iteration: 420/537, Loss: 0.001492148032411933\n",
            "Iteration: 421/537, Loss: 0.0005998462438583374\n",
            "Iteration: 422/537, Loss: 0.000221658730879426\n",
            "Iteration: 423/537, Loss: 0.0003943357733078301\n",
            "Iteration: 424/537, Loss: 0.004149008076637983\n",
            "Iteration: 425/537, Loss: 0.00026593825896270573\n",
            "Iteration: 426/537, Loss: 0.0005026560975238681\n",
            "Iteration: 427/537, Loss: 0.05769561976194382\n",
            "Iteration: 428/537, Loss: 0.0014916833024471998\n",
            "Iteration: 429/537, Loss: 0.00027750537265092134\n",
            "Iteration: 430/537, Loss: 0.0005424996488727629\n",
            "Iteration: 431/537, Loss: 0.0013023691717535257\n",
            "Iteration: 432/537, Loss: 0.0006713253678753972\n",
            "Iteration: 433/537, Loss: 0.0033788082655519247\n",
            "Iteration: 434/537, Loss: 0.0009610865963622928\n",
            "Iteration: 435/537, Loss: 0.00040096649900078773\n",
            "Iteration: 436/537, Loss: 0.0002413718611933291\n",
            "Iteration: 437/537, Loss: 0.00013650726759806275\n",
            "Iteration: 438/537, Loss: 0.0006926439818926156\n",
            "Iteration: 439/537, Loss: 0.0002848951262421906\n",
            "Iteration: 440/537, Loss: 0.00037929703830741346\n",
            "Iteration: 441/537, Loss: 0.0017499547684565187\n",
            "Iteration: 442/537, Loss: 0.0030117337591946125\n",
            "Iteration: 443/537, Loss: 0.0001360349851893261\n",
            "Iteration: 444/537, Loss: 0.016879502683877945\n",
            "Iteration: 445/537, Loss: 0.0013073044829070568\n",
            "Iteration: 446/537, Loss: 0.0006505599594675004\n",
            "Iteration: 447/537, Loss: 0.09696608036756516\n",
            "Iteration: 448/537, Loss: 0.015395906753838062\n",
            "Iteration: 449/537, Loss: 0.039007630199193954\n",
            "Iteration: 450/537, Loss: 0.001487230765633285\n",
            "Iteration: 451/537, Loss: 0.0003077080473303795\n",
            "Iteration: 452/537, Loss: 0.0013171961763873696\n",
            "Iteration: 453/537, Loss: 0.0001979753578780219\n",
            "Iteration: 454/537, Loss: 0.0010653389617800713\n",
            "Iteration: 455/537, Loss: 0.0009475453407503664\n",
            "Iteration: 456/537, Loss: 0.001663758303038776\n",
            "Iteration: 457/537, Loss: 0.036770809441804886\n",
            "Iteration: 458/537, Loss: 0.32089513540267944\n",
            "Iteration: 459/537, Loss: 0.0006403790903277695\n",
            "Iteration: 460/537, Loss: 0.04925817251205444\n",
            "Iteration: 461/537, Loss: 0.000665034691337496\n",
            "Iteration: 462/537, Loss: 0.0007147772703319788\n",
            "Iteration: 463/537, Loss: 0.0012797736562788486\n",
            "Iteration: 464/537, Loss: 0.013721332885324955\n",
            "Iteration: 465/537, Loss: 0.003738650120794773\n",
            "Iteration: 466/537, Loss: 0.0027155522257089615\n",
            "Iteration: 467/537, Loss: 0.0005711551639251411\n",
            "Iteration: 468/537, Loss: 0.0006390446797013283\n",
            "Iteration: 469/537, Loss: 0.0002027588925557211\n",
            "Iteration: 470/537, Loss: 0.0003305993741378188\n",
            "Iteration: 471/537, Loss: 7.283248123712838e-05\n",
            "Iteration: 472/537, Loss: 0.0006306074210442603\n",
            "Iteration: 473/537, Loss: 0.0002796573389787227\n",
            "Iteration: 474/537, Loss: 0.001723624998703599\n",
            "Iteration: 475/537, Loss: 0.00025689764879643917\n",
            "Iteration: 476/537, Loss: 0.0006818682886660099\n",
            "Iteration: 477/537, Loss: 0.0008771977736614645\n",
            "Iteration: 478/537, Loss: 0.0021603424102067947\n",
            "Iteration: 479/537, Loss: 0.0004814440035261214\n",
            "Iteration: 480/537, Loss: 0.0021610651165246964\n",
            "Iteration: 481/537, Loss: 0.0002290545089635998\n",
            "Iteration: 482/537, Loss: 0.0001899872295325622\n",
            "Iteration: 483/537, Loss: 0.001225305488333106\n",
            "Iteration: 484/537, Loss: 0.00159233168233186\n",
            "Iteration: 485/537, Loss: 0.000337795092491433\n",
            "Iteration: 486/537, Loss: 0.07147100567817688\n",
            "Iteration: 487/537, Loss: 0.22553421556949615\n",
            "Iteration: 488/537, Loss: 0.0012359356041997671\n",
            "Iteration: 489/537, Loss: 0.0007022207719273865\n",
            "Iteration: 490/537, Loss: 0.0008197144488804042\n",
            "Iteration: 491/537, Loss: 0.0003972985432483256\n",
            "Iteration: 492/537, Loss: 0.00024041497090365738\n",
            "Iteration: 493/537, Loss: 0.08867364376783371\n",
            "Iteration: 494/537, Loss: 0.0012271525338292122\n",
            "Iteration: 495/537, Loss: 0.0015232325531542301\n",
            "Iteration: 496/537, Loss: 0.0004245559684932232\n",
            "Iteration: 497/537, Loss: 0.03925875574350357\n",
            "Iteration: 498/537, Loss: 0.0006043352768756449\n",
            "Iteration: 499/537, Loss: 0.0013807513751089573\n",
            "Iteration: 500/537, Loss: 0.0011457279324531555\n",
            "Iteration: 501/537, Loss: 0.0008524175500497222\n",
            "Iteration: 502/537, Loss: 0.0009897617856040597\n",
            "Iteration: 503/537, Loss: 0.000860302010551095\n",
            "Iteration: 504/537, Loss: 0.0109716160222888\n",
            "Iteration: 505/537, Loss: 0.0004521616792771965\n",
            "Iteration: 506/537, Loss: 0.0006222798838280141\n",
            "Iteration: 507/537, Loss: 0.0007890365668572485\n",
            "Iteration: 508/537, Loss: 0.00100816844496876\n",
            "Iteration: 509/537, Loss: 0.001273355563171208\n",
            "Iteration: 510/537, Loss: 0.0002962048747576773\n",
            "Iteration: 511/537, Loss: 0.014380844309926033\n",
            "Iteration: 512/537, Loss: 0.0019890039693564177\n",
            "Iteration: 513/537, Loss: 0.003543216735124588\n",
            "Iteration: 514/537, Loss: 0.0004081001097802073\n",
            "Iteration: 515/537, Loss: 0.0005146586918272078\n",
            "Iteration: 516/537, Loss: 0.0001635882945265621\n",
            "Iteration: 517/537, Loss: 0.0005583035526797175\n",
            "Iteration: 518/537, Loss: 0.004189305938780308\n",
            "Iteration: 519/537, Loss: 0.0007296637631952763\n",
            "Iteration: 520/537, Loss: 0.000583056767936796\n",
            "Iteration: 521/537, Loss: 0.00012521486496552825\n",
            "Iteration: 522/537, Loss: 0.0006267528515309095\n",
            "Iteration: 523/537, Loss: 0.004575237166136503\n",
            "Iteration: 524/537, Loss: 0.0012094259727746248\n",
            "Iteration: 525/537, Loss: 0.0020883972756564617\n",
            "Iteration: 526/537, Loss: 0.0007861282210797071\n",
            "Iteration: 527/537, Loss: 0.0010554654290899634\n",
            "Iteration: 528/537, Loss: 0.010435929521918297\n",
            "Iteration: 529/537, Loss: 0.1991724818944931\n",
            "Iteration: 530/537, Loss: 0.00022640844690613449\n",
            "Iteration: 531/537, Loss: 0.08816361427307129\n",
            "Iteration: 532/537, Loss: 0.0012371724005788565\n",
            "Iteration: 533/537, Loss: 0.0015426881145685911\n",
            "Iteration: 534/537, Loss: 0.00030694136512465775\n",
            "Iteration: 535/537, Loss: 0.0007283980376087129\n",
            "Iteration: 536/537, Loss: 0.004953991621732712\n",
            "Iteration: 537/537, Loss: 0.5059767961502075\n",
            "Epoch 48 train loss: 0.017282446798211826\n",
            "Epoch 48 test loss: 0.054769958066861836\n",
            "-------------------------------------\n",
            "Iteration: 1/537, Loss: 0.0005015537608414888\n",
            "Iteration: 2/537, Loss: 0.0037280835676938295\n",
            "Iteration: 3/537, Loss: 0.00043366168392822146\n",
            "Iteration: 4/537, Loss: 0.0006407038308680058\n",
            "Iteration: 5/537, Loss: 0.0006579728215001523\n",
            "Iteration: 6/537, Loss: 0.006049040239304304\n",
            "Iteration: 7/537, Loss: 0.0006017470150254667\n",
            "Iteration: 8/537, Loss: 0.0005410637822933495\n",
            "Iteration: 9/537, Loss: 0.004132021218538284\n",
            "Iteration: 10/537, Loss: 0.0003559062606655061\n",
            "Iteration: 11/537, Loss: 0.0014143106527626514\n",
            "Iteration: 12/537, Loss: 0.0004705788451246917\n",
            "Iteration: 13/537, Loss: 0.00042016152292490005\n",
            "Iteration: 14/537, Loss: 0.0008382817613892257\n",
            "Iteration: 15/537, Loss: 0.0004954601754434407\n",
            "Iteration: 16/537, Loss: 0.0002934923686552793\n",
            "Iteration: 17/537, Loss: 0.0003382951545063406\n",
            "Iteration: 18/537, Loss: 0.0002094629599014297\n",
            "Iteration: 19/537, Loss: 0.0017301691696047783\n",
            "Iteration: 20/537, Loss: 0.0006141712656244636\n",
            "Iteration: 21/537, Loss: 0.0002299789193784818\n",
            "Iteration: 22/537, Loss: 0.0022968370467424393\n",
            "Iteration: 23/537, Loss: 0.00022551938309334219\n",
            "Iteration: 24/537, Loss: 0.003785042092204094\n",
            "Iteration: 25/537, Loss: 0.0011586608598008752\n",
            "Iteration: 26/537, Loss: 0.00013733896776102483\n",
            "Iteration: 27/537, Loss: 0.0024688742123544216\n",
            "Iteration: 28/537, Loss: 0.10981132090091705\n",
            "Iteration: 29/537, Loss: 0.0006843592855148017\n",
            "Iteration: 30/537, Loss: 0.00020375898748170584\n",
            "Iteration: 31/537, Loss: 0.0004130231973249465\n",
            "Iteration: 32/537, Loss: 0.0003773625649046153\n",
            "Iteration: 33/537, Loss: 0.00030893695657141507\n",
            "Iteration: 34/537, Loss: 0.009450522251427174\n",
            "Iteration: 35/537, Loss: 0.009809065610170364\n",
            "Iteration: 36/537, Loss: 0.0010677927639335394\n",
            "Iteration: 37/537, Loss: 0.0003533685812726617\n",
            "Iteration: 38/537, Loss: 0.007342739496380091\n",
            "Iteration: 39/537, Loss: 0.00023865047842264175\n",
            "Iteration: 40/537, Loss: 0.0005096654640510678\n",
            "Iteration: 41/537, Loss: 0.009179194457828999\n",
            "Iteration: 42/537, Loss: 0.0014712759293615818\n",
            "Iteration: 43/537, Loss: 0.00027114152908325195\n",
            "Iteration: 44/537, Loss: 0.0008895020000636578\n",
            "Iteration: 45/537, Loss: 0.0010359141742810607\n",
            "Iteration: 46/537, Loss: 0.00045741681242361665\n",
            "Iteration: 47/537, Loss: 0.00035718781873583794\n",
            "Iteration: 48/537, Loss: 0.003534839255735278\n",
            "Iteration: 49/537, Loss: 0.010217101313173771\n",
            "Iteration: 50/537, Loss: 0.0019281406421214342\n",
            "Iteration: 51/537, Loss: 0.0007964057731442153\n",
            "Iteration: 52/537, Loss: 0.00026362048811279237\n",
            "Iteration: 53/537, Loss: 0.0028583912644535303\n",
            "Iteration: 54/537, Loss: 0.0002733472501859069\n",
            "Iteration: 55/537, Loss: 0.0029048449359834194\n",
            "Iteration: 56/537, Loss: 0.33531123399734497\n",
            "Iteration: 57/537, Loss: 0.001852935878559947\n",
            "Iteration: 58/537, Loss: 0.0013750296784564853\n",
            "Iteration: 59/537, Loss: 0.0002766076067928225\n",
            "Iteration: 60/537, Loss: 0.0004497114277910441\n",
            "Iteration: 61/537, Loss: 0.0015140273608267307\n",
            "Iteration: 62/537, Loss: 0.0007959940121509135\n",
            "Iteration: 63/537, Loss: 0.017528768628835678\n",
            "Iteration: 64/537, Loss: 0.002095729811117053\n",
            "Iteration: 65/537, Loss: 0.003917187452316284\n",
            "Iteration: 66/537, Loss: 0.00029783271020278335\n",
            "Iteration: 67/537, Loss: 0.0007627364830113947\n",
            "Iteration: 68/537, Loss: 0.0007341012824326754\n",
            "Iteration: 69/537, Loss: 5.8053141401614994e-05\n",
            "Iteration: 70/537, Loss: 0.0006459870492108166\n",
            "Iteration: 71/537, Loss: 0.23786258697509766\n",
            "Iteration: 72/537, Loss: 0.03983655944466591\n",
            "Iteration: 73/537, Loss: 0.003932449501007795\n",
            "Iteration: 74/537, Loss: 0.029298432171344757\n",
            "Iteration: 75/537, Loss: 0.0005858977092429996\n",
            "Iteration: 76/537, Loss: 0.00013948805280961096\n",
            "Iteration: 77/537, Loss: 0.07311055809259415\n",
            "Iteration: 78/537, Loss: 0.003383482340723276\n",
            "Iteration: 79/537, Loss: 0.0004904051893390715\n",
            "Iteration: 80/537, Loss: 0.15056277811527252\n",
            "Iteration: 81/537, Loss: 0.00042258010944351554\n",
            "Iteration: 82/537, Loss: 0.0006379600381478667\n",
            "Iteration: 83/537, Loss: 0.0007462027715519071\n",
            "Iteration: 84/537, Loss: 0.0002042936539510265\n",
            "Iteration: 85/537, Loss: 0.02752450853586197\n",
            "Iteration: 86/537, Loss: 0.002798965899273753\n",
            "Iteration: 87/537, Loss: 0.0011189996730536222\n",
            "Iteration: 88/537, Loss: 0.0002452966873534024\n",
            "Iteration: 89/537, Loss: 0.00022792629897594452\n",
            "Iteration: 90/537, Loss: 0.030689353123307228\n",
            "Iteration: 91/537, Loss: 0.004037350881844759\n",
            "Iteration: 92/537, Loss: 0.000825179391540587\n",
            "Iteration: 93/537, Loss: 0.0025344486348330975\n",
            "Iteration: 94/537, Loss: 0.0007483536610379815\n",
            "Iteration: 95/537, Loss: 0.0012186213862150908\n",
            "Iteration: 96/537, Loss: 0.0034338091500103474\n",
            "Iteration: 97/537, Loss: 0.0014510821783915162\n",
            "Iteration: 98/537, Loss: 0.004455220885574818\n",
            "Iteration: 99/537, Loss: 0.0007160596433095634\n",
            "Iteration: 100/537, Loss: 0.029274553060531616\n",
            "Iteration: 101/537, Loss: 0.0002483624266460538\n",
            "Iteration: 102/537, Loss: 0.00014937735977582633\n",
            "Iteration: 103/537, Loss: 0.003019134746864438\n",
            "Iteration: 104/537, Loss: 0.00021388237655628473\n",
            "Iteration: 105/537, Loss: 0.0004932751762680709\n",
            "Iteration: 106/537, Loss: 0.0002985040773637593\n",
            "Iteration: 107/537, Loss: 0.00296105002053082\n",
            "Iteration: 108/537, Loss: 0.0008879624074324965\n",
            "Iteration: 109/537, Loss: 0.008271731436252594\n",
            "Iteration: 110/537, Loss: 0.0002479964750818908\n",
            "Iteration: 111/537, Loss: 8.084932778729126e-05\n",
            "Iteration: 112/537, Loss: 0.0016911550192162395\n",
            "Iteration: 113/537, Loss: 0.0019035468576475978\n",
            "Iteration: 114/537, Loss: 0.00038058453355915844\n",
            "Iteration: 115/537, Loss: 0.0009144119103439152\n",
            "Iteration: 116/537, Loss: 0.037069741636514664\n",
            "Iteration: 117/537, Loss: 0.003075165208429098\n",
            "Iteration: 118/537, Loss: 0.00016063847579061985\n",
            "Iteration: 119/537, Loss: 0.002774917520582676\n",
            "Iteration: 120/537, Loss: 0.0022635776549577713\n",
            "Iteration: 121/537, Loss: 0.001135412254370749\n",
            "Iteration: 122/537, Loss: 0.005484206601977348\n",
            "Iteration: 123/537, Loss: 0.00015167647507041693\n",
            "Iteration: 124/537, Loss: 0.0017380168428644538\n",
            "Iteration: 125/537, Loss: 0.0009801337728276849\n",
            "Iteration: 126/537, Loss: 0.07199903577566147\n",
            "Iteration: 127/537, Loss: 0.00013546814443543553\n",
            "Iteration: 128/537, Loss: 0.008712938986718655\n",
            "Iteration: 129/537, Loss: 0.001015526708215475\n",
            "Iteration: 130/537, Loss: 0.0021347098518162966\n",
            "Iteration: 131/537, Loss: 0.02099895477294922\n",
            "Iteration: 132/537, Loss: 0.0004056574543938041\n",
            "Iteration: 133/537, Loss: 0.0005780612700618804\n",
            "Iteration: 134/537, Loss: 0.004157242830842733\n",
            "Iteration: 135/537, Loss: 0.0011839050566777587\n",
            "Iteration: 136/537, Loss: 0.0029877403285354376\n",
            "Iteration: 137/537, Loss: 0.0948987826704979\n",
            "Iteration: 138/537, Loss: 0.0004409743705764413\n",
            "Iteration: 139/537, Loss: 0.0003814820956904441\n",
            "Iteration: 140/537, Loss: 0.0018509848741814494\n",
            "Iteration: 141/537, Loss: 0.0015258773928508162\n",
            "Iteration: 142/537, Loss: 0.00013221973495092243\n",
            "Iteration: 143/537, Loss: 0.0017813449958339334\n",
            "Iteration: 144/537, Loss: 0.0018218656769022346\n",
            "Iteration: 145/537, Loss: 0.00013636291259899735\n",
            "Iteration: 146/537, Loss: 0.12438990920782089\n",
            "Iteration: 147/537, Loss: 0.008443497121334076\n",
            "Iteration: 148/537, Loss: 0.0005304731894284487\n",
            "Iteration: 149/537, Loss: 0.0003658425121102482\n",
            "Iteration: 150/537, Loss: 0.0001724947360344231\n",
            "Iteration: 151/537, Loss: 0.0005287794629111886\n",
            "Iteration: 152/537, Loss: 0.0032423969823867083\n",
            "Iteration: 153/537, Loss: 0.0002131447836291045\n",
            "Iteration: 154/537, Loss: 0.00044012448051944375\n",
            "Iteration: 155/537, Loss: 0.0043058935552835464\n",
            "Iteration: 156/537, Loss: 0.0015661277575418353\n",
            "Iteration: 157/537, Loss: 0.0006768310558982193\n",
            "Iteration: 158/537, Loss: 0.00034219061490148306\n",
            "Iteration: 159/537, Loss: 0.0002068749163299799\n",
            "Iteration: 160/537, Loss: 0.000416019291151315\n",
            "Iteration: 161/537, Loss: 0.0012074311962351203\n",
            "Iteration: 162/537, Loss: 0.007019550073891878\n",
            "Iteration: 163/537, Loss: 0.0007739681750535965\n",
            "Iteration: 164/537, Loss: 0.002264435635879636\n",
            "Iteration: 165/537, Loss: 0.0005324319354258478\n",
            "Iteration: 166/537, Loss: 0.00039768050191923976\n",
            "Iteration: 167/537, Loss: 0.0009774321224540472\n",
            "Iteration: 168/537, Loss: 0.0003055371926166117\n",
            "Iteration: 169/537, Loss: 0.00021684194507542998\n",
            "Iteration: 170/537, Loss: 0.0010770391672849655\n",
            "Iteration: 171/537, Loss: 0.0029328791424632072\n",
            "Iteration: 172/537, Loss: 0.0009152268758043647\n",
            "Iteration: 173/537, Loss: 0.02432863973081112\n",
            "Iteration: 174/537, Loss: 0.0015218566404655576\n",
            "Iteration: 175/537, Loss: 0.0010428032837808132\n",
            "Iteration: 176/537, Loss: 0.00024334107001777738\n",
            "Iteration: 177/537, Loss: 0.0003004811005666852\n",
            "Iteration: 178/537, Loss: 0.005565231200307608\n",
            "Iteration: 179/537, Loss: 0.0018096576677635312\n",
            "Iteration: 180/537, Loss: 0.00014172772353049368\n",
            "Iteration: 181/537, Loss: 0.002392988419160247\n",
            "Iteration: 182/537, Loss: 0.0015568409580737352\n",
            "Iteration: 183/537, Loss: 0.0007419500034302473\n",
            "Iteration: 184/537, Loss: 0.00037006725324317813\n",
            "Iteration: 185/537, Loss: 0.0006836447864770889\n",
            "Iteration: 186/537, Loss: 0.001645184587687254\n",
            "Iteration: 187/537, Loss: 0.00027935681282542646\n",
            "Iteration: 188/537, Loss: 0.0020209657959640026\n",
            "Iteration: 189/537, Loss: 0.007294708397239447\n",
            "Iteration: 190/537, Loss: 0.009660528041422367\n",
            "Iteration: 191/537, Loss: 0.0006844780873507261\n",
            "Iteration: 192/537, Loss: 0.00021656240278389305\n",
            "Iteration: 193/537, Loss: 0.0012935801642015576\n",
            "Iteration: 194/537, Loss: 0.008143344894051552\n",
            "Iteration: 195/537, Loss: 0.001578922150656581\n",
            "Iteration: 196/537, Loss: 0.0001897029869724065\n",
            "Iteration: 197/537, Loss: 0.0003679913643281907\n",
            "Iteration: 198/537, Loss: 0.0020820724312216043\n",
            "Iteration: 199/537, Loss: 0.03911883756518364\n",
            "Iteration: 200/537, Loss: 0.0028224834240972996\n",
            "Iteration: 201/537, Loss: 0.003361073089763522\n",
            "Iteration: 202/537, Loss: 0.0015157784800976515\n",
            "Iteration: 203/537, Loss: 0.0003565973893273622\n",
            "Iteration: 204/537, Loss: 0.000859511608723551\n",
            "Iteration: 205/537, Loss: 0.0006082350737415254\n",
            "Iteration: 206/537, Loss: 0.06116804480552673\n",
            "Iteration: 207/537, Loss: 0.004717352334409952\n",
            "Iteration: 208/537, Loss: 0.0014955817023292184\n",
            "Iteration: 209/537, Loss: 0.007459969259798527\n",
            "Iteration: 210/537, Loss: 0.0004219945112708956\n",
            "Iteration: 211/537, Loss: 0.0005854011978954077\n",
            "Iteration: 212/537, Loss: 0.006893471349030733\n",
            "Iteration: 213/537, Loss: 0.0008542521973140538\n",
            "Iteration: 214/537, Loss: 0.00014598655980080366\n",
            "Iteration: 215/537, Loss: 0.00023498063092119992\n",
            "Iteration: 216/537, Loss: 0.0015295364428311586\n",
            "Iteration: 217/537, Loss: 0.0007286726031452417\n",
            "Iteration: 218/537, Loss: 0.0014527629828080535\n",
            "Iteration: 219/537, Loss: 0.0025887370575219393\n",
            "Iteration: 220/537, Loss: 0.0018380846595391631\n",
            "Iteration: 221/537, Loss: 0.000145057711051777\n",
            "Iteration: 222/537, Loss: 0.00017387489788234234\n",
            "Iteration: 223/537, Loss: 0.00018803507555276155\n",
            "Iteration: 224/537, Loss: 0.0003902470925822854\n",
            "Iteration: 225/537, Loss: 0.0014060067478567362\n",
            "Iteration: 226/537, Loss: 0.11138526350259781\n",
            "Iteration: 227/537, Loss: 6.279013905441388e-05\n",
            "Iteration: 228/537, Loss: 0.002786981174722314\n",
            "Iteration: 229/537, Loss: 0.00043823986197821796\n",
            "Iteration: 230/537, Loss: 0.0008416774217039347\n",
            "Iteration: 231/537, Loss: 0.004852833691984415\n",
            "Iteration: 232/537, Loss: 0.0006659133359789848\n",
            "Iteration: 233/537, Loss: 0.005175323691219091\n",
            "Iteration: 234/537, Loss: 0.004858627915382385\n",
            "Iteration: 235/537, Loss: 0.0032067950814962387\n",
            "Iteration: 236/537, Loss: 0.003637250978499651\n",
            "Iteration: 237/537, Loss: 0.0004456486785784364\n",
            "Iteration: 238/537, Loss: 0.005634375847876072\n",
            "Iteration: 239/537, Loss: 0.0012196630705147982\n",
            "Iteration: 240/537, Loss: 0.001651845988817513\n",
            "Iteration: 241/537, Loss: 0.00441353116184473\n",
            "Iteration: 242/537, Loss: 0.00022208067821338773\n",
            "Iteration: 243/537, Loss: 0.0005252447444945574\n",
            "Iteration: 244/537, Loss: 0.00039281242061406374\n",
            "Iteration: 245/537, Loss: 0.001395277213305235\n",
            "Iteration: 246/537, Loss: 0.0012915690895169973\n",
            "Iteration: 247/537, Loss: 0.005452210083603859\n",
            "Iteration: 248/537, Loss: 0.002202404662966728\n",
            "Iteration: 249/537, Loss: 0.000560496118851006\n",
            "Iteration: 250/537, Loss: 0.00070886843604967\n",
            "Iteration: 251/537, Loss: 0.0004978982033208013\n",
            "Iteration: 252/537, Loss: 0.0023499326780438423\n",
            "Iteration: 253/537, Loss: 0.000146694655995816\n",
            "Iteration: 254/537, Loss: 0.0007989765144884586\n",
            "Iteration: 255/537, Loss: 0.00015382509445771575\n",
            "Iteration: 256/537, Loss: 0.0013483685906976461\n",
            "Iteration: 257/537, Loss: 0.00040814097155816853\n",
            "Iteration: 258/537, Loss: 0.0006555629661306739\n",
            "Iteration: 259/537, Loss: 0.005612812004983425\n",
            "Iteration: 260/537, Loss: 0.0013003477361053228\n",
            "Iteration: 261/537, Loss: 0.0006244331016205251\n",
            "Iteration: 262/537, Loss: 0.00021597123122774065\n",
            "Iteration: 263/537, Loss: 0.0005910663167014718\n",
            "Iteration: 264/537, Loss: 0.00038361086626537144\n",
            "Iteration: 265/537, Loss: 0.0002211755490861833\n",
            "Iteration: 266/537, Loss: 0.0011036787182092667\n",
            "Iteration: 267/537, Loss: 0.0022001478355377913\n",
            "Iteration: 268/537, Loss: 0.0005429308512248099\n",
            "Iteration: 269/537, Loss: 0.0061781699769198895\n",
            "Iteration: 270/537, Loss: 0.0027876668609678745\n",
            "Iteration: 271/537, Loss: 0.00033610581886023283\n",
            "Iteration: 272/537, Loss: 0.0072966995649039745\n",
            "Iteration: 273/537, Loss: 0.0018752377945929766\n",
            "Iteration: 274/537, Loss: 0.0008818336646072567\n",
            "Iteration: 275/537, Loss: 0.0063828760758042336\n",
            "Iteration: 276/537, Loss: 0.0002652950643096119\n",
            "Iteration: 277/537, Loss: 0.0007634526118636131\n",
            "Iteration: 278/537, Loss: 0.003926184959709644\n",
            "Iteration: 279/537, Loss: 0.0002765110111795366\n",
            "Iteration: 280/537, Loss: 0.0027344627305865288\n",
            "Iteration: 281/537, Loss: 0.0013281161664053798\n",
            "Iteration: 282/537, Loss: 0.012896175496280193\n",
            "Iteration: 283/537, Loss: 0.00036926515167579055\n",
            "Iteration: 284/537, Loss: 0.002799460431560874\n",
            "Iteration: 285/537, Loss: 0.04709143564105034\n",
            "Iteration: 286/537, Loss: 0.00038273294921964407\n",
            "Iteration: 287/537, Loss: 6.734933413099498e-05\n",
            "Iteration: 288/537, Loss: 0.0005051647312939167\n",
            "Iteration: 289/537, Loss: 0.008818409405648708\n",
            "Iteration: 290/537, Loss: 0.0031428630463778973\n",
            "Iteration: 291/537, Loss: 0.0066239978186786175\n",
            "Iteration: 292/537, Loss: 0.11196034401655197\n",
            "Iteration: 293/537, Loss: 0.00035698869032785296\n",
            "Iteration: 294/537, Loss: 0.0031092867720872164\n",
            "Iteration: 295/537, Loss: 0.00014710990944877267\n",
            "Iteration: 296/537, Loss: 0.0032536587677896023\n",
            "Iteration: 297/537, Loss: 0.0037247631698846817\n",
            "Iteration: 298/537, Loss: 0.0005172635428607464\n",
            "Iteration: 299/537, Loss: 0.003062207018956542\n",
            "Iteration: 300/537, Loss: 0.01863299496471882\n",
            "Iteration: 301/537, Loss: 0.00011997178080491722\n",
            "Iteration: 302/537, Loss: 0.00041873884038068354\n",
            "Iteration: 303/537, Loss: 0.0004737609997391701\n",
            "Iteration: 304/537, Loss: 0.0013891677372157574\n",
            "Iteration: 305/537, Loss: 0.0009482866153120995\n",
            "Iteration: 306/537, Loss: 0.002753951121121645\n",
            "Iteration: 307/537, Loss: 0.0032704693730920553\n",
            "Iteration: 308/537, Loss: 0.023849885910749435\n",
            "Iteration: 309/537, Loss: 0.0004692888760473579\n",
            "Iteration: 310/537, Loss: 0.0008090004557743669\n",
            "Iteration: 311/537, Loss: 0.000319776707328856\n",
            "Iteration: 312/537, Loss: 0.0011952234199270606\n",
            "Iteration: 313/537, Loss: 0.0007251211209222674\n",
            "Iteration: 314/537, Loss: 0.000524188217241317\n",
            "Iteration: 315/537, Loss: 0.00010206451406702399\n",
            "Iteration: 316/537, Loss: 0.0003177090547978878\n",
            "Iteration: 317/537, Loss: 0.005861232988536358\n",
            "Iteration: 318/537, Loss: 0.0013313231756910682\n",
            "Iteration: 319/537, Loss: 0.0002437273651594296\n",
            "Iteration: 320/537, Loss: 0.0001919815840665251\n",
            "Iteration: 321/537, Loss: 0.00029231715598143637\n",
            "Iteration: 322/537, Loss: 0.0005703647038899362\n",
            "Iteration: 323/537, Loss: 0.0010813161497935653\n",
            "Iteration: 324/537, Loss: 0.056299880146980286\n",
            "Iteration: 325/537, Loss: 0.00011799354251706973\n",
            "Iteration: 326/537, Loss: 8.603107562521473e-05\n",
            "Iteration: 327/537, Loss: 0.0009414925589226186\n",
            "Iteration: 328/537, Loss: 0.0004065867979079485\n",
            "Iteration: 329/537, Loss: 9.464634786127135e-05\n",
            "Iteration: 330/537, Loss: 0.0028342041186988354\n",
            "Iteration: 331/537, Loss: 0.00022758214618079364\n",
            "Iteration: 332/537, Loss: 0.0010510951979085803\n",
            "Iteration: 333/537, Loss: 0.00019142190285492688\n",
            "Iteration: 334/537, Loss: 0.00026986972079612315\n",
            "Iteration: 335/537, Loss: 0.00019964578677900136\n",
            "Iteration: 336/537, Loss: 0.0020300333853811026\n",
            "Iteration: 337/537, Loss: 0.0067363022826612\n",
            "Iteration: 338/537, Loss: 0.01022227481007576\n",
            "Iteration: 339/537, Loss: 0.0008434454211965203\n",
            "Iteration: 340/537, Loss: 0.003184998407959938\n",
            "Iteration: 341/537, Loss: 0.0005823862156830728\n",
            "Iteration: 342/537, Loss: 0.00039815373020246625\n",
            "Iteration: 343/537, Loss: 0.00025920153711922467\n",
            "Iteration: 344/537, Loss: 0.003245122265070677\n",
            "Iteration: 345/537, Loss: 0.0012776415096595883\n",
            "Iteration: 346/537, Loss: 0.00046480706078000367\n",
            "Iteration: 347/537, Loss: 0.0035186666063964367\n",
            "Iteration: 348/537, Loss: 0.0003111818805336952\n",
            "Iteration: 349/537, Loss: 0.00016925825912039727\n",
            "Iteration: 350/537, Loss: 0.00011982543219346553\n",
            "Iteration: 351/537, Loss: 0.0009472240344621241\n",
            "Iteration: 352/537, Loss: 0.001894614310003817\n",
            "Iteration: 353/537, Loss: 0.05964108556509018\n",
            "Iteration: 354/537, Loss: 0.0024319093208760023\n",
            "Iteration: 355/537, Loss: 0.00014684617053717375\n",
            "Iteration: 356/537, Loss: 0.00010858365567401052\n",
            "Iteration: 357/537, Loss: 0.0003427386691328138\n",
            "Iteration: 358/537, Loss: 0.06859132647514343\n",
            "Iteration: 359/537, Loss: 0.00033849760075099766\n",
            "Iteration: 360/537, Loss: 0.0003076157299801707\n",
            "Iteration: 361/537, Loss: 0.009172530844807625\n",
            "Iteration: 362/537, Loss: 0.00031647030846215785\n",
            "Iteration: 363/537, Loss: 0.00025423208717256784\n",
            "Iteration: 364/537, Loss: 0.002247630152851343\n",
            "Iteration: 365/537, Loss: 0.0005065212026238441\n",
            "Iteration: 366/537, Loss: 7.792840187903494e-05\n",
            "Iteration: 367/537, Loss: 0.00033548290957696736\n",
            "Iteration: 368/537, Loss: 0.0016807487700134516\n",
            "Iteration: 369/537, Loss: 0.030293449759483337\n",
            "Iteration: 370/537, Loss: 0.0014715816359966993\n",
            "Iteration: 371/537, Loss: 0.00020622614829335362\n",
            "Iteration: 372/537, Loss: 0.00011657830327749252\n",
            "Iteration: 373/537, Loss: 0.002637771423906088\n",
            "Iteration: 374/537, Loss: 0.23344069719314575\n",
            "Iteration: 375/537, Loss: 0.0003697747888509184\n",
            "Iteration: 376/537, Loss: 0.0001915594912134111\n",
            "Iteration: 377/537, Loss: 0.0014602105366066098\n",
            "Iteration: 378/537, Loss: 0.0005612241802737117\n",
            "Iteration: 379/537, Loss: 0.00015582122432533652\n",
            "Iteration: 380/537, Loss: 0.00032097820076160133\n",
            "Iteration: 381/537, Loss: 0.0028733741492033005\n",
            "Iteration: 382/537, Loss: 0.0008105197921395302\n",
            "Iteration: 383/537, Loss: 0.0007047304534353316\n",
            "Iteration: 384/537, Loss: 0.0006546668009832501\n",
            "Iteration: 385/537, Loss: 0.0007265566382557154\n",
            "Iteration: 386/537, Loss: 0.0006844634772278368\n",
            "Iteration: 387/537, Loss: 0.00023037160281091928\n",
            "Iteration: 388/537, Loss: 0.002354644937440753\n",
            "Iteration: 389/537, Loss: 0.00010549255239311606\n",
            "Iteration: 390/537, Loss: 0.0005322376964613795\n",
            "Iteration: 391/537, Loss: 0.0847334936261177\n",
            "Iteration: 392/537, Loss: 0.06151513755321503\n",
            "Iteration: 393/537, Loss: 0.00045267248060554266\n",
            "Iteration: 394/537, Loss: 0.0010106383124366403\n",
            "Iteration: 395/537, Loss: 0.00042117966222576797\n",
            "Iteration: 396/537, Loss: 0.00013242570275906473\n",
            "Iteration: 397/537, Loss: 0.0012469789944589138\n",
            "Iteration: 398/537, Loss: 0.00029181441641412675\n",
            "Iteration: 399/537, Loss: 0.0001121990499086678\n",
            "Iteration: 400/537, Loss: 0.0004481275682337582\n",
            "Iteration: 401/537, Loss: 0.00031476651201955974\n",
            "Iteration: 402/537, Loss: 0.0005141361034475267\n",
            "Iteration: 403/537, Loss: 0.32343029975891113\n",
            "Iteration: 404/537, Loss: 0.0017485953867435455\n",
            "Iteration: 405/537, Loss: 0.00033671126584522426\n",
            "Iteration: 406/537, Loss: 0.00010766424384200945\n",
            "Iteration: 407/537, Loss: 0.002578789135441184\n",
            "Iteration: 408/537, Loss: 0.0016292545478790998\n",
            "Iteration: 409/537, Loss: 0.0003895000263582915\n",
            "Iteration: 410/537, Loss: 0.0010164728155359626\n",
            "Iteration: 411/537, Loss: 0.0005988819757476449\n",
            "Iteration: 412/537, Loss: 0.00022789521608501673\n",
            "Iteration: 413/537, Loss: 0.0036976970732212067\n",
            "Iteration: 414/537, Loss: 0.003399156266823411\n",
            "Iteration: 415/537, Loss: 0.010178572498261929\n",
            "Iteration: 416/537, Loss: 0.0006998537573963404\n",
            "Iteration: 417/537, Loss: 0.0021618094760924578\n",
            "Iteration: 418/537, Loss: 0.0035973223857581615\n",
            "Iteration: 419/537, Loss: 0.0003499533631838858\n",
            "Iteration: 420/537, Loss: 0.0003664335235953331\n",
            "Iteration: 421/537, Loss: 0.0012308439472690225\n",
            "Iteration: 422/537, Loss: 0.0001229776389664039\n",
            "Iteration: 423/537, Loss: 0.00019476391025818884\n",
            "Iteration: 424/537, Loss: 0.00019186086137779057\n",
            "Iteration: 425/537, Loss: 0.0003306861035525799\n",
            "Iteration: 426/537, Loss: 6.910840602358803e-05\n",
            "Iteration: 427/537, Loss: 0.00028815437690354884\n",
            "Iteration: 428/537, Loss: 0.01019197702407837\n",
            "Iteration: 429/537, Loss: 0.0013262820430099964\n",
            "Iteration: 430/537, Loss: 0.005380613729357719\n",
            "Iteration: 431/537, Loss: 0.0002588331699371338\n",
            "Iteration: 432/537, Loss: 0.007381232921034098\n",
            "Iteration: 433/537, Loss: 0.00016058656910900027\n",
            "Iteration: 434/537, Loss: 0.1627427637577057\n",
            "Iteration: 435/537, Loss: 0.002227629069238901\n",
            "Iteration: 436/537, Loss: 0.0006796125089749694\n",
            "Iteration: 437/537, Loss: 0.045812755823135376\n",
            "Iteration: 438/537, Loss: 0.0003203407395631075\n",
            "Iteration: 439/537, Loss: 9.982775372918695e-05\n",
            "Iteration: 440/537, Loss: 0.0015663978410884738\n",
            "Iteration: 441/537, Loss: 0.0007655307417735457\n",
            "Iteration: 442/537, Loss: 0.06228560954332352\n",
            "Iteration: 443/537, Loss: 0.0007610960747115314\n",
            "Iteration: 444/537, Loss: 0.0004217950627207756\n",
            "Iteration: 445/537, Loss: 0.00023978311219252646\n",
            "Iteration: 446/537, Loss: 0.00019728956976905465\n",
            "Iteration: 447/537, Loss: 0.0034072156995534897\n",
            "Iteration: 448/537, Loss: 0.0036127385683357716\n",
            "Iteration: 449/537, Loss: 0.00012023829185636714\n",
            "Iteration: 450/537, Loss: 0.00014494419156108052\n",
            "Iteration: 451/537, Loss: 0.0001306392514379695\n",
            "Iteration: 452/537, Loss: 0.0002798285277094692\n",
            "Iteration: 453/537, Loss: 0.0027594282291829586\n",
            "Iteration: 454/537, Loss: 0.007806724403053522\n",
            "Iteration: 455/537, Loss: 0.0018452524673193693\n",
            "Iteration: 456/537, Loss: 0.00193129968829453\n",
            "Iteration: 457/537, Loss: 0.0024503227323293686\n",
            "Iteration: 458/537, Loss: 0.00044168924796395004\n",
            "Iteration: 459/537, Loss: 0.004290888085961342\n",
            "Iteration: 460/537, Loss: 0.4090109169483185\n",
            "Iteration: 461/537, Loss: 0.00018879526760429144\n",
            "Iteration: 462/537, Loss: 0.01673748902976513\n",
            "Iteration: 463/537, Loss: 0.0005524330190382898\n",
            "Iteration: 464/537, Loss: 0.005314238835126162\n",
            "Iteration: 465/537, Loss: 0.09508220851421356\n",
            "Iteration: 466/537, Loss: 0.003522004932165146\n",
            "Iteration: 467/537, Loss: 0.0005434374324977398\n",
            "Iteration: 468/537, Loss: 0.0014498168602585793\n",
            "Iteration: 469/537, Loss: 0.0005428586155176163\n",
            "Iteration: 470/537, Loss: 0.0006217060727067292\n",
            "Iteration: 471/537, Loss: 0.00037510856054723263\n",
            "Iteration: 472/537, Loss: 0.0017148458864539862\n",
            "Iteration: 473/537, Loss: 0.0009351980406790972\n",
            "Iteration: 474/537, Loss: 0.05486566573381424\n",
            "Iteration: 475/537, Loss: 0.007902305573225021\n",
            "Iteration: 476/537, Loss: 0.003706536255776882\n",
            "Iteration: 477/537, Loss: 0.05805393308401108\n",
            "Iteration: 478/537, Loss: 0.004102872218936682\n",
            "Iteration: 479/537, Loss: 0.0009372725617140532\n",
            "Iteration: 480/537, Loss: 0.000851021904964\n",
            "Iteration: 481/537, Loss: 0.0005808748537674546\n",
            "Iteration: 482/537, Loss: 0.002788273384794593\n",
            "Iteration: 483/537, Loss: 0.0016929650446400046\n",
            "Iteration: 484/537, Loss: 0.0016129936557263136\n",
            "Iteration: 485/537, Loss: 0.0013853451237082481\n",
            "Iteration: 486/537, Loss: 0.002857880899682641\n",
            "Iteration: 487/537, Loss: 0.0004494823806453496\n",
            "Iteration: 488/537, Loss: 0.11747117340564728\n",
            "Iteration: 489/537, Loss: 0.0014558536931872368\n",
            "Iteration: 490/537, Loss: 0.0035937416832894087\n",
            "Iteration: 491/537, Loss: 0.004221740178763866\n",
            "Iteration: 492/537, Loss: 0.000882783206179738\n",
            "Iteration: 493/537, Loss: 0.003957930486649275\n",
            "Iteration: 494/537, Loss: 0.000899053062312305\n",
            "Iteration: 495/537, Loss: 0.00499417120590806\n",
            "Iteration: 496/537, Loss: 0.0018060833681374788\n",
            "Iteration: 497/537, Loss: 0.0005636618006974459\n",
            "Iteration: 498/537, Loss: 0.35344821214675903\n",
            "Iteration: 499/537, Loss: 0.00017548134201206267\n",
            "Iteration: 500/537, Loss: 0.0010895744198933244\n",
            "Iteration: 501/537, Loss: 0.0007424342911690474\n",
            "Iteration: 502/537, Loss: 0.006835645996034145\n",
            "Iteration: 503/537, Loss: 0.00042150873923674226\n",
            "Iteration: 504/537, Loss: 0.002278311178088188\n",
            "Iteration: 505/537, Loss: 0.004845642019063234\n",
            "Iteration: 506/537, Loss: 0.0006566627416759729\n",
            "Iteration: 507/537, Loss: 0.0044660670682787895\n",
            "Iteration: 508/537, Loss: 0.0005200309096835554\n",
            "Iteration: 509/537, Loss: 0.008190718479454517\n",
            "Iteration: 510/537, Loss: 0.00012666647671721876\n",
            "Iteration: 511/537, Loss: 0.20700299739837646\n",
            "Iteration: 512/537, Loss: 0.00038878648774698377\n",
            "Iteration: 513/537, Loss: 0.0005315745947882533\n",
            "Iteration: 514/537, Loss: 0.02674553170800209\n",
            "Iteration: 515/537, Loss: 0.0005795746110379696\n",
            "Iteration: 516/537, Loss: 0.002159262541681528\n",
            "Iteration: 517/537, Loss: 0.0002569384523667395\n",
            "Iteration: 518/537, Loss: 0.005586974788457155\n",
            "Iteration: 519/537, Loss: 0.0014936054358258843\n",
            "Iteration: 520/537, Loss: 0.003005178179591894\n",
            "Iteration: 521/537, Loss: 0.001572950161062181\n",
            "Iteration: 522/537, Loss: 0.002854331396520138\n",
            "Iteration: 523/537, Loss: 0.0005183070315979421\n",
            "Iteration: 524/537, Loss: 0.001151718432083726\n",
            "Iteration: 525/537, Loss: 0.0006316197104752064\n",
            "Iteration: 526/537, Loss: 0.002084301318973303\n",
            "Iteration: 527/537, Loss: 0.000387812324333936\n",
            "Iteration: 528/537, Loss: 0.00017921364633366466\n",
            "Iteration: 529/537, Loss: 0.0005495544755831361\n",
            "Iteration: 530/537, Loss: 0.0025350255891680717\n",
            "Iteration: 531/537, Loss: 0.0010267863981425762\n",
            "Iteration: 532/537, Loss: 0.0015624325023964047\n",
            "Iteration: 533/537, Loss: 0.001529874512925744\n",
            "Iteration: 534/537, Loss: 0.0005639108130708337\n",
            "Iteration: 535/537, Loss: 0.000539348111487925\n",
            "Iteration: 536/537, Loss: 0.0011206843191757798\n",
            "Iteration: 537/537, Loss: 0.7442821860313416\n",
            "Epoch 49 train loss: 0.01114295034119402\n",
            "Epoch 49 test loss: 0.07161556134265588\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iigMuWghd27",
        "outputId": "b065b1c3-10e1-4f83-f166-71fb7161f4fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "# Plot\n",
        "print(train_loss_list)\n",
        "print(test_loss_list)\n",
        "\n",
        "plt.plot(train_loss_list)\n",
        "plt.plot(test_loss_list)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train/Test Loss')\n",
        "plt.legend(['Train', 'Test'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8751343211342947, 0.5352398935580553, 0.4092632952585012, 0.30201235091664047, 0.26986005987965084, 0.20271444015093912, 0.1828086928315322, 0.22344859593783747, 0.16051800841391017, 0.14614733533276456, 0.13857798891907708, 0.14032699921046118, 0.14099020954959435, 0.1090775514665377, 0.11350145022292904, 0.07714882263808825, 0.07728082344722474, 0.07334912696300903, 0.0828254692099496, 0.09088125794339454, 0.07026057130146816, 0.07167971577647955, 0.06451727528620625, 0.06654046142304289, 0.06748944981766386, 0.05959264102668954, 0.04232047065766008, 0.02797595562640529, 0.04420343116477515, 0.04227565050598154, 0.038067953032072895, 0.04231125953904478, 0.03692038958671969, 0.03478334835874441, 0.03340234454175231, 0.032357043842562855, 0.020325234656299557, 0.0346747175261677, 0.043592596070070704, 0.02376674917067277, 0.016547095710394324, 0.022259297736996184, 0.019224169135826, 0.014716392869249119, 0.018174238165927656, 0.01622389992342139, 0.019602919221728663, 0.00833150005679167, 0.017282446798211826, 0.01114295034119402]\n",
            "[0.5970182651722873, 0.5129179004303835, 0.44681475282543237, 0.32342374522101, 0.2490378832065121, 0.20284322818258294, 0.20494777655435933, 0.2461175118021115, 0.182118564069754, 0.2119689942251339, 0.1521658914025735, 0.25118207772848783, 0.18649322339678528, 0.20260553625412286, 0.09843762154966869, 0.1393918813266619, 0.155640354178225, 0.08358019655699739, 0.09486264635084404, 0.08268799962199948, 0.1947393172718067, 0.12982052690780688, 0.12926245462364966, 0.0988114900436442, 0.08980992994728257, 0.12394740605744085, 0.09590856762370095, 0.13767581931444714, 0.10690757150037421, 0.0938880931226002, 0.09165623065006609, 0.11700245617259362, 0.10020322830597353, 0.08128093142209022, 0.11530897175522384, 0.08839532811112737, 0.12448062943930617, 0.09467844221615922, 0.102188863673062, 0.05495839630193249, 0.08596828773879679, 0.0784364718706692, 0.06120703452788466, 0.0715015933480269, 0.06753486510122592, 0.07363599311491406, 0.08100563144506412, 0.07657962553019859, 0.054769958066861836, 0.07161556134265588]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}